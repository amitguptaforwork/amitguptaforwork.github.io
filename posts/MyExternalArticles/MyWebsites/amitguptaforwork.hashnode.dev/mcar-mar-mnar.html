<!DOCTYPE html><html lang="en" data-theme="light" class="">
<!-- Mirrored from amitguptaforwork.hashnode.dev/mcar-mar-mnar by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 02 Oct 2025 10:24:50 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head><meta charSet="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta name="image" property="og:image" content="https://hashnode.com/utility/r?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1759335348948%2F4ead57b0-0b81-454f-a5d5-938972c37be8.jpeg%3Fw%3D1200%26h%3D630%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp%26fm%3Dpng"/><link rel="canonical" href="mcar-mar-mnar.html"/><title>📘 Mcar, Mar, Mnar —</title><meta name="description" content="Understanding the Terms Using an Analogy- Shoppers &amp; Shopping Carts Analogy
1. MCAR – Missing Completely At Random
Story: You’re analyzing what items shoppers add to their online carts. Suddenly, due to a system glitch, a random subset of cart items ..."/><meta property="og:title" content="📘 Mcar, Mar, Mnar —"/><meta property="og:description" content="Understanding the Terms Using an Analogy- Shoppers &amp; Shopping Carts Analogy
1. MCAR – Missing Completely At Random
Story: You’re analyzing what items shoppers add to their online carts. Suddenly, due to a system glitch, a random subset of cart items ..."/><meta property="og:site_name" content="Amit&#x27;s Learning Notes"/><meta property="og:type" content="article"/><meta property="og:url" content="mcar-mar-mnar.html"/><meta name="author" content="Amit Gupta"/><meta property="article:author" content="https://hashnode.com/@learnwithamit"/><link rel="author" href="https://hashnode.com/@learnwithamit"/><link rel="apple-touch-icon" sizes="180x180" href="../cdn.hashnode.com/res/hashnode/image/upload/v1611242155728/W3_BYVVVh.png"/><link rel="icon" type="image/png" sizes="32x32" href="../cdn.hashnode.com/res/hashnode/image/upload/v1611242173172/AOX1gE2jc.png"/><link rel="icon" type="image/png" sizes="16x16" href="../cdn.hashnode.com/res/hashnode/image/upload/v1611242187756/TRTNYp32O.png"/><link rel="mask-icon" href="static/images/brand/safari-pinned-tab-new.html" color="#2962ff"/><meta name="msapplication-TileColor" content="#ffffff"/><meta name="theme-color" content="#f6f7fb"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="📘 Mcar, Mar, Mnar —"/><meta property="twitter:description" content="Understanding the Terms Using an Analogy- Shoppers &amp; Shopping Carts Analogy
1. MCAR – Missing Completely At Random
Story: You’re analyzing what items shoppers add to their online carts. Suddenly, due to a system glitch, a random subset of cart items ..."/><meta property="twitter:image" content="https://hashnode.com/utility/r?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1759335348948%2F4ead57b0-0b81-454f-a5d5-938972c37be8.jpeg%3Fw%3D1200%26h%3D630%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp%26fm%3Dpng"/><style>/* Monkai theme */
          .hljs{display:block;overflow-x:auto;padding:.5em;background:#23241f}.hljs,.hljs-subst,.hljs-tag{color:#f8f8f2}.hljs-emphasis,.hljs-strong{color:#a8a8a2}.hljs-bullet,.hljs-link,.hljs-literal,.hljs-number,.hljs-quote,.hljs-regexp{color:#ae81ff}.hljs-code,.hljs-section,.hljs-selector-class,.hljs-title{color:#a6e22e}.hljs-strong{font-weight:700}.hljs-emphasis{font-style:italic}.hljs-attr,.hljs-keyword,.hljs-name,.hljs-selector-tag{color:#f92672}.hljs-attribute,.hljs-symbol{color:#66d9ef}.hljs-class .hljs-title,.hljs-params{color:#f8f8f2}.hljs-addition,.hljs-built_in,.hljs-builtin-name,.hljs-selector-attr,.hljs-selector-id,.hljs-selector-pseudo,.hljs-string,.hljs-template-variable,.hljs-type,.hljs-variable{color:#e6db74}.hljs-comment,.hljs-deletion,.hljs-meta{color:#75715e}
            /* Monkai theme ends */</style><link rel="alternate" type="application/rss+xml" title="RSS Feed for 📘 Mcar, Mar, Mnar —" href="mcar-mar-mnar/rss.html"/><link rel="preload" as="image" href="../cdn.hashnode.com/res/hashnode/image/upload/v1759335348948/4ead57b0-0b81-454f-a5d5-938972c37be8a723.jpg?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=compress,format&amp;format=webp"/><meta name="next-head-count" content="27"/><style>#nprogress{pointer-events:none}#nprogress .bar{background:#29d;position:fixed;z-index:1031;top:0;left:0;width:100%;height:2px}#nprogress .peg{display:block;position:absolute;right:0;width:100px;height:100%;box-shadow:0 0 10px #29d,0 0 5px #29d;opacity:1;-webkit-transform:rotate(3deg) translate(0,-4px);-ms-transform:rotate(3deg) translate(0,-4px);transform:rotate(3deg) translate(0,-4px)}#nprogress .spinner{display:block;position:fixed;z-index:1031;top:15px;right:15px}#nprogress .spinner-icon{width:18px;height:18px;box-sizing:border-box;border:solid 2px transparent;border-top-color:#29d;border-left-color:#29d;border-radius:50%;-webkit-animation:nprogress-spinner .4s linear infinite;animation:nprogress-spinner .4s linear infinite}.nprogress-custom-parent{overflow:hidden;position:relative}.nprogress-custom-parent #nprogress .bar,.nprogress-custom-parent #nprogress .spinner{position:absolute}@-webkit-keyframes nprogress-spinner{0%{-webkit-transform:rotate(0)}100%{-webkit-transform:rotate(360deg)}}@keyframes nprogress-spinner{0%{transform:rotate(0)}100%{transform:rotate(360deg)}}</style><script async="" src="https://ping.hashnode.com/gtag/js?id=G-72XG3F8LNJ"></script><script type="text/javascript">
    window.dataLayer = window.dataLayer || [];
    function gtag(){window.dataLayer.push(arguments);}
    gtag('js', new Date());
  </script><link rel="preload" href="_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="_next/static/css/62323deb9f18e06c.css" as="style"/><link rel="stylesheet" href="_next/static/css/62323deb9f18e06c.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script defer="" src="_next/static/chunks/8820-56721d947d773244.js"></script><script defer="" src="_next/static/chunks/3364-87c2ffca7936c855.js"></script><script defer="" src="_next/static/chunks/8226.64bcf70d809f2c9f.js"></script><script defer="" src="_next/static/chunks/5950-ac4ab424aa3bdc31.js"></script><script defer="" src="_next/static/chunks/7179.58a3a6a905ed3c5b.js"></script><script src="_next/static/chunks/webpack-13f9006097c88b74.js" defer=""></script><script src="_next/static/chunks/framework-ce84985cd166733a.js" defer=""></script><script src="_next/static/chunks/main-7636331dc094a8a9.js" defer=""></script><script src="_next/static/chunks/pages/_app-75660f2a45d790be.js" defer=""></script><script src="_next/static/chunks/0b5ea8d6-208c5eca82a94965.js" defer=""></script><script src="_next/static/chunks/6365-3d0e1852af604b43.js" defer=""></script><script src="_next/static/chunks/6933-f312215db97b5535.js" defer=""></script><script src="_next/static/chunks/pages/%5b...slug%5d-0670863b9e4a6e11.js" defer=""></script><script src="_next/static/MQT9dcvepPIR9449I1Jve/_buildManifest.js" defer=""></script><script src="_next/static/MQT9dcvepPIR9449I1Jve/_ssgManifest.js" defer=""></script><style id="__jsx-229592980">@font-face{font-family:"Suisse Intl";src:url("fonts/SuisseIntl-Book-WebXL.woff2")format("woff2"),url("fonts/SuisseIntl-Book-WebXL.woff")format("woff");font-weight:450;font-style:normal;font-display:block}@font-face{font-family:"Suisse Intl";src:url("fonts/SuisseIntl-Medium-WebXL.woff2")format("woff2"),url("fonts/SuisseIntl-Medium-WebXL.woff")format("woff");font-weight:500;font-style:normal;font-display:block}@font-face{font-family:"Suisse Intl";src:url("fonts/SuisseIntl-SemiBold-WebXL.woff2")format("woff2"),url("fonts/SuisseIntl-SemiBold-WebXL.woff")format("woff");font-weight:600;font-style:normal;font-display:block}@font-face{font-family:"Suisse Intl";src:url("fonts/SuisseIntl-Bold-WebXL.woff2")format("woff2"),url("fonts/SuisseIntl-Bold-WebXL.woff")format("woff");font-weight:700;font-style:normal;font-display:block}html{--font-inter:__Inter_611a59;--font-suisse-intl:'Suisse Intl';--font-mermaid:var(--font-inter)}</style></head><body class="bg-white leading-normal antialiased dark:bg-slate-950"><input type="hidden" id="hn-user"/><style id="__jsx-229592980">@font-face{font-family:"Suisse Intl";src:url("fonts/SuisseIntl-Book-WebXL.woff2")format("woff2"),url("fonts/SuisseIntl-Book-WebXL.woff")format("woff");font-weight:450;font-style:normal;font-display:block}@font-face{font-family:"Suisse Intl";src:url("fonts/SuisseIntl-Medium-WebXL.woff2")format("woff2"),url("fonts/SuisseIntl-Medium-WebXL.woff")format("woff");font-weight:500;font-style:normal;font-display:block}@font-face{font-family:"Suisse Intl";src:url("fonts/SuisseIntl-SemiBold-WebXL.woff2")format("woff2"),url("fonts/SuisseIntl-SemiBold-WebXL.woff")format("woff");font-weight:600;font-style:normal;font-display:block}@font-face{font-family:"Suisse Intl";src:url("fonts/SuisseIntl-Bold-WebXL.woff2")format("woff2"),url("fonts/SuisseIntl-Bold-WebXL.woff")format("woff");font-weight:700;font-style:normal;font-display:block}html{--font-inter:__Inter_611a59;--font-suisse-intl:'Suisse Intl';--font-mermaid:var(--font-inter)}</style><div id="__next"><div class="bg-white dark:bg-slate-950" data-theme="light"><script type="application/ld+json">{"@context":"https://schema.org","@type":"NewsArticle","url":"https://amitguptaforwork.hashnode.dev/mcar-mar-mnar","mainEntityOfPage":"https://amitguptaforwork.hashnode.dev/mcar-mar-mnar","headline":"📘 Mcar, Mar, Mnar —","description":"Understanding the Terms Using an Analogy- Shoppers &amp; Shopping Carts Analogy\n1. MCAR – Missing Completely At Random\nStory: You’re analyzing what items shoppers add to their online carts. Suddenly, due to a system glitch, a random subset of cart items ...","datePublished":"2025-10-01T14:26:36.843Z","dateModified":"2025-10-01T16:16:07.545Z","isAccessibleForFree":true,"author":{"@type":"Person","name":"Amit Gupta","url":"https://hashnode.com/@learnwithamit"},"publisher":{"@type":"Organization","name":"Amit's Learning Notes","url":"https://amitguptaforwork.hashnode.dev","logo":"https://hashnode.com/utility/r?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1559814205701%2Fek9fO-yT0.jpeg%3Fw%3D800%26bm%3Dnormal%26balph%3D100%26txt64%3DQW1pdCdzIExlYXJuaW5nIE5vdGVz%26txtsize%3D42%26txtfit%3Dmax%26txtalign%3Dmiddle%2Ccenter%26txtfont%3DHelvetica%20Neue%2CBold%26txtclr%3D000000%26blend%3Dffffff"},"image":{"@type":"ImageObject","url":"https://cdn.hashnode.com/res/hashnode/image/upload/v1759335348948/4ead57b0-0b81-454f-a5d5-938972c37be8.jpeg"}}</script><header class="blog-header z-50 w-full border-b relative transform-none md:sticky md:top-0 md:left-0 md:backdrop-blur-lg border-black/10 bg-white bg-opacity-70 dark:border-white/10 dark:bg-slate-900 dark:bg-opacity-70"><div class="container mx-auto px-2 md:px-4 md:py-1 2xl:px-10"><div class="relative z-40 flex flex-row items-center justify-between pb-2 pt-8 md:py-4"><div class="mb-2 flex flex-row items-center md:mb-0 dark:text-white"><a aria-label="Back to blog home" class="blog-back-to-home-button focus-ring-base flex flex-row items-center rounded-full font-medium transition duration-100 ease-in-out focus-ring-colors-base hover:bg-black/10 dark:hover:bg-white/20 mr-2 p-3" data-state="closed" href="index.html"><svg class="h-4 w-4 fill-current pr-1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 17"><path d="M7.75135 16.7197L0.885098 9.55347C0.683348 9.31347 0.600098 9.08847 0.600098 8.89722C0.600098 8.70597 0.68331 8.44834 0.850898 8.27509L7.71715 1.10884C8.06035 0.749066 8.6299 0.737366 8.9884 1.08189C9.34933 1.42408 9.36107 1.99576 9.01535 2.35351L2.7466 8.89722L9.0466 15.4747C9.39231 15.831 9.38057 16.404 9.01965 16.7463C8.6626 17.091 8.0926 17.0797 7.75135 16.7197Z"></path></svg></a><div class="mr-2"><button type="button" aria-label="Open blog links" class="blog-bars-button focus-ring-base flex flex-row items-center rounded-full font-medium transition duration-100 ease-in-out focus-ring-colors-base hover:bg-black/10 dark:hover:bg-white/20 mr-2 p-2" data-state="closed"><svg class="h-6 w-6 stroke-current" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M20.9889 11.9969H11.9945H3M20.9889 17.8745H3M21 6.12451H3" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></button></div><div class="hidden md:block"><div class="blog-title text-lg md:text-xl text-left break-words font-heading font-semibold leading-snug md:font-bold dark:text-white"><a href="index4550.html?source=top_nav_blog_home" class="focus-ring-base flex flex-row items-center focus-ring-colors-base" aria-label="Amit&#x27;s Learning Notes home page">Amit&#x27;s Learning Notes</a></div></div></div><div class="flex flex-row items-center dark:text-white"><button type="button" aria-label="Open blog search" class="blog-search-button focus-ring-base flex flex-row items-center rounded-full font-medium transition duration-100 ease-in-out focus-ring-colors-base hover:bg-black/10 dark:hover:bg-white/20 mr-2 p-2" data-state="closed"><svg class="h-6 w-6 stroke-current" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none"><path d="M21 21L15.8091 15.8091M18 10.5C18 14.6421 14.6421 18 10.5 18C6.35786 18 3 14.6421 3 10.5C3 6.35786 6.35786 3 10.5 3C14.6421 3 18 6.35786 18 10.5Z" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></button><button type="button" aria-label="Toggle blog theme" class="blog-theme-switcher focus-ring-base flex flex-row items-center rounded-full font-medium transition duration-100 ease-in-out focus-ring-colors-base hover:bg-black/10 dark:hover:bg-white/20 mr-2 p-2" data-state="closed"><svg class="h-6 w-6 stroke-current" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none"><path d="M3 11.4489C3 16.7238 7.16904 21 12.3118 21C16.2709 21 19.6529 18.4657 21 14.8925C19.9331 15.4065 18.7418 15.6938 17.485 15.6938C12.9137 15.6938 9.20787 11.8928 9.20787 7.20396C9.20787 5.24299 9.85605 3.4373 10.9446 2C6.45002 2.6783 3 6.65034 3 11.4489Z" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></button><div class="hidden md:mr-2 md:block"><div class="flex animate-pulse flex-row items-center rounded-full border-1-1/2 px-4 py-2 text-center text-sm font-medium bg-slate-100 dark:border-slate-900 dark:bg-slate-800"><svg class="invisible mr-2 h-5 w-5" viewBox="0 0 22 22" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.2953 4.58698C13.9301 4.37611 13.683 4.00814 13.626 3.59028L13.4336 2.17939C13.3415 1.50373 12.7644 1 12.0825 1L9.91765 1C9.23573 1 8.65865 1.50373 8.56652 2.17939L8.37412 3.5903C8.31709 4.00849 8.06963 4.37672 7.70395 4.58749L7.0923 4.94003C6.72723 5.15046 6.28538 5.18029 5.89533 5.02084L4.57791 4.48227C3.9467 4.22423 3.22192 4.47214 2.88096 5.06269L1.79854 6.9375C1.45758 7.52805 1.60528 8.27969 2.14436 8.69731L3.26948 9.56895C3.60259 9.82701 3.79768 10.2246 3.79799 10.646L3.7985 11.3519C3.7988 11.774 3.60364 12.1724 3.26999 12.4309L2.14433 13.3029C1.60525 13.7205 1.45755 14.4722 1.79851 15.0627L2.88093 16.9375C3.22189 17.5281 3.94667 17.776 4.57788 17.518L5.89588 16.9792C6.28624 16.8196 6.72847 16.8496 7.0937 17.0604L7.70483 17.4133C8.07006 17.6241 8.31718 17.9921 8.37416 18.41L8.56652 19.8206C8.65865 20.4963 9.23573 21 9.91765 21L12.0825 21C12.7644 21 13.3415 20.4963 13.4336 19.8206L13.626 18.4098C13.683 17.9916 13.9305 17.6234 14.2962 17.4127L14.9077 17.0602C15.2728 16.8497 15.7146 16.8199 16.1047 16.9794L17.4221 17.5179C18.0533 17.776 18.7781 17.5281 19.119 16.9375L20.2015 15.0627C20.5424 14.4721 20.3947 13.7205 19.8557 13.3029L18.7305 12.4312C18.3974 12.1732 18.2023 11.7756 18.202 11.3542L18.2015 10.6483C18.2012 10.2263 18.3964 9.82784 18.73 9.56935L19.8556 8.69734C20.3947 8.27972 20.5424 7.52809 20.2014 6.93753L19.119 5.06272C18.7781 4.47217 18.0533 4.22427 17.4221 4.48231L16.1042 5.02106C15.7138 5.18064 15.2716 5.15063 14.9064 4.93976L14.2953 4.58698Z" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M13.3619 12.3637C12.6088 13.6681 10.9408 14.1151 9.63638 13.362C8.33195 12.6088 7.88502 10.9409 8.63813 9.63643C9.39125 8.33199 11.0592 7.88506 12.3637 8.63818C13.6681 9.39129 14.115 11.0593 13.3619 12.3637Z" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="invisible">Follow</span></div></div><div class="hidden md:mr-2 md:block"><div class="animate-pulse rounded-full border-1-1/2 p-2 bg-slate-100 dark:border-slate-900 dark:bg-slate-800"><svg class="invisible h-5 w-5" viewBox="0 0 22 17" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.1728 3.50973C12.587 3.50973 12.9228 3.17395 12.9228 2.75973C12.9228 2.34552 12.587 2.00973 12.1728 2.00973V3.50973ZM18.958 9.10996C18.958 8.69575 18.6222 8.35996 18.208 8.35996C17.7938 8.35996 17.458 8.69575 17.458 9.10996H18.958ZM1.5018 5.97023C1.1338 5.7801 0.681352 5.9243 0.491222 6.2923C0.301092 6.66029 0.445283 7.11275 0.813283 7.30288L1.5018 5.97023ZM15.6397 8.12794C15.9841 7.8979 16.0769 7.43217 15.8468 7.08771C15.6168 6.74325 15.1511 6.6505 14.8066 6.88055L15.6397 8.12794ZM7.29742 9.80877L6.95316 10.4751L7.29742 9.80877ZM9.154 10.5963L9.2714 9.85557L9.154 10.5963ZM12.1823 9.53509L11.7658 8.91139L12.1823 9.53509ZM10.4253 10.5251L10.2259 9.80209L10.4253 10.5251ZM16.1797 3.11305C15.8868 2.82016 15.4119 2.82016 15.1191 3.11305C14.8262 3.40595 14.8262 3.88082 15.1191 4.17371L16.1797 3.11305ZM17.3215 5.31545L16.7911 5.84578C16.9358 5.99043 17.1332 6.06972 17.3377 6.06527C17.5423 6.06083 17.7361 5.97304 17.8743 5.82224L17.3215 5.31545ZM21.5529 1.80928C21.8328 1.50394 21.8121 1.02952 21.5068 0.749625C21.2015 0.46973 20.727 0.490357 20.4471 0.795697L21.5529 1.80928ZM17.458 14.2317C17.458 14.8735 16.9378 15.3937 16.296 15.3937V16.8937C17.7662 16.8937 18.958 15.7019 18.958 14.2317H17.458ZM16.296 15.3937H2.912V16.8937H16.296V15.3937ZM2.912 15.3937C2.27025 15.3937 1.75 14.8735 1.75 14.2317H0.25C0.25 15.7019 1.44182 16.8937 2.912 16.8937V15.3937ZM1.75 14.2317V4.67173H0.25V14.2317H1.75ZM1.75 4.67173C1.75 4.02998 2.27024 3.50973 2.912 3.50973V2.00973C1.44182 2.00973 0.25 3.20155 0.25 4.67173H1.75ZM2.912 3.50973H12.1728V2.00973H2.912V3.50973ZM18.958 14.2317V9.10996H17.458V14.2317H18.958ZM0.813283 7.30288L6.95316 10.4751L7.64168 9.14245L1.5018 5.97023L0.813283 7.30288ZM12.5988 10.1588L15.6397 8.12794L14.8066 6.88055L11.7658 8.91139L12.5988 10.1588ZM6.95316 10.4751C7.82539 10.9257 8.40949 11.2377 9.03661 11.3371L9.2714 9.85557C8.93995 9.80304 8.60585 9.6406 7.64168 9.14245L6.95316 10.4751ZM11.7658 8.91139C10.8633 9.51412 10.5494 9.71287 10.2259 9.80209L10.6247 11.2481C11.2368 11.0793 11.7824 10.704 12.5988 10.1588L11.7658 8.91139ZM9.03661 11.3371C9.56626 11.421 10.1077 11.3907 10.6247 11.2481L10.2259 9.80209C9.91519 9.88779 9.58974 9.90602 9.2714 9.85557L9.03661 11.3371ZM15.1191 4.17371L16.7911 5.84578L17.8518 4.78512L16.1797 3.11305L15.1191 4.17371ZM17.8743 5.82224L21.5529 1.80928L20.4471 0.795697L16.7686 4.80866L17.8743 5.82224Z"></path></svg></div></div><div class="h-10 w-10 animate-pulse rounded-full border-1-1/2 bg-slate-100 dark:border-slate-900 dark:bg-slate-800"></div></div></div><div class="mx-auto my-5 flex w-2/3 flex-row items-center justify-center md:hidden"><div class="blog-title text-2xl text-center break-words font-heading font-semibold leading-snug md:font-bold dark:text-white"><a href="index4550.html?source=top_nav_blog_home" class="focus-ring-base flex flex-row items-center focus-ring-colors-base" aria-label="Amit&#x27;s Learning Notes home page">Amit&#x27;s Learning Notes</a></div></div><div class="blog-sub-header mb-4 md:hidden" data-testid="blog-sub-header"><div class="md:(mb-0 ml-auto) flex flex-row items-center justify-center gap-x-3"><div class="flex animate-pulse flex-row items-center rounded-full border-1-1/2 px-4 py-2 text-center text-sm font-medium bg-slate-100 dark:border-slate-900 dark:bg-slate-800"><svg class="invisible mr-2 h-5 w-5" viewBox="0 0 22 22" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.2953 4.58698C13.9301 4.37611 13.683 4.00814 13.626 3.59028L13.4336 2.17939C13.3415 1.50373 12.7644 1 12.0825 1L9.91765 1C9.23573 1 8.65865 1.50373 8.56652 2.17939L8.37412 3.5903C8.31709 4.00849 8.06963 4.37672 7.70395 4.58749L7.0923 4.94003C6.72723 5.15046 6.28538 5.18029 5.89533 5.02084L4.57791 4.48227C3.9467 4.22423 3.22192 4.47214 2.88096 5.06269L1.79854 6.9375C1.45758 7.52805 1.60528 8.27969 2.14436 8.69731L3.26948 9.56895C3.60259 9.82701 3.79768 10.2246 3.79799 10.646L3.7985 11.3519C3.7988 11.774 3.60364 12.1724 3.26999 12.4309L2.14433 13.3029C1.60525 13.7205 1.45755 14.4722 1.79851 15.0627L2.88093 16.9375C3.22189 17.5281 3.94667 17.776 4.57788 17.518L5.89588 16.9792C6.28624 16.8196 6.72847 16.8496 7.0937 17.0604L7.70483 17.4133C8.07006 17.6241 8.31718 17.9921 8.37416 18.41L8.56652 19.8206C8.65865 20.4963 9.23573 21 9.91765 21L12.0825 21C12.7644 21 13.3415 20.4963 13.4336 19.8206L13.626 18.4098C13.683 17.9916 13.9305 17.6234 14.2962 17.4127L14.9077 17.0602C15.2728 16.8497 15.7146 16.8199 16.1047 16.9794L17.4221 17.5179C18.0533 17.776 18.7781 17.5281 19.119 16.9375L20.2015 15.0627C20.5424 14.4721 20.3947 13.7205 19.8557 13.3029L18.7305 12.4312C18.3974 12.1732 18.2023 11.7756 18.202 11.3542L18.2015 10.6483C18.2012 10.2263 18.3964 9.82784 18.73 9.56935L19.8556 8.69734C20.3947 8.27972 20.5424 7.52809 20.2014 6.93753L19.119 5.06272C18.7781 4.47217 18.0533 4.22427 17.4221 4.48231L16.1042 5.02106C15.7138 5.18064 15.2716 5.15063 14.9064 4.93976L14.2953 4.58698Z" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M13.3619 12.3637C12.6088 13.6681 10.9408 14.1151 9.63638 13.362C8.33195 12.6088 7.88502 10.9409 8.63813 9.63643C9.39125 8.33199 11.0592 7.88506 12.3637 8.63818C13.6681 9.39129 14.115 11.0593 13.3619 12.3637Z" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="invisible">Follow</span></div><div class="animate-pulse rounded-full border-1-1/2 p-2 bg-slate-100 dark:border-slate-900 dark:bg-slate-800"><svg class="invisible h-5 w-5" viewBox="0 0 22 17" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.1728 3.50973C12.587 3.50973 12.9228 3.17395 12.9228 2.75973C12.9228 2.34552 12.587 2.00973 12.1728 2.00973V3.50973ZM18.958 9.10996C18.958 8.69575 18.6222 8.35996 18.208 8.35996C17.7938 8.35996 17.458 8.69575 17.458 9.10996H18.958ZM1.5018 5.97023C1.1338 5.7801 0.681352 5.9243 0.491222 6.2923C0.301092 6.66029 0.445283 7.11275 0.813283 7.30288L1.5018 5.97023ZM15.6397 8.12794C15.9841 7.8979 16.0769 7.43217 15.8468 7.08771C15.6168 6.74325 15.1511 6.6505 14.8066 6.88055L15.6397 8.12794ZM7.29742 9.80877L6.95316 10.4751L7.29742 9.80877ZM9.154 10.5963L9.2714 9.85557L9.154 10.5963ZM12.1823 9.53509L11.7658 8.91139L12.1823 9.53509ZM10.4253 10.5251L10.2259 9.80209L10.4253 10.5251ZM16.1797 3.11305C15.8868 2.82016 15.4119 2.82016 15.1191 3.11305C14.8262 3.40595 14.8262 3.88082 15.1191 4.17371L16.1797 3.11305ZM17.3215 5.31545L16.7911 5.84578C16.9358 5.99043 17.1332 6.06972 17.3377 6.06527C17.5423 6.06083 17.7361 5.97304 17.8743 5.82224L17.3215 5.31545ZM21.5529 1.80928C21.8328 1.50394 21.8121 1.02952 21.5068 0.749625C21.2015 0.46973 20.727 0.490357 20.4471 0.795697L21.5529 1.80928ZM17.458 14.2317C17.458 14.8735 16.9378 15.3937 16.296 15.3937V16.8937C17.7662 16.8937 18.958 15.7019 18.958 14.2317H17.458ZM16.296 15.3937H2.912V16.8937H16.296V15.3937ZM2.912 15.3937C2.27025 15.3937 1.75 14.8735 1.75 14.2317H0.25C0.25 15.7019 1.44182 16.8937 2.912 16.8937V15.3937ZM1.75 14.2317V4.67173H0.25V14.2317H1.75ZM1.75 4.67173C1.75 4.02998 2.27024 3.50973 2.912 3.50973V2.00973C1.44182 2.00973 0.25 3.20155 0.25 4.67173H1.75ZM2.912 3.50973H12.1728V2.00973H2.912V3.50973ZM18.958 14.2317V9.10996H17.458V14.2317H18.958ZM0.813283 7.30288L6.95316 10.4751L7.64168 9.14245L1.5018 5.97023L0.813283 7.30288ZM12.5988 10.1588L15.6397 8.12794L14.8066 6.88055L11.7658 8.91139L12.5988 10.1588ZM6.95316 10.4751C7.82539 10.9257 8.40949 11.2377 9.03661 11.3371L9.2714 9.85557C8.93995 9.80304 8.60585 9.6406 7.64168 9.14245L6.95316 10.4751ZM11.7658 8.91139C10.8633 9.51412 10.5494 9.71287 10.2259 9.80209L10.6247 11.2481C11.2368 11.0793 11.7824 10.704 12.5988 10.1588L11.7658 8.91139ZM9.03661 11.3371C9.56626 11.421 10.1077 11.3907 10.6247 11.2481L10.2259 9.80209C9.91519 9.88779 9.58974 9.90602 9.2714 9.85557L9.03661 11.3371ZM15.1191 4.17371L16.7911 5.84578L17.8518 4.78512L16.1797 3.11305L15.1191 4.17371ZM17.8743 5.82224L21.5529 1.80928L20.4471 0.795697L16.7686 4.80866L17.8743 5.82224Z"></path></svg></div></div><div class="mt-6"><div class="blog-social-media-section flex flex-row flex-wrap gap-y-2 justify-center gap-x-1.5 text-slate-700 dark:text-slate-300"><a href="https://amitguptaforwork.github.io/" aria-label="Check out my website, external website, opens in new tab" target="_blank" rel="me noopener ugc nofollow" class="focus-ring-base flex flex-row items-center justify-center rounded-full p-2 transition-colors duration-150 focus-ring-colors-base hover:bg-black/10 dark:hover:bg-white/20"><svg class="h-5 w-5 fill-current" viewBox="0 0 24 24"><path d="M17.9 17.39c-.26-.8-1.01-1.39-1.9-1.39h-1v-3a1 1 0 0 0-1-1H8v-2h2a1 1 0 0 0 1-1V7h2a2 2 0 0 0 2-2v-.41c2.93 1.18 5 4.05 5 7.41 0 2.08-.8 3.97-2.1 5.39M11 19.93c-3.95-.49-7-3.85-7-7.93 0-.62.08-1.22.21-1.79L9 15v1a2 2 0 0 0 2 2m1-16A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10A10 10 0 0 0 12 2z"></path></svg></a><a href="https://www.linkedin.com/in/amitguptaforwork/" aria-label="Find me on LinkedIn, external website, opens in new tab" target="_blank" rel="me noopener ugc nofollow" class="focus-ring-base flex flex-row items-center justify-center rounded-full p-2 transition-colors duration-150 focus-ring-colors-base hover:bg-black/10 dark:hover:bg-white/20"><svg class="h-5 w-5 fill-current" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a><a href="rss.xml" aria-label="Open blog XML Feed, opens in new tab" target="_blank" rel="me noopener" class="focus-ring-base flex flex-row items-center justify-center rounded-full p-2 transition-colors duration-150 focus-ring-colors-base hover:bg-black/10 dark:hover:bg-white/20"><svg class="h-5 w-5 fill-current" viewBox="0 0 448 512"><path d="M80 368c17.645 0 32 14.355 32 32s-14.355 32-32 32-32-14.355-32-32 14.355-32 32-32m0-48c-44.183 0-80 35.817-80 80s35.817 80 80 80 80-35.817 80-80-35.817-80-80-80zm367.996 147.615c-6.449-237.834-198.057-429.163-435.61-435.61C5.609 31.821 0 37.229 0 44.007v24.02c0 6.482 5.147 11.808 11.626 11.992 211.976 6.04 382.316 176.735 388.354 388.354.185 6.479 5.51 11.626 11.992 11.626h24.02c6.78.001 12.187-5.608 12.004-12.384zm-136.239-.05C305.401 305.01 174.966 174.599 12.435 168.243 5.643 167.977 0 173.444 0 180.242v24.024c0 6.431 5.072 11.705 11.497 11.98 136.768 5.847 246.411 115.511 252.258 252.258.275 6.425 5.549 11.497 11.98 11.497h24.024c6.797-.001 12.264-5.644 11.998-12.436z"></path></svg></a></div></div></div></div></header><div class="blog-post-area relative z-40"><main class="blog-post-detail-card pb-24"><article><style>
    [data-rmiz-ghost] {
      position: absolute;
      pointer-events: none;
    }
    [data-rmiz-btn-zoom],
    [data-rmiz-btn-unzoom] {
      background-color: rgba(0, 0, 0, 0.7);
      border-radius: 50%;
      border: none;
      box-shadow: 0 0 1px rgba(255, 255, 255, 0.5);
      color: #fff;
      height: 40px;
      margin: 0;
      outline-offset: 2px;
      padding: 9px;
      touch-action: manipulation;
      width: 40px;
      -webkit-appearance: none;
      -moz-appearance: none;
      appearance: none;
    }
    [data-rmiz-btn-zoom]:not(:focus):not(:active) {
      position: absolute;
      clip: rect(0 0 0 0);
      clip-path: inset(50%);
      height: 1px;
      overflow: hidden;
      pointer-events: none;
      white-space: nowrap;
      width: 1px;
    }
    [data-rmiz-btn-zoom] {
      position: absolute;
      inset: 10px 10px auto auto;
      cursor: zoom-in;
    }
    [data-rmiz-btn-unzoom] {
      position: absolute;
      inset: 20px 20px auto auto;
      cursor: zoom-out;
      z-index: 1;
      display: none;
    }
    [data-rmiz-content="found"] img,
    [data-rmiz-content="found"] svg,
    [data-rmiz-content="found"] [role="img"],
    [data-rmiz-content="found"] [data-zoom] {
      cursor: zoom-in;
    }
    [data-rmiz-modal]::backdrop {
      display: none;
    }
    [data-rmiz-modal][open] {
      position: fixed;
      width: 100vw;
      width: 100dvw;
      height: 100vh;
      height: 100dvh;
      max-width: none;
      max-height: none;
      margin: 0;
      padding: 0;
      border: 0;
      background: transparent;
      overflow: hidden;
    }
    [data-rmiz-modal-overlay] {
      position: absolute;
      inset: 0;
      transition: background-color 0.3s;
    }
    [data-rmiz-modal-overlay="hidden"] {
      background-color: rgba(255, 255, 255, 0);
    }
    [data-rmiz-modal-overlay="visible"] {
      /* This bg color is different from default */
      background-color: rgba(0, 0, 0, 0.5);
    }
    [data-rmiz-modal-content] {
      position: relative;
      width: 100%;
      height: 100%;
    }
    [data-rmiz-modal-img] {
      position: absolute;
      cursor: zoom-out;
      image-rendering: high-quality;
      transform-origin: top left;
      transition: transform 0.3s;
      /* This is added additionally to override prose styles of image*/
      margin: 0 !important;
    }
    @media (prefers-reduced-motion: reduce) {
      [data-rmiz-modal-overlay],
      [data-rmiz-modal-img] {
        transition-duration: 0.01ms !important;
      }
    }
</style><div class="blog-article-page container relative mx-auto grid grid-cols-8"><div class="col-span-full lg:col-span-6 lg:col-start-2"><div class="relative"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:52.5%"></span><img alt="📘 Mcar, Mar, Mnar —" src="../cdn.hashnode.com/res/hashnode/image/upload/v1759335348948/4ead57b0-0b81-454f-a5d5-938972c37be8a723.jpg?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=compress,format&amp;format=webp" decoding="async" data-nimg="responsive" class="mb-0 block w-full" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(https://amitguptaforwork.hashnode.dev/%22https:/cdn.hashnode.com/res/hashnode/image/upload/v1759335348948/4ead57b0-0b81-454f-a5d5-938972c37be8.jpeg?w=400&amp;h=210&amp;fit=crop&amp;crop=entropy&amp;auto=compress,format&amp;format=webp&amp;fm=blurhash%22)"/><noscript><img alt="📘 Mcar, Mar, Mnar —" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="mb-0 block w-full" src="../cdn.hashnode.com/res/hashnode/image/upload/v1759335348948/4ead57b0-0b81-454f-a5d5-938972c37be8a723.jpg?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=compress,format&amp;format=webp"/></noscript></span></div><div class="mt-6 break-words px-4 text-center font-heading text-3xl font-bold text-slate-900 dark:text-white md:mt-10 md:px-5 md:text-4xl lg:px-8 xl:px-20 xl:text-5xl mb-8 md:mb-14"><h1 class="leading-tight" data-query="post-title">📘 Mcar, Mar, Mnar —</h1></div><div class="relative z-20 mb-8 flex flex-row flex-wrap items-center justify-center px-4 md:-mt-7 md:mb-14 md:text-lg last:md:mb-10"><div class="mb-5 flex w-full flex-row items-center justify-center md:mb-0 md:w-auto md:justify-start"><div style="z-index:1" class="overflow-hidden rounded-full  bg-slate-200  dark:bg-white/20 md:mr-3 h-10 w-10 md:h-12 md:w-12"><a href="https://hashnode.com/@learnwithamit" class="relative block h-full w-full"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27200%27%20height=%27200%27/%3e"/></span><img alt="Amit Gupta&#x27;s photo" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" class="relative z-20 block w-full rounded-full" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Amit Gupta&#x27;s photo" loading="lazy" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="relative z-20 block w-full rounded-full" src="../cdn.hashnode.com/res/hashnode/image/upload/v1744625750145/964daf99-0a9d-4c05-89d2-b108baacda11a430.png?w=200&amp;h=200&amp;fit=crop&amp;crop=faces&amp;auto=compress,format&amp;format=webp"/></noscript></span></a></div><a href="https://hashnode.com/@learnwithamit" class="ml-2 font-semibold text-slate-600 dark:text-white md:ml-0"><span>Amit Gupta</span></a></div><div class="mb-5 flex w-full flex-row items-center justify-center md:mb-0 md:w-auto md:justify-start"><span class="mx-3 hidden font-bold text-slate-500 md:block">·</span><a href="mcar-mar-mnar.html" class="tooltip-handle text-slate-600 dark:text-slate-400" data-title="Oct 1, 2025 14:26"><time dateTime="2025-10-01T14:26:36.843Z">Oct 1, 2025</time></a><span class="mx-3 block font-bold text-slate-500">·</span><p class="flex flex-row items-center text-slate-600 dark:text-slate-400"><svg class="mr-2 h-5 w-5 fill-current opacity-75" viewBox="0 0 576 512"><path d="M540.9 56.77c-45.95-16.66-90.23-24.09-129.1-24.75-60.7.94-102.7 17.45-123.8 27.72-21.1-10.27-64.1-26.8-123.2-27.74-40-.05-84.4 8.35-129.7 24.77C14.18 64.33 0 84.41 0 106.7v302.9c0 14.66 6.875 28.06 18.89 36.8 11.81 8.531 26.64 10.98 40.73 6.781 118.9-36.34 209.3 19.05 214.3 22.19C277.8 477.6 281.2 480 287.1 480c6.52 0 10.12-2.373 14.07-4.578 10.78-6.688 98.3-57.66 214.3-22.27 14.11 4.25 28.86 1.75 40.75-6.812C569.1 437.6 576 424.2 576 409.6V106.7c0-22.28-14.2-42.35-35.1-49.93zM272 438.1c-24.95-12.03-71.01-29.37-130.5-29.37-27.83 0-58.5 3.812-91.19 13.77-4.406 1.344-9 .594-12.69-2.047C34.02 417.8 32 413.1 32 409.6V106.7c0-8.859 5.562-16.83 13.86-19.83C87.66 71.7 127.9 63.95 164.5 64c51.8.81 89.7 15.26 107.5 23.66V438.1zm272-28.5c0 4.375-2.016 8.234-5.594 10.84-3.766 2.703-8.297 3.422-12.69 2.125C424.1 391.6 341.3 420.4 304 438.3V87.66c17.8-8.4 55.7-22.85 107.4-23.66 35.31-.063 76.34 7.484 118.8 22.88 8.2 3 13.8 10.96 13.8 19.82v302.9z"></path></svg><span>7<!-- --> min read</span></p></div></div></div></div><div class="blog-content-wrapper article-main-wrapper container relative z-30 mx-auto grid grid-flow-row grid-cols-8 xl:gap-6 2xl:grid-cols-10"><section class="blog-content-main z-20 col-span-8 mb-10 px-4 md:z-10 lg:col-span-6 lg:col-start-2 lg:px-0 xl:col-span-6 xl:col-start-2 2xl:col-span-6 2xl:col-start-3"><div class="relative"><div class="relative w-full overflow-hidden border border-slate-200 bg-white dark:border-slate-800/80 dark:bg-slate-950 mx-auto mb-10 max-w-[812px] rounded-xl"><div class="pr-4 pb-4 max-h-[400px] overflow-hidden"><h2 class="px-6 py-5 pb-4 text-lg font-semibold text-slate-800 dark:text-slate-100"><span>Table of contents</span></h2><ul class="pl-4 dark:border-slate-800"><li><a href="#heading-understanding-the-terms-using-an-analogy-shoppers-amp-shopping-carts-analogy" aria-label="Understanding the Terms Using an Analogy- Shoppers &amp;amp; Shopping Carts Analogy" class="mb-1 flex items-center gap-x-2 rounded-lg px-2 focus:outline-none hover:bg-slate-100 focus:bg-slate-100 dark:hover:bg-slate-800 dark:focus:bg-slate-800"><div id="0d54f408-d734-4f19-ae67-76fd78baf26b" class="w-full break-words py-2 text-base focus:outline-none text-slate-700  dark:text-slate-200">Understanding the Terms Using an Analogy- Shoppers &amp; Shopping Carts Analogy</div></a><ul class="pl-4 dark:border-slate-800"><li><a href="#heading-1-mcar-missing-completely-at-random" aria-label="1. MCAR – Missing Completely At Random" class="mb-1 flex items-center gap-x-2 rounded-lg px-2 focus:outline-none hover:bg-slate-100 focus:bg-slate-100 dark:hover:bg-slate-800 dark:focus:bg-slate-800 -mt-1"><span class="text-slate-400 dark:text-slate-500"><svg class="h-4 w-4 stroke-current" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M6.6665 4L10.6665 8L6.6665 12"></path></svg></span><div id="84251c14-d5ea-4069-a4c2-7e803e38760e" class="w-full break-words py-2 text-base focus:outline-none text-slate-600 dark:text-slate-300">1. MCAR – Missing Completely At Random</div></a></li><li><a href="#heading-2-mar-missing-at-random" aria-label="2. MAR – Missing At Random" class="mb-1 flex items-center gap-x-2 rounded-lg px-2 focus:outline-none hover:bg-slate-100 focus:bg-slate-100 dark:hover:bg-slate-800 dark:focus:bg-slate-800 -mt-1"><span class="text-slate-400 dark:text-slate-500"><svg class="h-4 w-4 stroke-current" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M6.6665 4L10.6665 8L6.6665 12"></path></svg></span><div id="27d4fa19-f1be-44bd-a38d-47acb0026af2" class="w-full break-words py-2 text-base focus:outline-none text-slate-600 dark:text-slate-300">2. MAR – Missing At Random</div></a></li><li><a href="#heading-3-mnar-missing-not-at-random" aria-label="3. MNAR – Missing Not At Random" class="mb-1 flex items-center gap-x-2 rounded-lg px-2 focus:outline-none hover:bg-slate-100 focus:bg-slate-100 dark:hover:bg-slate-800 dark:focus:bg-slate-800 -mt-1"><span class="text-slate-400 dark:text-slate-500"><svg class="h-4 w-4 stroke-current" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M6.6665 4L10.6665 8L6.6665 12"></path></svg></span><div id="924bf17c-929c-4081-ab52-82014f57551f" class="w-full break-words py-2 text-base focus:outline-none text-slate-600 dark:text-slate-300">3. MNAR – Missing Not At Random</div></a></li><li><a href="#heading-recap" aria-label="📝 Recap" class="mb-1 flex items-center gap-x-2 rounded-lg px-2 focus:outline-none hover:bg-slate-100 focus:bg-slate-100 dark:hover:bg-slate-800 dark:focus:bg-slate-800 -mt-1"><span class="text-slate-400 dark:text-slate-500"><svg class="h-4 w-4 stroke-current" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M6.6665 4L10.6665 8L6.6665 12"></path></svg></span><div id="cb081174-cd0f-4a49-bbca-9e7ae6a215f1" class="w-full break-words py-2 text-base focus:outline-none text-slate-600 dark:text-slate-300">📝 Recap</div></a></li></ul></li><li><a href="#heading-how-to-check-programmatically" aria-label="How To Check Programmatically" class="mb-1 flex items-center gap-x-2 rounded-lg px-2 focus:outline-none hover:bg-slate-100 focus:bg-slate-100 dark:hover:bg-slate-800 dark:focus:bg-slate-800"><div id="d502bfec-4f5a-4afd-ac1a-782a847d858b" class="w-full break-words py-2 text-base focus:outline-none text-slate-700  dark:text-slate-200">How To Check Programmatically</div></a><ul class="pl-4 dark:border-slate-800"><li><a href="#heading-testing-mcar-missing-completely-at-random" aria-label="Testing MCAR (Missing Completely At Random)" class="mb-1 flex items-center gap-x-2 rounded-lg px-2 focus:outline-none hover:bg-slate-100 focus:bg-slate-100 dark:hover:bg-slate-800 dark:focus:bg-slate-800 -mt-1"><span class="text-slate-400 dark:text-slate-500"><svg class="h-4 w-4 stroke-current" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M6.6665 4L10.6665 8L6.6665 12"></path></svg></span><div id="72c60015-95be-4d18-8d21-363a08fa58ce" class="w-full break-words py-2 text-base focus:outline-none text-slate-600 dark:text-slate-300">Testing MCAR (Missing Completely At Random)</div></a><ul class="pl-4 dark:border-slate-800"><li><a href="#heading-littles-mcar-test" aria-label="Little’s MCAR test" class="mb-1 flex items-center gap-x-2 rounded-lg px-2 focus:outline-none hover:bg-slate-100 focus:bg-slate-100 dark:hover:bg-slate-800 dark:focus:bg-slate-800 -mt-1"><span class="text-slate-400 dark:text-slate-500"><svg class="h-4 w-4 stroke-current" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M6.6665 4L10.6665 8L6.6665 12"></path></svg></span><div id="f498281d-b432-4f82-b897-61fe79ece742" class="w-full break-words py-2 text-base focus:outline-none text-slate-600 dark:text-slate-300">Little’s MCAR test</div></a></li><li><a href="#heading-cross-tab-with-target-chi-square" aria-label="Cross-tab with Target + Chi-square" class="mb-1 flex items-center gap-x-2 rounded-lg px-2 focus:outline-none hover:bg-slate-100 focus:bg-slate-100 dark:hover:bg-slate-800 dark:focus:bg-slate-800 -mt-1"><span class="text-slate-400 dark:text-slate-500"><svg class="h-4 w-4 stroke-current" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M6.6665 4L10.6665 8L6.6665 12"></path></svg></span><div id="c75365a6-f524-4f99-a806-c5bc7ae26187" class="w-full break-words py-2 text-base focus:outline-none text-slate-600 dark:text-slate-300">Cross-tab with Target + Chi-square</div></a></li><li><a href="#heading-compare-distributions-manually" aria-label="Compare distributions manually" class="mb-1 flex items-center gap-x-2 rounded-lg px-2 focus:outline-none hover:bg-slate-100 focus:bg-slate-100 dark:hover:bg-slate-800 dark:focus:bg-slate-800 -mt-1"><span class="text-slate-400 dark:text-slate-500"><svg class="h-4 w-4 stroke-current" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M6.6665 4L10.6665 8L6.6665 12"></path></svg></span><div id="af722907-885e-4267-a27d-61d6a5fd6330" class="w-full break-words py-2 text-base focus:outline-none text-slate-600 dark:text-slate-300">Compare distributions manually</div></a></li></ul></li><li><a href="#heading-testing-for-mar-missing-at-random" aria-label="Testing for MAR (Missing At Random)" class="mb-1 flex items-center gap-x-2 rounded-lg px-2 focus:outline-none hover:bg-slate-100 focus:bg-slate-100 dark:hover:bg-slate-800 dark:focus:bg-slate-800 -mt-1"><span class="text-slate-400 dark:text-slate-500"><svg class="h-4 w-4 stroke-current" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M6.6665 4L10.6665 8L6.6665 12"></path></svg></span><div id="a257c107-58dc-4add-8786-637ea8cc5224" class="w-full break-words py-2 text-base focus:outline-none text-slate-600 dark:text-slate-300">Testing for MAR (Missing At Random)</div></a><ul class="pl-4 dark:border-slate-800"><li><a href="#heading-diagnosing-mnar-missing-not-at-random" aria-label="Diagnosing MNAR (Missing Not At Random)" class="mb-1 flex items-center gap-x-2 rounded-lg px-2 focus:outline-none hover:bg-slate-100 focus:bg-slate-100 dark:hover:bg-slate-800 dark:focus:bg-slate-800 -mt-1"><span class="text-slate-400 dark:text-slate-500"><svg class="h-4 w-4 stroke-current" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M6.6665 4L10.6665 8L6.6665 12"></path></svg></span><div id="2022af84-7ec0-4ae1-920e-54922b26faf3" class="w-full break-words py-2 text-base focus:outline-none text-slate-600 dark:text-slate-300">Diagnosing MNAR (Missing Not At Random)</div></a></li></ul></li></ul></li><li><a href="#heading-handling-missing-data-mechanisms" aria-label="🛠 Handling Missing Data Mechanisms" class="mb-1 flex items-center gap-x-2 rounded-lg px-2 focus:outline-none hover:bg-slate-100 focus:bg-slate-100 dark:hover:bg-slate-800 dark:focus:bg-slate-800"><div id="681eba31-4e59-46ff-bd96-a3857f93a4fe" class="w-full break-words py-2 text-base focus:outline-none text-slate-700  dark:text-slate-200">🛠 Handling Missing Data Mechanisms</div></a><ul class="pl-4 dark:border-slate-800"><li><a href="#heading-1-mcar" aria-label="1. MCAR" class="mb-1 flex items-center gap-x-2 rounded-lg px-2 focus:outline-none hover:bg-slate-100 focus:bg-slate-100 dark:hover:bg-slate-800 dark:focus:bg-slate-800 -mt-1"><span class="text-slate-400 dark:text-slate-500"><svg class="h-4 w-4 stroke-current" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M6.6665 4L10.6665 8L6.6665 12"></path></svg></span><div id="255f15b0-6d3e-4af5-b590-1600cc1e474e" class="w-full break-words py-2 text-base focus:outline-none text-slate-600 dark:text-slate-300">1. MCAR</div></a></li><li><a href="#heading-2-mar" aria-label="2. MAR" class="mb-1 flex items-center gap-x-2 rounded-lg px-2 focus:outline-none hover:bg-slate-100 focus:bg-slate-100 dark:hover:bg-slate-800 dark:focus:bg-slate-800 -mt-1"><span class="text-slate-400 dark:text-slate-500"><svg class="h-4 w-4 stroke-current" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M6.6665 4L10.6665 8L6.6665 12"></path></svg></span><div id="5e02dd39-772e-4db6-b37e-41765e33d8d0" class="w-full break-words py-2 text-base focus:outline-none text-slate-600 dark:text-slate-300">2. MAR</div></a></li><li><a href="#heading-step-by-step-intuition" aria-label="Step-by-step intuition:" class="mb-1 flex items-center gap-x-2 rounded-lg px-2 focus:outline-none hover:bg-slate-100 focus:bg-slate-100 dark:hover:bg-slate-800 dark:focus:bg-slate-800 -mt-1"><span class="text-slate-400 dark:text-slate-500"><svg class="h-4 w-4 stroke-current" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M6.6665 4L10.6665 8L6.6665 12"></path></svg></span><div id="55cec363-4166-4e25-b34f-a01021ec1593" class="w-full break-words py-2 text-base focus:outline-none text-slate-600 dark:text-slate-300">Step-by-step intuition:</div></a></li><li><a href="#heading-3-mnar" aria-label="3. MNAR" class="mb-1 flex items-center gap-x-2 rounded-lg px-2 focus:outline-none hover:bg-slate-100 focus:bg-slate-100 dark:hover:bg-slate-800 dark:focus:bg-slate-800 -mt-1"><span class="text-slate-400 dark:text-slate-500"><svg class="h-4 w-4 stroke-current" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M6.6665 4L10.6665 8L6.6665 12"></path></svg></span><div id="43c07aa0-e737-4b4c-83d3-5b157798fe91" class="w-full break-words py-2 text-base focus:outline-none text-slate-600 dark:text-slate-300">3. MNAR</div></a></li></ul></li></ul></div></div><div id="post-content-parent" class="relative mb-10 pb-14"><div id="post-content-wrapper" class="prose prose-base mx-auto mb-10 min-h-30 break-words dark:prose-dark lg:prose-lg"><h1 id="heading-understanding-the-terms-using-an-analogy-shoppers-amp-shopping-carts-analogy">Understanding the Terms Using an Analogy- <strong>Shoppers &amp; Shopping Carts Analogy</strong></h1>
<h2 id="heading-1-mcar-missing-completely-at-random">1. MCAR – Missing Completely At Random</h2>
<p><strong>Story:</strong> You’re analyzing what items shoppers add to their online carts. Suddenly, due to a system glitch, a random subset of cart items (across all shoppers) <strong>fails to get recorded</strong>. The missingness has nothing to do with what the items are or who the shopper is — just bad luck</p>
<p><strong>Key idea:</strong> Missingness is completely random. Dropping the missing data still gives you a fair picture of what people buy.</p>
<p>💡 Real-world analogy: A lab machine randomly crashes and corrupts some test results.</p>
<h2 id="heading-2-mar-missing-at-random">2. MAR – Missing At Random</h2>
<p><strong>Story:</strong> Suppose items added using a <strong>mobile app</strong> are sometimes not saved to the cart due to slower connections. But <strong>within the mobile shoppers</strong>, whether an item is missing doesn’t depend on the type of item itself — only on the platform used.</p>
<p><strong>Key idea:</strong> Missingness depends on an <strong>observed factor</strong> (web vs mobile), but not on the item itself. If you know the shopper’s platform, you can correct for this bias.</p>
<p>💡 Real-world analogy: Older survey respondents are less likely to report income → depends on age (observed), not income value itself.</p>
<h2 id="heading-3-mnar-missing-not-at-random">3. MNAR – Missing Not At Random</h2>
<p><strong>Story:</strong> Some shoppers deliberately <strong>remove expensive luxury items</strong> from their cart before checking out, because they’re embarrassed or may not be able to afford them. The chance of an item being missing directly depends on the item’s <strong>price</strong> itself (higher price → more likely to go missing).</p>
<p><strong>Key idea:</strong> Missingness depends <strong>directly on the unseen value</strong> (price of item missing). Hardest case to handle, because the missingness itself hides something important.</p>
<p>💡 Real-world analogy: High-income people avoid disclosing income → missingness depends on the true income itself.</p>
<h2 id="heading-recap">📝 Recap</h2>
<ul>
<li><p><strong>MCAR:</strong> Missing values are caused by pure chance → <em>A random system glitch deletes some cart items</em> 🌐 — purely random.</p>
<p>  <strong>MAR:</strong> Missingness depends on other observed variables, not the missing value itself → <em>Mobile app shoppers lose items due to bad connection</em> 📱 — depends on platform (observed), not the item itself.</p>
<p>  <strong>MNAR:</strong> Missingness depends directly on the missing value itself → <em>Shoppers deliberately remove very expensive items</em> 💎 — depends on the value (price) that’s missing.</p>
</li>
</ul>
<p>👉 Think: <strong>MCAR = random glitch 💨, MAR = depends on something you can observe 👀, MNAR = depends on what’s hidden 🔒.</strong></p>
<h1 id="heading-how-to-check-programmatically">How To Check Programmatically</h1>
<h2 id="heading-testing-mcar-missing-completely-at-random"><strong>Testing MCAR (Missing Completely At Random)</strong></h2>
<h3 id="heading-littles-mcar-test">Little’s MCAR test</h3>
<ul>
<li><p>ONLY FOR NUMERIC COLUMNS ☹</p>
</li>
<li><p><strong>Purpose</strong>: Global test for whether data are Missing Completely At Random (MCAR) across the whole dataset, considering multivariate patterns of missingness.</p>
</li>
<li><p><strong>Interpretation</strong>:</p>
</li>
<li><ul>
<li><ul>
<li><p>High p value → cannot reject MCAR (missingness is plausibly random).</p>
<ul>
<li><p>Low p value → reject MCAR (missingness related to data → MAR or MNAR).</p>
</li>
<li><p><strong>Strength</strong>: Multivariate test, designed for exactly this situation.</p>
<ul>
<li><strong>Weakness</strong>: Sensitive to sample size, not always available in every package.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code class="lang-python"><span class="hljs-comment"># 1. LITTLE’S MCAR TEST ---------------------------------------</span>
<span class="hljs-comment"># note: not natively in statsmodels yet (as of v0.14)</span>
<span class="hljs-comment"># package option: `little-mcar-test`</span>
<span class="hljs-comment"># install: pip install little-mcar-test</span>

<span class="hljs-keyword">from</span> little_mcar_test <span class="hljs-keyword">import</span> mcartest

<span class="hljs-comment"># run on numeric part of your dataset</span>
numeric_df = df.select_dtypes(include=[np.number])
stat, dof, p_value = mcartest(numeric_df.to_numpy())
print(<span class="hljs-string">f"Little’s MCAR test: χ2 = <span class="hljs-subst">{stat}</span>, dof = <span class="hljs-subst">{dof}</span>, p = <span class="hljs-subst">{p_value}</span>"</span>)

<span class="hljs-keyword">if</span> p_value &gt; <span class="hljs-number">0.05</span>:
    print(<span class="hljs-string">"Fail to reject MCAR → data plausibly MCAR"</span>)
<span class="hljs-keyword">else</span>:
    print(<span class="hljs-string">"Reject MCAR → data likely MAR or MNAR"</span>)
</code></pre>
<h3 id="heading-cross-tab-with-target-chi-square">Cross-tab with Target + Chi-square</h3>
<ul>
<li><p>We create a new column first (a “missingness indicator”)</p>
</li>
<li><p>Cross-tabulate this with the target variable and run a chi-square test of independence.</p>
</li>
</ul>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> chi2_contingency
<span class="hljs-comment"># heuristic check: does missingness in one col depend on another categorical col?</span>

df[<span class="hljs-string">'col_missing'</span>] = df[<span class="hljs-string">'some_feature'</span>].isna().astype(int)
ctab = pd.crosstab(df[<span class="hljs-string">'col_missing'</span>], df[<span class="hljs-string">'target'</span>])
chi2, p, dof, exp = chi2_contingency(ctab)
print(<span class="hljs-string">"p-value ="</span>, p)
</code></pre>
<ul>
<li><p><strong>Interpretation</strong>:</p>
<ul>
<li><p>If p &lt; 0.05 → missingness of this feature is associated with the target.</p>
<ul>
<li>That suggests missingness carries information → at least MAR, possibly MNAR.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Strength</strong>: Very intuitive for predictive modeling (if missingness depends on target → definitely not MCAR).</p>
</li>
<li><p><strong>Weakness</strong>:</p>
<ul>
<li><p>Only checks relation with target, not with all other features.</p>
<ul>
<li>So you might miss situations where missingness is related to a predictor but not the target.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="heading-compare-distributions-manually">Compare distributions manually</h3>
<ul>
<li>Examine whether the distribution of observed variables differs between rows with and without missing values (should not).</li>
</ul>
<pre><code class="lang-python">df.groupby(df[<span class="hljs-string">'col_with_missing'</span>].isna())[<span class="hljs-string">'other_col'</span>].mean()
</code></pre>
<h2 id="heading-testing-for-mar-missing-at-random"><strong>Testing for MAR (Missing At Random)</strong></h2>
<ul>
<li><ul>
<li><p>Strict tests don’t exist (because MAR involves unobserved missing values).</p>
<ul>
<li><p><strong>Practical check</strong>: Create "missingness indicator" (flag if a variable is missing) and test correlation with other observed variables.</p>
</li>
<li><pre><code class="lang-python">    df[<span class="hljs-string">'age_missing'</span>] = df[<span class="hljs-string">'age'</span>].isna()

    <span class="hljs-comment">#We want to compare age with gender to assess if missing age </span>
    <span class="hljs-comment">#is related to gender value.</span>
    <span class="hljs-comment">#As we are grouping by gender and then calculating mean of the age_missing column, </span>
    <span class="hljs-comment">#this mean equals the proportion of missing ages in that group.</span>
    df.groupby(<span class="hljs-string">'gender'</span>)[<span class="hljs-string">'age_missing'</span>].mean()
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>What this tells you</strong></p>
</li>
<li><p>The result is the missingness rate of the <code>age</code> column for each gender:</p>
<ul>
<li><p>If the value is 0.0 for a gender, no missing ages in that group.</p>
</li>
<li><p>If the value is 1.0 for a gender, all ages are missing in that group.</p>
</li>
<li><p>Values between 0 and 1 indicate the fraction of records with missing age within that gender group.</p>
</li>
</ul>
</li>
</ul>
<h3 id="heading-diagnosing-mnar-missing-not-at-random"><strong>Diagnosing MNAR (Missing Not At Random)</strong></h3>
<ul>
<li><p>True MNAR is <strong>untestable from the data alone</strong> (since it depends on the unobserved value).</p>
</li>
<li><p>Requires:<br />  • Domain expertise (e.g., we know high salaries are underreported).<br />  • <strong>Sensitivity analysis</strong>: Assume plausible MNAR mechanisms and check how conclusions change.<br />  • Specialized models (selection models, Heckman correction, pattern-mixture models).</p>
</li>
</ul>
<h1 id="heading-handling-missing-data-mechanisms"><strong>🛠 Handling Missing Data Mechanisms</strong></h1>
<h3 id="heading-1-mcar"><strong>1. MCAR</strong></h3>
<p>➡️ <strong>Safe to drop rows or columns</strong> (analysis unbiased, only reduced sample size).<br />Options:</p>
<pre><code class="lang-python">df_clean = df.dropna()                     <span class="hljs-comment"># drop rows</span>
df_clean = df.dropna(axis=<span class="hljs-number">1</span>, thresh=<span class="hljs-number">0.7</span>*len(df))  <span class="hljs-comment"># drop columns with &gt;30% missing</span>
</code></pre>
<p>Or simply use mean/median imputation without worrying about bias.</p>
<hr />
<h3 id="heading-2-mar"><strong>2. MAR</strong></h3>
<p>➡️ <strong>Best handled by imputation methods that leverage observed data</strong>.<br />Options:</p>
<ul>
<li><p><strong>Group-wise imputation</strong>:</p>
<ul>
<li><p>We fill missing values in the income column by gender, using each gender's median income. It then stores the result back in the income column.</p>
<pre><code class="lang-python">  df[<span class="hljs-string">'income'</span>] = df.groupby(<span class="hljs-string">'gender'</span>)[<span class="hljs-string">'income'</span>].transform(
      <span class="hljs-keyword">lambda</span> x: x.fillna(x.median()))
</code></pre>
</li>
</ul>
</li>
<li><p><strong>Multiple Imputation (MICE/Iterative)</strong>:</p>
<ul>
<li><p>Use the other features to predict missing values, one column at a time, in a round-robin fashion</p>
<pre><code class="lang-python">  <span class="hljs-keyword">from</span> sklearn.experimental <span class="hljs-keyword">import</span> enable_iterative_imputer
  <span class="hljs-keyword">from</span> sklearn.impute <span class="hljs-keyword">import</span> IterativeImputer
  imp = IterativeImputer(random_state=<span class="hljs-number">0</span>)
  df[numeric_cols] = imp.fit_transform(df[numeric_cols])
</code></pre>
<ul>
<li><p><strong>How IterativeImputer works (high-level)</strong></p>
<ul>
<li><p>It treats each feature with missing values as the target and uses the other features as predictors.</p>
</li>
<li><p>It iteratively:</p>
<ol>
<li><p>Regresses the missing values of one feature on the others.</p>
</li>
<li><p>Replaces missing values with the predicted ones.</p>
</li>
<li><p>Moves to the next feature with missing values and repeats.</p>
</li>
</ol>
</li>
<li><p>This process continues for several iterations until convergence, producing several imputed values that reflect potential relationships among features.</p>
</li>
<li><h3 id="heading-step-by-step-intuition">Step-by-step intuition:</h3>
<ol>
<li><p><strong>Start with missing values</strong><br /> You have a dataset where some entries are <code>NaN</code>.</p>
<p> Example:</p>
<pre><code class="lang-python"> Age   Salary   Experience
 <span class="hljs-number">25</span>    <span class="hljs-number">50</span>k      <span class="hljs-number">2</span>
 NaN   <span class="hljs-number">60</span>k      <span class="hljs-number">3</span>
 <span class="hljs-number">30</span>    NaN      <span class="hljs-number">4</span>
 <span class="hljs-number">28</span>    <span class="hljs-number">55</span>k      NaN
</code></pre>
</li>
<li><p><strong>Choose one column with missing values (say Age).</strong><br /> Treat <em>Age</em> as the "target variable" and the other columns (Salary, Experience) as predictors.</p>
</li>
<li><p><strong>Train a regression model</strong><br /> Use the rows where <em>Age</em> is known to train a model like:</p>
<pre><code class="lang-python"> Age ~ Salary + Experience
</code></pre>
</li>
<li><p><strong>Predict missing values for Age</strong><br /> Use the trained model to fill in the <code>NaN</code>s in Age.</p>
</li>
<li><p><strong>Move to the next column with missing values (say Salary)</strong><br /> Now treat <em>Salary</em> as the target and use Age and Experience (with the newly imputed values) as predictors to estimate missing Salary.</p>
</li>
<li><p><strong>Repeat for all columns with missing values.</strong></p>
</li>
<li><p><strong>Iterate multiple times</strong><br /> Because each column’s imputation depends on the others (which may also be imputed), the algorithm cycles through all columns multiple times. With each iteration, the imputations become more stable and consistent.</p>
<p> That’s why it’s called <strong>Iterative</strong> Imputer.</p>
</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>            <strong>When to use</strong></p>
<ul>
<li><p>When you believe there are inter-feature relationships and you want a more sophisticated imputation than simple statistics (e.g., mean/median).</p>
</li>
<li><p>For datasets where the pattern of missingness might depend on other observed features.</p>
</li>
</ul>
<p>            <strong>Notes and best practices</strong></p>
<ul>
<li><p>Ensure numeric_cols is a list of the numeric column names you want to impute.</p>
</li>
<li><p>You can customize the underlying estimator (e.g., linear regression, decision trees) by passing estimator= to the IterativeImputer.</p>
</li>
<li><p>Be mindful of potential data leakage: fit on training data only, and apply the same transformation to validation/test data.</p>
</li>
</ul>
<ul>
<li><p><strong>Add missing indicator features</strong>:</p>
<pre><code class="lang-python">  df[<span class="hljs-string">'income_missing'</span>] = df[<span class="hljs-string">'income'</span>].isna().astype(int)
</code></pre>
</li>
</ul>
<hr />
<h3 id="heading-3-mnar"><strong>3. MNAR</strong></h3>
<p>➡️ <strong>Hardest case — no purely data-driven solution.</strong><br />Typical strategies:</p>
<ul>
<li><p><strong>Domain-informed imputation</strong> (use expert rules, external benchmarks).</p>
</li>
<li><p><strong>Sensitivity analysis</strong>: Try imputations under different assumptions (e.g., assume the missing group is 10% higher/lower).</p>
</li>
<li><p><strong>Heckman correction / selection models</strong> (statsmodels has limited support).</p>
</li>
<li><p><strong>Pattern mixture models / Bayesian methods</strong>.</p>
</li>
</ul>
<p>👉 Often you need to:<br />A. Report that data may be MNAR.<br />B. Perform robustness checks (see if conclusions hold under different plausible imputations).</p>
</div></div><div class="mb-5 flex w-full flex-row flex-wrap justify-center md:mb-0"><a class="mb-3 mr-3 rounded-lg border bg-slate-100 px-2 py-1 text-base font-medium text-slate-700 hover:bg-slate-200 dark:border-slate-800 dark:bg-slate-800 dark:text-slate-100 dark:hover:bg-slate-700" href="tag/data-analysis5255.html?source=tags_bottom_blogs"><span>data analysis</span></a><a class="mb-3 mr-3 rounded-lg border bg-slate-100 px-2 py-1 text-base font-medium text-slate-700 hover:bg-slate-200 dark:border-slate-800 dark:bg-slate-800 dark:text-slate-100 dark:hover:bg-slate-700" href="tag/analogy5255.html?source=tags_bottom_blogs"><span>analogy</span></a><a class="mb-3 mr-3 rounded-lg border bg-slate-100 px-2 py-1 text-base font-medium text-slate-700 hover:bg-slate-200 dark:border-slate-800 dark:bg-slate-800 dark:text-slate-100 dark:hover:bg-slate-700" href="tag/missing-data5255.html?source=tags_bottom_blogs"><span>missing data</span></a></div></div></section></div><div class="absolute h-px w-px overflow-hidden" id="refNode1" style="top:100px;left:100px"> </div><div class="absolute left-0 top-0 h-px w-px overflow-hidden" id="refNode2"></div><div class="absolute z-50 mt-4 hidden"><div class="flex flex-row items-center rounded-lg border border-slate-300 bg-white p-4 text-slate-800 shadow-lg dark:border-white dark:bg-slate-800 dark:text-slate-300"><span class="mr-3 block">Share this</span><a href="https://twitter.com/share?url=https%3A%2F%2Famitguptaforwork.hashnode.dev%2Fmcar-mar-mnar&amp;text=%20%40learnwithamit" class="rounded-full border border-transparent py-1 font-medium text-slate-700 dark:text-slate-200 hover:bg-slate-200 disabled:opacity-50 hover:dark:bg-slate-700 flex flex-row items-center focus:outline-none px-2 text-sm" variant="transparent" target="_blank" rel="noopener"><svg class="h-6 w-6 stroke-current" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.643 13.346L4.26862 4.86856C3.85863 4.32329 4.2478 3.54408 4.93001 3.54431L7.2184 3.54508C7.47633 3.54517 7.71945 3.66557 7.87585 3.87066L12.9065 10.4675M10.643 13.346L5.19311 20.5093M10.643 13.346L15.8028 20.077C15.9588 20.2805 16.2003 20.4001 16.4567 20.4009L18.7925 20.4082C19.4778 20.4104 19.8683 19.6261 19.4536 19.0805L12.9065 10.4675M12.9065 10.4675L18.2181 3.50928" stroke-width="1.5" stroke-linecap="round"></path></svg></a><a href="http://www.reddit.com/submit?title=%F0%9F%93%98%20Mcar%2C%20Mar%2C%20Mnar%20%E2%80%94&amp;selftext=true&amp;text=%20https%3A%2F%2Famitguptaforwork.hashnode.dev%2Fmcar-mar-mnar" class="rounded-full border border-transparent py-1 font-medium hover:bg-slate-200 disabled:opacity-50 hover:dark:bg-slate-700 flex flex-row items-center focus:outline-none px-2 text-sm text-red-600 dark:text-red-600" variant="transparent" target="_blank" rel="noopener"><svg class="h-6 w-6 fill-current" viewBox="0 0 512 512"><path d="M201.5 305.5c-13.8 0-24.9-11.1-24.9-24.6 0-13.8 11.1-24.9 24.9-24.9 13.6 0 24.6 11.1 24.6 24.9 0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4 0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7 0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9 0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5 0 52.6 59.2 95.2 132 95.2 73.1 0 132.3-42.6 132.3-95.2 0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6 0-2.2-2.2-6.1-2.2-8.3 0-2.5 2.5-2.5 6.4 0 8.6 22.8 22.8 87.3 22.8 110.2 0 2.5-2.2 2.5-6.1 0-8.6-2.2-2.2-6.1-2.2-8.3 0zm7.7-75c-13.6 0-24.6 11.1-24.6 24.9 0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.1 24.9-24.6 0-13.8-11-24.9-24.9-24.9z"></path></svg></a><button class="rounded-full border border-transparent py-1 font-medium text-slate-700 dark:text-slate-200 hover:bg-slate-200 disabled:opacity-50 hover:dark:bg-slate-700 flex flex-row items-center focus:outline-none px-2 text-sm" type="button" variant="transparent" data-clipboard-text=" https://amitguptaforwork.hashnode.dev/mcar-mar-mnar" id="text-sharer"><svg class="h-6 w-6 fill-current" viewBox="0 0 384 512"><path d="M336 64h-88.6c.4-2.6.6-5.3.6-8 0-30.9-25.1-56-56-56s-56 25.1-56 56c0 2.7.2 5.4.6 8H48C21.5 64 0 85.5 0 112v352c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V112c0-26.5-21.5-48-48-48zM192 32c13.3 0 24 10.7 24 24s-10.7 24-24 24-24-10.7-24-24 10.7-24 24-24zm160 432c0 8.8-7.2 16-16 16H48c-8.8 0-16-7.2-16-16V112c0-8.8 7.2-16 16-16h48v20c0 6.6 5.4 12 12 12h168c6.6 0 12-5.4 12-12V96h48c8.8 0 16 7.2 16 16z"></path></svg></button></div></div><div class="container relative z-20 mx-auto grid grid-flow-row grid-cols-8 xl:gap-6 2xl:grid-cols-10"><div class="blog-comments-section-wrapper col-span-8 px-4 lg:col-span-6 lg:col-start-2 lg:px-0 xl:col-span-6 xl:col-start-2 2xl:col-span-6 2xl:col-start-3"></div></div></article></main></div></div></div><script type="text/javascript">
              var SUPPORTS_PASSIVE = false;
              try {
                var opts = Object.defineProperty({}, 'passive', {
                  get: function() {
                    SUPPORTS_PASSIVE = true;
                  }
                });
                window.addEventListener("testPassive", null, opts);
                window.removeEventListener("testPassive", null, opts);
              } catch (e) {}
            </script><script type="text/javascript">
              // Array.prototype.flat polyfill
              if (!Array.prototype.flat) {
                // eslint-disable-next-line no-extend-native
                Object.defineProperty(Array.prototype, 'flat', {
                  configurable: true,
                  writable: true,
                  value() {
                    // eslint-disable-next-line prefer-rest-params
                    const depth = typeof arguments[0] === 'undefined' ? 1 : Number(arguments[0]) || 0;
                    const result = [];
                    const { forEach } = result;

                    // eslint-disable-next-line no-var
                    var flatDeep = function (arr, depth) {
                      forEach.call(arr, (val) => {
                        if (depth > 0 && Array.isArray(val)) {
                          flatDeep(val, depth - 1);
                        } else {
                          result.push(val);
                        }
                      });
                    };

                    flatDeep(this, depth);
                    return result;
                  },
                });
              }
            </script><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":"{\"_id\":\"68dd3a1cf0b5f3410cada41b\",\"partOfPublication\":true,\"author\":{\"_id\":\"67fce0562e7f83246dceb67d\",\"name\":\"Amit Gupta\",\"photo\":\"https://cdn.hashnode.com/res/hashnode/image/upload/v1744625750145/964daf99-0a9d-4c05-89d2-b108baacda11.png\",\"username\":\"learnwithamit\",\"bio\":\"\",\"socialMedia\":{},\"isDeactivated\":false},\"bookmarkedIn\":[],\"publication\":{\"_id\":\"68dd2c04a3a82b4b10e631ed\",\"author\":{\"_id\":\"67fce0562e7f83246dceb67d\",\"name\":\"Amit Gupta\",\"photo\":\"https://cdn.hashnode.com/res/hashnode/image/upload/v1744625750145/964daf99-0a9d-4c05-89d2-b108baacda11.png\",\"username\":\"learnwithamit\"},\"badgePageEnabled\":true,\"description\":\"Just wanted to share my notes as I learn new exciting things\\n\",\"domain\":\"\",\"domainStatus\":{},\"wwwPrefixedDomainStatus\":{},\"customCSSEnabled\":false,\"customCSSPublished\":{\"homeMin\":\"\",\"postMin\":\"\",\"staticMin\":\"\"},\"customRules\":[],\"darkModeEnabled\":false,\"darkModeLogo\":\"\",\"disableFooterBranding\":false,\"isSubscriptionModalDisabled\":false,\"publicMembersCount\":1,\"displayTitle\":\"\",\"favicon\":\"\",\"gaTrackingID\":\"\",\"gTagManagerID\":\"\",\"hasBadges\":false,\"headerColor\":\"\",\"hideMembersPage\":false,\"isTeam\":true,\"layout\":\"grid\",\"membersPageEnabled\":true,\"menu\":[],\"metaHTML\":\"\u003cp\u003eJust wanted to share my notes as I learn new exciting things\u003c/p\u003e\\n\",\"metaHTMLSanitized\":\"Just wanted to share my notes as I learn new exciting things\\n\",\"newsletterEnabled\":true,\"proTeamEnabled\":false,\"newsletterPageEnabled\":false,\"ogImage\":\"\",\"logo\":\"\",\"textSelectionSharerEnabled\":true,\"title\":\"Amit's Learning Notes\",\"urlPattern\":\"simple\",\"username\":\"amitguptaforwork\",\"viewCountVisible\":false,\"readTimeHidden\":false,\"links\":{\"twitter\":\"\",\"instagram\":\"\",\"github\":\"\",\"website\":\"https://amitguptaforwork.github.io/\",\"hashnode\":\"\",\"youtube\":\"\",\"dailydev\":\"\",\"linkedin\":\"https://www.linkedin.com/in/amitguptaforwork/\",\"mastodon\":\"\",\"facebook\":\"\"},\"numPosts\":2,\"sponsorship\":{\"content\":\"\",\"contentMarkdown\":\"\"},\"allowContributorEdits\":true,\"allowCrawlingByGPT\":false},\"tags\":[{\"_id\":\"56744722958ef13879b951ac\",\"slug\":\"data-analysis\",\"name\":\"data analysis\",\"isActive\":true,\"isApproved\":true},{\"_id\":\"63f336bb322fd23a23d459f7\",\"slug\":\"analogy\",\"name\":\"analogy\",\"isActive\":true,\"isApproved\":true},{\"_id\":\"62e40f16430bd1d04a83efb7\",\"slug\":\"missing-data\",\"name\":\"missing data\",\"isActive\":true,\"isApproved\":true}],\"coAuthors\":[],\"responseCount\":0,\"replyCount\":0,\"contentMarkdown\":\"# Understanding the Terms Using an Analogy- **Shoppers \u0026 Shopping Carts Analogy**\\n\\n## 1\\\\. MCAR – Missing Completely At Random\\n\\n**Story:** You’re analyzing what items shoppers add to their online carts. Suddenly, due to a system glitch, a random subset of cart items (across all shoppers) **fails to get recorded**. The missingness has nothing to do with what the items are or who the shopper is — just bad luck\\n\\n**Key idea:** Missingness is completely random. Dropping the missing data still gives you a fair picture of what people buy.\\n\\n💡 Real-world analogy: A lab machine randomly crashes and corrupts some test results.\\n\\n## 2\\\\. MAR – Missing At Random\\n\\n**Story:** Suppose items added using a **mobile app** are sometimes not saved to the cart due to slower connections. But **within the mobile shoppers**, whether an item is missing doesn’t depend on the type of item itself — only on the platform used.\\n\\n**Key idea:** Missingness depends on an **observed factor** (web vs mobile), but not on the item itself. If you know the shopper’s platform, you can correct for this bias.\\n\\n💡 Real-world analogy: Older survey respondents are less likely to report income → depends on age (observed), not income value itself.\\n\\n## 3\\\\. MNAR – Missing Not At Random\\n\\n**Story:** Some shoppers deliberately **remove expensive luxury items** from their cart before checking out, because they’re embarrassed or may not be able to afford them. The chance of an item being missing directly depends on the item’s **price** itself (higher price → more likely to go missing).\\n\\n**Key idea:** Missingness depends **directly on the unseen value** (price of item missing). Hardest case to handle, because the missingness itself hides something important.\\n\\n💡 Real-world analogy: High-income people avoid disclosing income → missingness depends on the true income itself.\\n\\n## 📝 Recap\\n\\n* **MCAR:** Missing values are caused by pure chance → *A random system glitch deletes some cart items* 🌐 — purely random.\\n    \\n    **MAR:** Missingness depends on other observed variables, not the missing value itself → *Mobile app shoppers lose items due to bad connection* 📱 — depends on platform (observed), not the item itself.\\n    \\n    **MNAR:** Missingness depends directly on the missing value itself → *Shoppers deliberately remove very expensive items* 💎 — depends on the value (price) that’s missing.\\n    \\n\\n👉 Think: **MCAR = random glitch 💨, MAR = depends on something you can observe 👀, MNAR = depends on what’s hidden 🔒.**\\n\\n# How To Check Programmatically\\n\\n## **Testing MCAR (Missing Completely At Random)**\\n\\n### Little’s MCAR test\\n\\n* ONLY FOR NUMERIC COLUMNS ☹\\n    \\n* **Purpose**: Global test for whether data are Missing Completely At Random (MCAR) across the whole dataset, considering multivariate patterns of missingness.\\n    \\n* **Interpretation**:\\n    \\n* * * High p value → cannot reject MCAR (missingness is plausibly random).\\n            \\n            * Low p value → reject MCAR (missingness related to data → MAR or MNAR).\\n                \\n            * **Strength**: Multivariate test, designed for exactly this situation.\\n                \\n        * **Weakness**: Sensitive to sample size, not always available in every package.\\n            \\n\\n```python\\n# 1. LITTLE’S MCAR TEST ---------------------------------------\\n# note: not natively in statsmodels yet (as of v0.14)\\n# package option: `little-mcar-test`\\n# install: pip install little-mcar-test\\n\\nfrom little_mcar_test import mcartest\\n\\n# run on numeric part of your dataset\\nnumeric_df = df.select_dtypes(include=[np.number])\\nstat, dof, p_value = mcartest(numeric_df.to_numpy())\\nprint(f\\\"Little’s MCAR test: χ2 = {stat}, dof = {dof}, p = {p_value}\\\")\\n\\nif p_value \u003e 0.05:\\n    print(\\\"Fail to reject MCAR → data plausibly MCAR\\\")\\nelse:\\n    print(\\\"Reject MCAR → data likely MAR or MNAR\\\")\\n```\\n\\n### Cross-tab with Target + Chi-square\\n\\n* We create a new column first (a “missingness indicator”)\\n    \\n* Cross-tabulate this with the target variable and run a chi-square test of independence.\\n    \\n\\n```python\\nimport pandas as pd\\nfrom scipy.stats import chi2_contingency\\n# heuristic check: does missingness in one col depend on another categorical col?\\n\\ndf['col_missing'] = df['some_feature'].isna().astype(int)\\nctab = pd.crosstab(df['col_missing'], df['target'])\\nchi2, p, dof, exp = chi2_contingency(ctab)\\nprint(\\\"p-value =\\\", p)\\n```\\n\\n* **Interpretation**:\\n    \\n    * If p \u0026lt; 0.05 → missingness of this feature is associated with the target.\\n        \\n        * That suggests missingness carries information → at least MAR, possibly MNAR.\\n            \\n* **Strength**: Very intuitive for predictive modeling (if missingness depends on target → definitely not MCAR).\\n    \\n* **Weakness**:\\n    \\n    * Only checks relation with target, not with all other features.\\n        \\n        * So you might miss situations where missingness is related to a predictor but not the target.\\n            \\n\\n### Compare distributions manually\\n\\n* Examine whether the distribution of observed variables differs between rows with and without missing values (should not).\\n    \\n\\n```python\\ndf.groupby(df['col_with_missing'].isna())['other_col'].mean()\\n```\\n\\n## **Testing for MAR (Missing At Random)**\\n\\n* * Strict tests don’t exist (because MAR involves unobserved missing values).\\n        \\n        * **Practical check**: Create \\\"missingness indicator\\\" (flag if a variable is missing) and test correlation with other observed variables.\\n            \\n        * ```python\\n              df['age_missing'] = df['age'].isna()\\n              \\n              #We want to compare age with gender to assess if missing age \\n              #is related to gender value.\\n              #As we are grouping by gender and then calculating mean of the age_missing column, \\n              #this mean equals the proportion of missing ages in that group.\\n              df.groupby('gender')['age_missing'].mean()\\n            ```\\n            \\n* **What this tells you**\\n    \\n* The result is the missingness rate of the `age` column for each gender:\\n    \\n    * If the value is 0.0 for a gender, no missing ages in that group.\\n        \\n    * If the value is 1.0 for a gender, all ages are missing in that group.\\n        \\n    * Values between 0 and 1 indicate the fraction of records with missing age within that gender group.\\n        \\n\\n### **Diagnosing MNAR (Missing Not At Random)**\\n\\n* True MNAR is **untestable from the data alone** (since it depends on the unobserved value).\\n    \\n* Requires:  \\n    • Domain expertise (e.g., we know high salaries are underreported).  \\n    • **Sensitivity analysis**: Assume plausible MNAR mechanisms and check how conclusions change.  \\n    • Specialized models (selection models, Heckman correction, pattern-mixture models).\\n    \\n\\n# **🛠 Handling Missing Data Mechanisms**\\n\\n### **1\\\\. MCAR**\\n\\n➡️ **Safe to drop rows or columns** (analysis unbiased, only reduced sample size).  \\nOptions:\\n\\n```python\\ndf_clean = df.dropna()                     # drop rows\\ndf_clean = df.dropna(axis=1, thresh=0.7*len(df))  # drop columns with \u003e30% missing\\n```\\n\\nOr simply use mean/median imputation without worrying about bias.\\n\\n---\\n\\n### **2\\\\. MAR**\\n\\n➡️ **Best handled by imputation methods that leverage observed data**.  \\nOptions:\\n\\n* **Group-wise imputation**:\\n    \\n    * We fill missing values in the income column by gender, using each gender's median income. It then stores the result back in the income column.\\n        \\n        ```python\\n        df['income'] = df.groupby('gender')['income'].transform(\\n            lambda x: x.fillna(x.median()))\\n        ```\\n        \\n* **Multiple Imputation (MICE/Iterative)**:\\n    \\n    * Use the other features to predict missing values, one column at a time, in a round-robin fashion\\n        \\n        ```python\\n        from sklearn.experimental import enable_iterative_imputer\\n        from sklearn.impute import IterativeImputer\\n        imp = IterativeImputer(random_state=0)\\n        df[numeric_cols] = imp.fit_transform(df[numeric_cols])\\n        ```\\n        \\n        * **How IterativeImputer works (high-level)**\\n            \\n            * It treats each feature with missing values as the target and uses the other features as predictors.\\n                \\n            * It iteratively:\\n                \\n                1. Regresses the missing values of one feature on the others.\\n                    \\n                2. Replaces missing values with the predicted ones.\\n                    \\n                3. Moves to the next feature with missing values and repeats.\\n                    \\n            * This process continues for several iterations until convergence, producing several imputed values that reflect potential relationships among features.\\n                \\n            * ### Step-by-step intuition:\\n                \\n                1. **Start with missing values**  \\n                    You have a dataset where some entries are `NaN`.\\n                    \\n                    Example:\\n                    \\n                    ```python\\n                    Age   Salary   Experience\\n                    25    50k      2\\n                    NaN   60k      3\\n                    30    NaN      4\\n                    28    55k      NaN\\n                    ```\\n                    \\n                2. **Choose one column with missing values (say Age).**  \\n                    Treat *Age* as the \\\"target variable\\\" and the other columns (Salary, Experience) as predictors.\\n                    \\n                3. **Train a regression model**  \\n                    Use the rows where *Age* is known to train a model like:\\n                    \\n                    ```python\\n                    Age ~ Salary + Experience\\n                    ```\\n                    \\n                4. **Predict missing values for Age**  \\n                    Use the trained model to fill in the `NaN`s in Age.\\n                    \\n                5. **Move to the next column with missing values (say Salary)**  \\n                    Now treat *Salary* as the target and use Age and Experience (with the newly imputed values) as predictors to estimate missing Salary.\\n                    \\n                6. **Repeat for all columns with missing values.**\\n                    \\n                7. **Iterate multiple times**  \\n                    Because each column’s imputation depends on the others (which may also be imputed), the algorithm cycles through all columns multiple times. With each iteration, the imputations become more stable and consistent.\\n                    \\n                    That’s why it’s called **Iterative** Imputer.\\n                    \\n                \\n            \\n            **When to use**\\n            \\n            * When you believe there are inter-feature relationships and you want a more sophisticated imputation than simple statistics (e.g., mean/median).\\n                \\n            * For datasets where the pattern of missingness might depend on other observed features.\\n                \\n            \\n            **Notes and best practices**\\n            \\n            * Ensure numeric\\\\_cols is a list of the numeric column names you want to impute.\\n                \\n            * You can customize the underlying estimator (e.g., linear regression, decision trees) by passing estimator= to the IterativeImputer.\\n                \\n            * Be mindful of potential data leakage: fit on training data only, and apply the same transformation to validation/test data.\\n                \\n* **Add missing indicator features**:\\n    \\n    ```python\\n    df['income_missing'] = df['income'].isna().astype(int)\\n    ```\\n    \\n\\n---\\n\\n### **3\\\\. MNAR**\\n\\n➡️ **Hardest case — no purely data-driven solution.**  \\nTypical strategies:\\n\\n* **Domain-informed imputation** (use expert rules, external benchmarks).\\n    \\n* **Sensitivity analysis**: Try imputations under different assumptions (e.g., assume the missing group is 10% higher/lower).\\n    \\n* **Heckman correction / selection models** (statsmodels has limited support).\\n    \\n* **Pattern mixture models / Bayesian methods**.\\n    \\n\\n👉 Often you need to:  \\nA. Report that data may be MNAR.  \\nB. Perform robustness checks (see if conclusions hold under different plausible imputations).\",\"content\":\"\u003ch1 id=\\\"heading-understanding-the-terms-using-an-analogy-shoppers-amp-shopping-carts-analogy\\\"\u003eUnderstanding the Terms Using an Analogy- \u003cstrong\u003eShoppers \u0026amp; Shopping Carts Analogy\u003c/strong\u003e\u003c/h1\u003e\\n\u003ch2 id=\\\"heading-1-mcar-missing-completely-at-random\\\"\u003e1. MCAR – Missing Completely At Random\u003c/h2\u003e\\n\u003cp\u003e\u003cstrong\u003eStory:\u003c/strong\u003e You’re analyzing what items shoppers add to their online carts. Suddenly, due to a system glitch, a random subset of cart items (across all shoppers) \u003cstrong\u003efails to get recorded\u003c/strong\u003e. The missingness has nothing to do with what the items are or who the shopper is — just bad luck\u003c/p\u003e\\n\u003cp\u003e\u003cstrong\u003eKey idea:\u003c/strong\u003e Missingness is completely random. Dropping the missing data still gives you a fair picture of what people buy.\u003c/p\u003e\\n\u003cp\u003e💡 Real-world analogy: A lab machine randomly crashes and corrupts some test results.\u003c/p\u003e\\n\u003ch2 id=\\\"heading-2-mar-missing-at-random\\\"\u003e2. MAR – Missing At Random\u003c/h2\u003e\\n\u003cp\u003e\u003cstrong\u003eStory:\u003c/strong\u003e Suppose items added using a \u003cstrong\u003emobile app\u003c/strong\u003e are sometimes not saved to the cart due to slower connections. But \u003cstrong\u003ewithin the mobile shoppers\u003c/strong\u003e, whether an item is missing doesn’t depend on the type of item itself — only on the platform used.\u003c/p\u003e\\n\u003cp\u003e\u003cstrong\u003eKey idea:\u003c/strong\u003e Missingness depends on an \u003cstrong\u003eobserved factor\u003c/strong\u003e (web vs mobile), but not on the item itself. If you know the shopper’s platform, you can correct for this bias.\u003c/p\u003e\\n\u003cp\u003e💡 Real-world analogy: Older survey respondents are less likely to report income → depends on age (observed), not income value itself.\u003c/p\u003e\\n\u003ch2 id=\\\"heading-3-mnar-missing-not-at-random\\\"\u003e3. MNAR – Missing Not At Random\u003c/h2\u003e\\n\u003cp\u003e\u003cstrong\u003eStory:\u003c/strong\u003e Some shoppers deliberately \u003cstrong\u003eremove expensive luxury items\u003c/strong\u003e from their cart before checking out, because they’re embarrassed or may not be able to afford them. The chance of an item being missing directly depends on the item’s \u003cstrong\u003eprice\u003c/strong\u003e itself (higher price → more likely to go missing).\u003c/p\u003e\\n\u003cp\u003e\u003cstrong\u003eKey idea:\u003c/strong\u003e Missingness depends \u003cstrong\u003edirectly on the unseen value\u003c/strong\u003e (price of item missing). Hardest case to handle, because the missingness itself hides something important.\u003c/p\u003e\\n\u003cp\u003e💡 Real-world analogy: High-income people avoid disclosing income → missingness depends on the true income itself.\u003c/p\u003e\\n\u003ch2 id=\\\"heading-recap\\\"\u003e📝 Recap\u003c/h2\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eMCAR:\u003c/strong\u003e Missing values are caused by pure chance → \u003cem\u003eA random system glitch deletes some cart items\u003c/em\u003e 🌐 — purely random.\u003c/p\u003e\\n\u003cp\u003e  \u003cstrong\u003eMAR:\u003c/strong\u003e Missingness depends on other observed variables, not the missing value itself → \u003cem\u003eMobile app shoppers lose items due to bad connection\u003c/em\u003e 📱 — depends on platform (observed), not the item itself.\u003c/p\u003e\\n\u003cp\u003e  \u003cstrong\u003eMNAR:\u003c/strong\u003e Missingness depends directly on the missing value itself → \u003cem\u003eShoppers deliberately remove very expensive items\u003c/em\u003e 💎 — depends on the value (price) that’s missing.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003e👉 Think: \u003cstrong\u003eMCAR = random glitch 💨, MAR = depends on something you can observe 👀, MNAR = depends on what’s hidden 🔒.\u003c/strong\u003e\u003c/p\u003e\\n\u003ch1 id=\\\"heading-how-to-check-programmatically\\\"\u003eHow To Check Programmatically\u003c/h1\u003e\\n\u003ch2 id=\\\"heading-testing-mcar-missing-completely-at-random\\\"\u003e\u003cstrong\u003eTesting MCAR (Missing Completely At Random)\u003c/strong\u003e\u003c/h2\u003e\\n\u003ch3 id=\\\"heading-littles-mcar-test\\\"\u003eLittle’s MCAR test\u003c/h3\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eONLY FOR NUMERIC COLUMNS ☹\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003ePurpose\u003c/strong\u003e: Global test for whether data are Missing Completely At Random (MCAR) across the whole dataset, considering multivariate patterns of missingness.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eInterpretation\u003c/strong\u003e:\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cul\u003e\\n\u003cli\u003e\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eHigh p value → cannot reject MCAR (missingness is plausibly random).\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eLow p value → reject MCAR (missingness related to data → MAR or MNAR).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eStrength\u003c/strong\u003e: Multivariate test, designed for exactly this situation.\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cstrong\u003eWeakness\u003c/strong\u003e: Sensitive to sample size, not always available in every package.\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003e\u003cspan class=\\\"hljs-comment\\\"\u003e# 1. LITTLE’S MCAR TEST ---------------------------------------\u003c/span\u003e\\n\u003cspan class=\\\"hljs-comment\\\"\u003e# note: not natively in statsmodels yet (as of v0.14)\u003c/span\u003e\\n\u003cspan class=\\\"hljs-comment\\\"\u003e# package option: `little-mcar-test`\u003c/span\u003e\\n\u003cspan class=\\\"hljs-comment\\\"\u003e# install: pip install little-mcar-test\u003c/span\u003e\\n\\n\u003cspan class=\\\"hljs-keyword\\\"\u003efrom\u003c/span\u003e little_mcar_test \u003cspan class=\\\"hljs-keyword\\\"\u003eimport\u003c/span\u003e mcartest\\n\\n\u003cspan class=\\\"hljs-comment\\\"\u003e# run on numeric part of your dataset\u003c/span\u003e\\nnumeric_df = df.select_dtypes(include=[np.number])\\nstat, dof, p_value = mcartest(numeric_df.to_numpy())\\nprint(\u003cspan class=\\\"hljs-string\\\"\u003ef\\\"Little’s MCAR test: χ2 = \u003cspan class=\\\"hljs-subst\\\"\u003e{stat}\u003c/span\u003e, dof = \u003cspan class=\\\"hljs-subst\\\"\u003e{dof}\u003c/span\u003e, p = \u003cspan class=\\\"hljs-subst\\\"\u003e{p_value}\u003c/span\u003e\\\"\u003c/span\u003e)\\n\\n\u003cspan class=\\\"hljs-keyword\\\"\u003eif\u003c/span\u003e p_value \u0026gt; \u003cspan class=\\\"hljs-number\\\"\u003e0.05\u003c/span\u003e:\\n    print(\u003cspan class=\\\"hljs-string\\\"\u003e\\\"Fail to reject MCAR → data plausibly MCAR\\\"\u003c/span\u003e)\\n\u003cspan class=\\\"hljs-keyword\\\"\u003eelse\u003c/span\u003e:\\n    print(\u003cspan class=\\\"hljs-string\\\"\u003e\\\"Reject MCAR → data likely MAR or MNAR\\\"\u003c/span\u003e)\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003ch3 id=\\\"heading-cross-tab-with-target-chi-square\\\"\u003eCross-tab with Target + Chi-square\u003c/h3\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eWe create a new column first (a “missingness indicator”)\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eCross-tabulate this with the target variable and run a chi-square test of independence.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003e\u003cspan class=\\\"hljs-keyword\\\"\u003eimport\u003c/span\u003e pandas \u003cspan class=\\\"hljs-keyword\\\"\u003eas\u003c/span\u003e pd\\n\u003cspan class=\\\"hljs-keyword\\\"\u003efrom\u003c/span\u003e scipy.stats \u003cspan class=\\\"hljs-keyword\\\"\u003eimport\u003c/span\u003e chi2_contingency\\n\u003cspan class=\\\"hljs-comment\\\"\u003e# heuristic check: does missingness in one col depend on another categorical col?\u003c/span\u003e\\n\\ndf[\u003cspan class=\\\"hljs-string\\\"\u003e'col_missing'\u003c/span\u003e] = df[\u003cspan class=\\\"hljs-string\\\"\u003e'some_feature'\u003c/span\u003e].isna().astype(int)\\nctab = pd.crosstab(df[\u003cspan class=\\\"hljs-string\\\"\u003e'col_missing'\u003c/span\u003e], df[\u003cspan class=\\\"hljs-string\\\"\u003e'target'\u003c/span\u003e])\\nchi2, p, dof, exp = chi2_contingency(ctab)\\nprint(\u003cspan class=\\\"hljs-string\\\"\u003e\\\"p-value =\\\"\u003c/span\u003e, p)\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eInterpretation\u003c/strong\u003e:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eIf p \u0026lt; 0.05 → missingness of this feature is associated with the target.\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003eThat suggests missingness carries information → at least MAR, possibly MNAR.\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eStrength\u003c/strong\u003e: Very intuitive for predictive modeling (if missingness depends on target → definitely not MCAR).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eWeakness\u003c/strong\u003e:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eOnly checks relation with target, not with all other features.\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003eSo you might miss situations where missingness is related to a predictor but not the target.\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003ch3 id=\\\"heading-compare-distributions-manually\\\"\u003eCompare distributions manually\u003c/h3\u003e\\n\u003cul\u003e\\n\u003cli\u003eExamine whether the distribution of observed variables differs between rows with and without missing values (should not).\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003edf.groupby(df[\u003cspan class=\\\"hljs-string\\\"\u003e'col_with_missing'\u003c/span\u003e].isna())[\u003cspan class=\\\"hljs-string\\\"\u003e'other_col'\u003c/span\u003e].mean()\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003ch2 id=\\\"heading-testing-for-mar-missing-at-random\\\"\u003e\u003cstrong\u003eTesting for MAR (Missing At Random)\u003c/strong\u003e\u003c/h2\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eStrict tests don’t exist (because MAR involves unobserved missing values).\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003ePractical check\u003c/strong\u003e: Create \\\"missingness indicator\\\" (flag if a variable is missing) and test correlation with other observed variables.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003e    df[\u003cspan class=\\\"hljs-string\\\"\u003e'age_missing'\u003c/span\u003e] = df[\u003cspan class=\\\"hljs-string\\\"\u003e'age'\u003c/span\u003e].isna()\\n\\n    \u003cspan class=\\\"hljs-comment\\\"\u003e#We want to compare age with gender to assess if missing age \u003c/span\u003e\\n    \u003cspan class=\\\"hljs-comment\\\"\u003e#is related to gender value.\u003c/span\u003e\\n    \u003cspan class=\\\"hljs-comment\\\"\u003e#As we are grouping by gender and then calculating mean of the age_missing column, \u003c/span\u003e\\n    \u003cspan class=\\\"hljs-comment\\\"\u003e#this mean equals the proportion of missing ages in that group.\u003c/span\u003e\\n    df.groupby(\u003cspan class=\\\"hljs-string\\\"\u003e'gender'\u003c/span\u003e)[\u003cspan class=\\\"hljs-string\\\"\u003e'age_missing'\u003c/span\u003e].mean()\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eWhat this tells you\u003c/strong\u003e\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eThe result is the missingness rate of the \u003ccode\u003eage\u003c/code\u003e column for each gender:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eIf the value is 0.0 for a gender, no missing ages in that group.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eIf the value is 1.0 for a gender, all ages are missing in that group.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eValues between 0 and 1 indicate the fraction of records with missing age within that gender group.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003ch3 id=\\\"heading-diagnosing-mnar-missing-not-at-random\\\"\u003e\u003cstrong\u003eDiagnosing MNAR (Missing Not At Random)\u003c/strong\u003e\u003c/h3\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eTrue MNAR is \u003cstrong\u003euntestable from the data alone\u003c/strong\u003e (since it depends on the unobserved value).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eRequires:\u003cbr /\u003e  • Domain expertise (e.g., we know high salaries are underreported).\u003cbr /\u003e  • \u003cstrong\u003eSensitivity analysis\u003c/strong\u003e: Assume plausible MNAR mechanisms and check how conclusions change.\u003cbr /\u003e  • Specialized models (selection models, Heckman correction, pattern-mixture models).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003ch1 id=\\\"heading-handling-missing-data-mechanisms\\\"\u003e\u003cstrong\u003e🛠 Handling Missing Data Mechanisms\u003c/strong\u003e\u003c/h1\u003e\\n\u003ch3 id=\\\"heading-1-mcar\\\"\u003e\u003cstrong\u003e1. MCAR\u003c/strong\u003e\u003c/h3\u003e\\n\u003cp\u003e➡️ \u003cstrong\u003eSafe to drop rows or columns\u003c/strong\u003e (analysis unbiased, only reduced sample size).\u003cbr /\u003eOptions:\u003c/p\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003edf_clean = df.dropna()                     \u003cspan class=\\\"hljs-comment\\\"\u003e# drop rows\u003c/span\u003e\\ndf_clean = df.dropna(axis=\u003cspan class=\\\"hljs-number\\\"\u003e1\u003c/span\u003e, thresh=\u003cspan class=\\\"hljs-number\\\"\u003e0.7\u003c/span\u003e*len(df))  \u003cspan class=\\\"hljs-comment\\\"\u003e# drop columns with \u0026gt;30% missing\u003c/span\u003e\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eOr simply use mean/median imputation without worrying about bias.\u003c/p\u003e\\n\u003chr /\u003e\\n\u003ch3 id=\\\"heading-2-mar\\\"\u003e\u003cstrong\u003e2. MAR\u003c/strong\u003e\u003c/h3\u003e\\n\u003cp\u003e➡️ \u003cstrong\u003eBest handled by imputation methods that leverage observed data\u003c/strong\u003e.\u003cbr /\u003eOptions:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eGroup-wise imputation\u003c/strong\u003e:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eWe fill missing values in the income column by gender, using each gender's median income. It then stores the result back in the income column.\u003c/p\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003e  df[\u003cspan class=\\\"hljs-string\\\"\u003e'income'\u003c/span\u003e] = df.groupby(\u003cspan class=\\\"hljs-string\\\"\u003e'gender'\u003c/span\u003e)[\u003cspan class=\\\"hljs-string\\\"\u003e'income'\u003c/span\u003e].transform(\\n      \u003cspan class=\\\"hljs-keyword\\\"\u003elambda\u003c/span\u003e x: x.fillna(x.median()))\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eMultiple Imputation (MICE/Iterative)\u003c/strong\u003e:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eUse the other features to predict missing values, one column at a time, in a round-robin fashion\u003c/p\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003e  \u003cspan class=\\\"hljs-keyword\\\"\u003efrom\u003c/span\u003e sklearn.experimental \u003cspan class=\\\"hljs-keyword\\\"\u003eimport\u003c/span\u003e enable_iterative_imputer\\n  \u003cspan class=\\\"hljs-keyword\\\"\u003efrom\u003c/span\u003e sklearn.impute \u003cspan class=\\\"hljs-keyword\\\"\u003eimport\u003c/span\u003e IterativeImputer\\n  imp = IterativeImputer(random_state=\u003cspan class=\\\"hljs-number\\\"\u003e0\u003c/span\u003e)\\n  df[numeric_cols] = imp.fit_transform(df[numeric_cols])\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eHow IterativeImputer works (high-level)\u003c/strong\u003e\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eIt treats each feature with missing values as the target and uses the other features as predictors.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eIt iteratively:\u003c/p\u003e\\n\u003col\u003e\\n\u003cli\u003e\u003cp\u003eRegresses the missing values of one feature on the others.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eReplaces missing values with the predicted ones.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eMoves to the next feature with missing values and repeats.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ol\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eThis process continues for several iterations until convergence, producing several imputed values that reflect potential relationships among features.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003ch3 id=\\\"heading-step-by-step-intuition\\\"\u003eStep-by-step intuition:\u003c/h3\u003e\\n\u003col\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eStart with missing values\u003c/strong\u003e\u003cbr /\u003e You have a dataset where some entries are \u003ccode\u003eNaN\u003c/code\u003e.\u003c/p\u003e\\n\u003cp\u003e Example:\u003c/p\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003e Age   Salary   Experience\\n \u003cspan class=\\\"hljs-number\\\"\u003e25\u003c/span\u003e    \u003cspan class=\\\"hljs-number\\\"\u003e50\u003c/span\u003ek      \u003cspan class=\\\"hljs-number\\\"\u003e2\u003c/span\u003e\\n NaN   \u003cspan class=\\\"hljs-number\\\"\u003e60\u003c/span\u003ek      \u003cspan class=\\\"hljs-number\\\"\u003e3\u003c/span\u003e\\n \u003cspan class=\\\"hljs-number\\\"\u003e30\u003c/span\u003e    NaN      \u003cspan class=\\\"hljs-number\\\"\u003e4\u003c/span\u003e\\n \u003cspan class=\\\"hljs-number\\\"\u003e28\u003c/span\u003e    \u003cspan class=\\\"hljs-number\\\"\u003e55\u003c/span\u003ek      NaN\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eChoose one column with missing values (say Age).\u003c/strong\u003e\u003cbr /\u003e Treat \u003cem\u003eAge\u003c/em\u003e as the \\\"target variable\\\" and the other columns (Salary, Experience) as predictors.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eTrain a regression model\u003c/strong\u003e\u003cbr /\u003e Use the rows where \u003cem\u003eAge\u003c/em\u003e is known to train a model like:\u003c/p\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003e Age ~ Salary + Experience\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003ePredict missing values for Age\u003c/strong\u003e\u003cbr /\u003e Use the trained model to fill in the \u003ccode\u003eNaN\u003c/code\u003es in Age.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eMove to the next column with missing values (say Salary)\u003c/strong\u003e\u003cbr /\u003e Now treat \u003cem\u003eSalary\u003c/em\u003e as the target and use Age and Experience (with the newly imputed values) as predictors to estimate missing Salary.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eRepeat for all columns with missing values.\u003c/strong\u003e\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eIterate multiple times\u003c/strong\u003e\u003cbr /\u003e Because each column’s imputation depends on the others (which may also be imputed), the algorithm cycles through all columns multiple times. With each iteration, the imputations become more stable and consistent.\u003c/p\u003e\\n\u003cp\u003e That’s why it’s called \u003cstrong\u003eIterative\u003c/strong\u003e Imputer.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ol\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003e            \u003cstrong\u003eWhen to use\u003c/strong\u003e\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eWhen you believe there are inter-feature relationships and you want a more sophisticated imputation than simple statistics (e.g., mean/median).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eFor datasets where the pattern of missingness might depend on other observed features.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003e            \u003cstrong\u003eNotes and best practices\u003c/strong\u003e\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eEnsure numeric_cols is a list of the numeric column names you want to impute.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eYou can customize the underlying estimator (e.g., linear regression, decision trees) by passing estimator= to the IterativeImputer.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eBe mindful of potential data leakage: fit on training data only, and apply the same transformation to validation/test data.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eAdd missing indicator features\u003c/strong\u003e:\u003c/p\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003e  df[\u003cspan class=\\\"hljs-string\\\"\u003e'income_missing'\u003c/span\u003e] = df[\u003cspan class=\\\"hljs-string\\\"\u003e'income'\u003c/span\u003e].isna().astype(int)\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003chr /\u003e\\n\u003ch3 id=\\\"heading-3-mnar\\\"\u003e\u003cstrong\u003e3. MNAR\u003c/strong\u003e\u003c/h3\u003e\\n\u003cp\u003e➡️ \u003cstrong\u003eHardest case — no purely data-driven solution.\u003c/strong\u003e\u003cbr /\u003eTypical strategies:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eDomain-informed imputation\u003c/strong\u003e (use expert rules, external benchmarks).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eSensitivity analysis\u003c/strong\u003e: Try imputations under different assumptions (e.g., assume the missing group is 10% higher/lower).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eHeckman correction / selection models\u003c/strong\u003e (statsmodels has limited support).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003ePattern mixture models / Bayesian methods\u003c/strong\u003e.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003e👉 Often you need to:\u003cbr /\u003eA. Report that data may be MNAR.\u003cbr /\u003eB. Perform robustness checks (see if conclusions hold under different plausible imputations).\u003c/p\u003e\\n\",\"cuid\":\"cmg82xvor000002ifbt0ccbob\",\"views\":14,\"title\":\"📘 Mcar, Mar, Mnar —\",\"slug\":\"mcar-mar-mnar\",\"dateAdded\":\"2025-10-01T14:26:36.843Z\",\"dateUpdated\":\"2025-10-01T16:16:07.545Z\",\"type\":\"story\",\"coverImage\":\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759335348948/4ead57b0-0b81-454f-a5d5-938972c37be8.jpeg\",\"isCoverImagePortrait\":false,\"isCoverAttributionHidden\":false,\"brief\":\"Understanding the Terms Using an Analogy- Shoppers \u0026 Shopping Carts Analogy\\n1. MCAR – Missing Completely At Random\\nStory: You’re analyzing what items shoppers add to their online carts. Suddenly, due to a system glitch, a random subset of cart items ...\",\"isFollowing\":false,\"totalReactions\":0,\"totalReactionsByCurrentUser\":0,\"series\":null,\"isPinnedToBlog\":false,\"readTime\":7,\"sB\":false,\"isAMA\":false,\"subtitle\":\"\",\"isPartOfSeries\":false,\"hasTags\":true,\"ogImage\":\"\",\"metaTitle\":\"\",\"metaDescription\":\"\",\"isRepublished\":false,\"autoPublishedFromRSS\":false,\"responses\":[],\"isFeatured\":false,\"hasLatex\":false,\"stickCoverToBottom\":false,\"hideBadges\":false,\"badges\":[],\"isDelisted\":false,\"audioUrls\":{},\"disableComments\":false,\"enableToc\":true,\"toc\":[[{\"id\":\"0d54f408-d734-4f19-ae67-76fd78baf26b\",\"level\":1,\"slug\":\"understanding-the-terms-using-an-analogy-shoppers-amp-shopping-carts-analogy\",\"title\":\"Understanding the Terms Using an Analogy- Shoppers \u0026amp; Shopping Carts Analogy\",\"parentId\":null}],[{\"id\":\"84251c14-d5ea-4069-a4c2-7e803e38760e\",\"level\":2,\"slug\":\"1-mcar-missing-completely-at-random\",\"title\":\"1. MCAR – Missing Completely At Random\",\"parentId\":\"0d54f408-d734-4f19-ae67-76fd78baf26b\"}],[{\"id\":\"27d4fa19-f1be-44bd-a38d-47acb0026af2\",\"level\":2,\"slug\":\"2-mar-missing-at-random\",\"title\":\"2. MAR – Missing At Random\",\"parentId\":\"0d54f408-d734-4f19-ae67-76fd78baf26b\"}],[{\"id\":\"924bf17c-929c-4081-ab52-82014f57551f\",\"level\":2,\"slug\":\"3-mnar-missing-not-at-random\",\"title\":\"3. MNAR – Missing Not At Random\",\"parentId\":\"0d54f408-d734-4f19-ae67-76fd78baf26b\"}],[{\"id\":\"cb081174-cd0f-4a49-bbca-9e7ae6a215f1\",\"level\":2,\"slug\":\"recap\",\"title\":\"📝 Recap\",\"parentId\":\"0d54f408-d734-4f19-ae67-76fd78baf26b\"}],[{\"id\":\"d502bfec-4f5a-4afd-ac1a-782a847d858b\",\"level\":1,\"slug\":\"how-to-check-programmatically\",\"title\":\"How To Check Programmatically\",\"parentId\":null}],[{\"id\":\"72c60015-95be-4d18-8d21-363a08fa58ce\",\"level\":2,\"slug\":\"testing-mcar-missing-completely-at-random\",\"title\":\"Testing MCAR (Missing Completely At Random)\",\"parentId\":\"d502bfec-4f5a-4afd-ac1a-782a847d858b\"}],[{\"id\":\"f498281d-b432-4f82-b897-61fe79ece742\",\"level\":3,\"slug\":\"littles-mcar-test\",\"title\":\"Little’s MCAR test\",\"parentId\":\"72c60015-95be-4d18-8d21-363a08fa58ce\"}],[{\"id\":\"c75365a6-f524-4f99-a806-c5bc7ae26187\",\"level\":3,\"slug\":\"cross-tab-with-target-chi-square\",\"title\":\"Cross-tab with Target + Chi-square\",\"parentId\":\"72c60015-95be-4d18-8d21-363a08fa58ce\"}],[{\"id\":\"af722907-885e-4267-a27d-61d6a5fd6330\",\"level\":3,\"slug\":\"compare-distributions-manually\",\"title\":\"Compare distributions manually\",\"parentId\":\"72c60015-95be-4d18-8d21-363a08fa58ce\"}],[{\"id\":\"a257c107-58dc-4add-8786-637ea8cc5224\",\"level\":2,\"slug\":\"testing-for-mar-missing-at-random\",\"title\":\"Testing for MAR (Missing At Random)\",\"parentId\":\"d502bfec-4f5a-4afd-ac1a-782a847d858b\"}],[{\"id\":\"2022af84-7ec0-4ae1-920e-54922b26faf3\",\"level\":3,\"slug\":\"diagnosing-mnar-missing-not-at-random\",\"title\":\"Diagnosing MNAR (Missing Not At Random)\",\"parentId\":\"a257c107-58dc-4add-8786-637ea8cc5224\"}],[{\"id\":\"681eba31-4e59-46ff-bd96-a3857f93a4fe\",\"level\":1,\"slug\":\"handling-missing-data-mechanisms\",\"title\":\"🛠 Handling Missing Data Mechanisms\",\"parentId\":null}],[{\"id\":\"255f15b0-6d3e-4af5-b590-1600cc1e474e\",\"level\":3,\"slug\":\"1-mcar\",\"title\":\"1. MCAR\",\"parentId\":\"681eba31-4e59-46ff-bd96-a3857f93a4fe\"}],[{\"id\":\"5e02dd39-772e-4db6-b37e-41765e33d8d0\",\"level\":3,\"slug\":\"2-mar\",\"title\":\"2. MAR\",\"parentId\":\"681eba31-4e59-46ff-bd96-a3857f93a4fe\"}],[{\"id\":\"55cec363-4166-4e25-b34f-a01021ec1593\",\"level\":3,\"slug\":\"step-by-step-intuition\",\"title\":\"Step-by-step intuition:\",\"parentId\":\"681eba31-4e59-46ff-bd96-a3857f93a4fe\"}],[{\"id\":\"43c07aa0-e737-4b4c-83d3-5b157798fe91\",\"level\":3,\"slug\":\"3-mnar\",\"title\":\"3. MNAR\",\"parentId\":\"681eba31-4e59-46ff-bd96-a3857f93a4fe\"}]],\"noIndex\":false}","publication":"{\"__typename\":\"Publication\",\"id\":\"68dd2c04a3a82b4b10e631ed\",\"url\":\"https://amitguptaforwork.hashnode.dev\",\"canonicalURL\":\"https://amitguptaforwork.hashnode.dev\",\"urlPattern\":\"SIMPLE\",\"title\":\"Amit's Learning Notes\",\"displayTitle\":null,\"hasBadges\":false,\"descriptionSEO\":\"Just wanted to share my notes as I learn new exciting things\\n\",\"publicMembers\":{\"totalDocuments\":1},\"about\":{\"html\":\"\u003cp\u003eJust wanted to share my notes as I learn new exciting things\u003c/p\u003e\\n\",\"text\":\"Just wanted to share my notes as I learn new exciting things\\n\"},\"features\":{\"proTeam\":{\"isEnabled\":false},\"newsletter\":{\"isEnabled\":true},\"viewCount\":{\"isEnabled\":false},\"readTime\":{\"isEnabled\":true},\"textSelectionSharer\":{\"isEnabled\":true},\"customCSS\":{\"isEnabled\":false,\"published\":null,\"draft\":null},\"gptBotCrawling\":{\"__typename\":\"GPTBotCrawlingFeature\",\"isEnabled\":false}},\"metaTags\":null,\"ogMetaData\":{\"image\":null},\"author\":{\"__typename\":\"User\",\"id\":\"67fce0562e7f83246dceb67d\",\"name\":\"Amit Gupta\",\"username\":\"learnwithamit\",\"profilePicture\":\"https://cdn.hashnode.com/res/hashnode/image/upload/v1744625750145/964daf99-0a9d-4c05-89d2-b108baacda11.png\"},\"preferences\":{\"__typename\":\"Preferences\",\"logo\":null,\"darkMode\":{\"__typename\":\"DarkModePreferences\",\"logo\":null,\"enabled\":false},\"navbarItems\":[],\"enabledPages\":{\"__typename\":\"PagesPreferences\",\"badges\":false,\"newsletter\":true,\"members\":true},\"layout\":\"grid\",\"disableFooterBranding\":false,\"isSubscriptionModalDisabled\":false},\"favicon\":null,\"headerColor\":null,\"integrations\":{\"fbPixelID\":null,\"fathomSiteID\":null,\"fathomCustomDomainEnabled\":null,\"fathomCustomDomain\":null,\"hotjarSiteID\":null,\"matomoSiteID\":null,\"matomoURL\":null,\"gaTrackingID\":null,\"gTagManagerID\":null,\"plausibleAnalyticsEnabled\":null,\"wmPaymentPointer\":null,\"koalaPublicKey\":null,\"msClarityID\":null},\"imprintV2\":null,\"postsCount\":{\"totalDocuments\":2},\"isTeam\":true,\"links\":{\"twitter\":null,\"instagram\":null,\"github\":null,\"website\":\"https://amitguptaforwork.github.io/\",\"hashnode\":null,\"youtube\":null,\"dailydev\":null,\"linkedin\":\"https://www.linkedin.com/in/amitguptaforwork/\",\"mastodon\":null,\"facebook\":null,\"bluesky\":null},\"domainInfo\":{\"__typename\":\"DomainInfo\",\"hashnodeSubdomain\":\"amitguptaforwork\",\"domain\":null,\"wwwPrefixedDomain\":null},\"redirectionRules\":[],\"totalRecommendedPublications\":0,\"sponsorship\":{\"content\":null,\"stripe\":null},\"allowContributorEdits\":true,\"rssImport\":null,\"post\":{\"id\":\"68dd3a1cf0b5f3410cada41b\",\"cuid\":\"cmg82xvor000002ifbt0ccbob\",\"title\":\"📘 Mcar, Mar, Mnar —\",\"subtitle\":null,\"slug\":\"mcar-mar-mnar\",\"brief\":\"Understanding the Terms Using an Analogy- Shoppers \u0026 Shopping Carts Analogy\\n1. MCAR – Missing Completely At Random\\nStory: You’re analyzing what items shoppers add to their online carts. Suddenly, due to a system glitch, a random subset of cart items ...\",\"featured\":false,\"publishedAt\":\"2025-10-01T14:26:36.843Z\",\"updatedAt\":\"2025-10-01T16:16:07.545Z\",\"author\":{\"__typename\":\"User\",\"id\":\"67fce0562e7f83246dceb67d\",\"name\":\"Amit Gupta\",\"username\":\"learnwithamit\",\"deactivated\":false,\"profilePicture\":\"https://cdn.hashnode.com/res/hashnode/image/upload/v1744625750145/964daf99-0a9d-4c05-89d2-b108baacda11.png\",\"bio\":{\"html\":\"\"},\"socialMediaLinks\":{\"website\":null,\"github\":null,\"twitter\":null,\"facebook\":null,\"stackoverflow\":null,\"linkedin\":null}},\"coAuthors\":[],\"seo\":{\"title\":null,\"description\":null,\"shouldNotIndex\":false},\"coverImage\":{\"url\":\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759335348948/4ead57b0-0b81-454f-a5d5-938972c37be8.jpeg\",\"isPortrait\":false,\"attribution\":null,\"isAttributionHidden\":false,\"photographer\":null},\"responseCount\":0,\"reactionCount\":0,\"replyCount\":0,\"content\":{\"html\":\"\u003ch1 id=\\\"heading-understanding-the-terms-using-an-analogy-shoppers-amp-shopping-carts-analogy\\\"\u003eUnderstanding the Terms Using an Analogy- \u003cstrong\u003eShoppers \u0026amp; Shopping Carts Analogy\u003c/strong\u003e\u003c/h1\u003e\\n\u003ch2 id=\\\"heading-1-mcar-missing-completely-at-random\\\"\u003e1. MCAR – Missing Completely At Random\u003c/h2\u003e\\n\u003cp\u003e\u003cstrong\u003eStory:\u003c/strong\u003e You’re analyzing what items shoppers add to their online carts. Suddenly, due to a system glitch, a random subset of cart items (across all shoppers) \u003cstrong\u003efails to get recorded\u003c/strong\u003e. The missingness has nothing to do with what the items are or who the shopper is — just bad luck\u003c/p\u003e\\n\u003cp\u003e\u003cstrong\u003eKey idea:\u003c/strong\u003e Missingness is completely random. Dropping the missing data still gives you a fair picture of what people buy.\u003c/p\u003e\\n\u003cp\u003e💡 Real-world analogy: A lab machine randomly crashes and corrupts some test results.\u003c/p\u003e\\n\u003ch2 id=\\\"heading-2-mar-missing-at-random\\\"\u003e2. MAR – Missing At Random\u003c/h2\u003e\\n\u003cp\u003e\u003cstrong\u003eStory:\u003c/strong\u003e Suppose items added using a \u003cstrong\u003emobile app\u003c/strong\u003e are sometimes not saved to the cart due to slower connections. But \u003cstrong\u003ewithin the mobile shoppers\u003c/strong\u003e, whether an item is missing doesn’t depend on the type of item itself — only on the platform used.\u003c/p\u003e\\n\u003cp\u003e\u003cstrong\u003eKey idea:\u003c/strong\u003e Missingness depends on an \u003cstrong\u003eobserved factor\u003c/strong\u003e (web vs mobile), but not on the item itself. If you know the shopper’s platform, you can correct for this bias.\u003c/p\u003e\\n\u003cp\u003e💡 Real-world analogy: Older survey respondents are less likely to report income → depends on age (observed), not income value itself.\u003c/p\u003e\\n\u003ch2 id=\\\"heading-3-mnar-missing-not-at-random\\\"\u003e3. MNAR – Missing Not At Random\u003c/h2\u003e\\n\u003cp\u003e\u003cstrong\u003eStory:\u003c/strong\u003e Some shoppers deliberately \u003cstrong\u003eremove expensive luxury items\u003c/strong\u003e from their cart before checking out, because they’re embarrassed or may not be able to afford them. The chance of an item being missing directly depends on the item’s \u003cstrong\u003eprice\u003c/strong\u003e itself (higher price → more likely to go missing).\u003c/p\u003e\\n\u003cp\u003e\u003cstrong\u003eKey idea:\u003c/strong\u003e Missingness depends \u003cstrong\u003edirectly on the unseen value\u003c/strong\u003e (price of item missing). Hardest case to handle, because the missingness itself hides something important.\u003c/p\u003e\\n\u003cp\u003e💡 Real-world analogy: High-income people avoid disclosing income → missingness depends on the true income itself.\u003c/p\u003e\\n\u003ch2 id=\\\"heading-recap\\\"\u003e📝 Recap\u003c/h2\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eMCAR:\u003c/strong\u003e Missing values are caused by pure chance → \u003cem\u003eA random system glitch deletes some cart items\u003c/em\u003e 🌐 — purely random.\u003c/p\u003e\\n\u003cp\u003e  \u003cstrong\u003eMAR:\u003c/strong\u003e Missingness depends on other observed variables, not the missing value itself → \u003cem\u003eMobile app shoppers lose items due to bad connection\u003c/em\u003e 📱 — depends on platform (observed), not the item itself.\u003c/p\u003e\\n\u003cp\u003e  \u003cstrong\u003eMNAR:\u003c/strong\u003e Missingness depends directly on the missing value itself → \u003cem\u003eShoppers deliberately remove very expensive items\u003c/em\u003e 💎 — depends on the value (price) that’s missing.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003e👉 Think: \u003cstrong\u003eMCAR = random glitch 💨, MAR = depends on something you can observe 👀, MNAR = depends on what’s hidden 🔒.\u003c/strong\u003e\u003c/p\u003e\\n\u003ch1 id=\\\"heading-how-to-check-programmatically\\\"\u003eHow To Check Programmatically\u003c/h1\u003e\\n\u003ch2 id=\\\"heading-testing-mcar-missing-completely-at-random\\\"\u003e\u003cstrong\u003eTesting MCAR (Missing Completely At Random)\u003c/strong\u003e\u003c/h2\u003e\\n\u003ch3 id=\\\"heading-littles-mcar-test\\\"\u003eLittle’s MCAR test\u003c/h3\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eONLY FOR NUMERIC COLUMNS ☹\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003ePurpose\u003c/strong\u003e: Global test for whether data are Missing Completely At Random (MCAR) across the whole dataset, considering multivariate patterns of missingness.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eInterpretation\u003c/strong\u003e:\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cul\u003e\\n\u003cli\u003e\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eHigh p value → cannot reject MCAR (missingness is plausibly random).\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eLow p value → reject MCAR (missingness related to data → MAR or MNAR).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eStrength\u003c/strong\u003e: Multivariate test, designed for exactly this situation.\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cstrong\u003eWeakness\u003c/strong\u003e: Sensitive to sample size, not always available in every package.\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003e\u003cspan class=\\\"hljs-comment\\\"\u003e# 1. LITTLE’S MCAR TEST ---------------------------------------\u003c/span\u003e\\n\u003cspan class=\\\"hljs-comment\\\"\u003e# note: not natively in statsmodels yet (as of v0.14)\u003c/span\u003e\\n\u003cspan class=\\\"hljs-comment\\\"\u003e# package option: `little-mcar-test`\u003c/span\u003e\\n\u003cspan class=\\\"hljs-comment\\\"\u003e# install: pip install little-mcar-test\u003c/span\u003e\\n\\n\u003cspan class=\\\"hljs-keyword\\\"\u003efrom\u003c/span\u003e little_mcar_test \u003cspan class=\\\"hljs-keyword\\\"\u003eimport\u003c/span\u003e mcartest\\n\\n\u003cspan class=\\\"hljs-comment\\\"\u003e# run on numeric part of your dataset\u003c/span\u003e\\nnumeric_df = df.select_dtypes(include=[np.number])\\nstat, dof, p_value = mcartest(numeric_df.to_numpy())\\nprint(\u003cspan class=\\\"hljs-string\\\"\u003ef\\\"Little’s MCAR test: χ2 = \u003cspan class=\\\"hljs-subst\\\"\u003e{stat}\u003c/span\u003e, dof = \u003cspan class=\\\"hljs-subst\\\"\u003e{dof}\u003c/span\u003e, p = \u003cspan class=\\\"hljs-subst\\\"\u003e{p_value}\u003c/span\u003e\\\"\u003c/span\u003e)\\n\\n\u003cspan class=\\\"hljs-keyword\\\"\u003eif\u003c/span\u003e p_value \u0026gt; \u003cspan class=\\\"hljs-number\\\"\u003e0.05\u003c/span\u003e:\\n    print(\u003cspan class=\\\"hljs-string\\\"\u003e\\\"Fail to reject MCAR → data plausibly MCAR\\\"\u003c/span\u003e)\\n\u003cspan class=\\\"hljs-keyword\\\"\u003eelse\u003c/span\u003e:\\n    print(\u003cspan class=\\\"hljs-string\\\"\u003e\\\"Reject MCAR → data likely MAR or MNAR\\\"\u003c/span\u003e)\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003ch3 id=\\\"heading-cross-tab-with-target-chi-square\\\"\u003eCross-tab with Target + Chi-square\u003c/h3\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eWe create a new column first (a “missingness indicator”)\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eCross-tabulate this with the target variable and run a chi-square test of independence.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003e\u003cspan class=\\\"hljs-keyword\\\"\u003eimport\u003c/span\u003e pandas \u003cspan class=\\\"hljs-keyword\\\"\u003eas\u003c/span\u003e pd\\n\u003cspan class=\\\"hljs-keyword\\\"\u003efrom\u003c/span\u003e scipy.stats \u003cspan class=\\\"hljs-keyword\\\"\u003eimport\u003c/span\u003e chi2_contingency\\n\u003cspan class=\\\"hljs-comment\\\"\u003e# heuristic check: does missingness in one col depend on another categorical col?\u003c/span\u003e\\n\\ndf[\u003cspan class=\\\"hljs-string\\\"\u003e'col_missing'\u003c/span\u003e] = df[\u003cspan class=\\\"hljs-string\\\"\u003e'some_feature'\u003c/span\u003e].isna().astype(int)\\nctab = pd.crosstab(df[\u003cspan class=\\\"hljs-string\\\"\u003e'col_missing'\u003c/span\u003e], df[\u003cspan class=\\\"hljs-string\\\"\u003e'target'\u003c/span\u003e])\\nchi2, p, dof, exp = chi2_contingency(ctab)\\nprint(\u003cspan class=\\\"hljs-string\\\"\u003e\\\"p-value =\\\"\u003c/span\u003e, p)\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eInterpretation\u003c/strong\u003e:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eIf p \u0026lt; 0.05 → missingness of this feature is associated with the target.\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003eThat suggests missingness carries information → at least MAR, possibly MNAR.\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eStrength\u003c/strong\u003e: Very intuitive for predictive modeling (if missingness depends on target → definitely not MCAR).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eWeakness\u003c/strong\u003e:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eOnly checks relation with target, not with all other features.\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003eSo you might miss situations where missingness is related to a predictor but not the target.\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003ch3 id=\\\"heading-compare-distributions-manually\\\"\u003eCompare distributions manually\u003c/h3\u003e\\n\u003cul\u003e\\n\u003cli\u003eExamine whether the distribution of observed variables differs between rows with and without missing values (should not).\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003edf.groupby(df[\u003cspan class=\\\"hljs-string\\\"\u003e'col_with_missing'\u003c/span\u003e].isna())[\u003cspan class=\\\"hljs-string\\\"\u003e'other_col'\u003c/span\u003e].mean()\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003ch2 id=\\\"heading-testing-for-mar-missing-at-random\\\"\u003e\u003cstrong\u003eTesting for MAR (Missing At Random)\u003c/strong\u003e\u003c/h2\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eStrict tests don’t exist (because MAR involves unobserved missing values).\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003ePractical check\u003c/strong\u003e: Create \\\"missingness indicator\\\" (flag if a variable is missing) and test correlation with other observed variables.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003e    df[\u003cspan class=\\\"hljs-string\\\"\u003e'age_missing'\u003c/span\u003e] = df[\u003cspan class=\\\"hljs-string\\\"\u003e'age'\u003c/span\u003e].isna()\\n\\n    \u003cspan class=\\\"hljs-comment\\\"\u003e#We want to compare age with gender to assess if missing age \u003c/span\u003e\\n    \u003cspan class=\\\"hljs-comment\\\"\u003e#is related to gender value.\u003c/span\u003e\\n    \u003cspan class=\\\"hljs-comment\\\"\u003e#As we are grouping by gender and then calculating mean of the age_missing column, \u003c/span\u003e\\n    \u003cspan class=\\\"hljs-comment\\\"\u003e#this mean equals the proportion of missing ages in that group.\u003c/span\u003e\\n    df.groupby(\u003cspan class=\\\"hljs-string\\\"\u003e'gender'\u003c/span\u003e)[\u003cspan class=\\\"hljs-string\\\"\u003e'age_missing'\u003c/span\u003e].mean()\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eWhat this tells you\u003c/strong\u003e\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eThe result is the missingness rate of the \u003ccode\u003eage\u003c/code\u003e column for each gender:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eIf the value is 0.0 for a gender, no missing ages in that group.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eIf the value is 1.0 for a gender, all ages are missing in that group.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eValues between 0 and 1 indicate the fraction of records with missing age within that gender group.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003ch3 id=\\\"heading-diagnosing-mnar-missing-not-at-random\\\"\u003e\u003cstrong\u003eDiagnosing MNAR (Missing Not At Random)\u003c/strong\u003e\u003c/h3\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eTrue MNAR is \u003cstrong\u003euntestable from the data alone\u003c/strong\u003e (since it depends on the unobserved value).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eRequires:\u003cbr /\u003e  • Domain expertise (e.g., we know high salaries are underreported).\u003cbr /\u003e  • \u003cstrong\u003eSensitivity analysis\u003c/strong\u003e: Assume plausible MNAR mechanisms and check how conclusions change.\u003cbr /\u003e  • Specialized models (selection models, Heckman correction, pattern-mixture models).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003ch1 id=\\\"heading-handling-missing-data-mechanisms\\\"\u003e\u003cstrong\u003e🛠 Handling Missing Data Mechanisms\u003c/strong\u003e\u003c/h1\u003e\\n\u003ch3 id=\\\"heading-1-mcar\\\"\u003e\u003cstrong\u003e1. MCAR\u003c/strong\u003e\u003c/h3\u003e\\n\u003cp\u003e➡️ \u003cstrong\u003eSafe to drop rows or columns\u003c/strong\u003e (analysis unbiased, only reduced sample size).\u003cbr /\u003eOptions:\u003c/p\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003edf_clean = df.dropna()                     \u003cspan class=\\\"hljs-comment\\\"\u003e# drop rows\u003c/span\u003e\\ndf_clean = df.dropna(axis=\u003cspan class=\\\"hljs-number\\\"\u003e1\u003c/span\u003e, thresh=\u003cspan class=\\\"hljs-number\\\"\u003e0.7\u003c/span\u003e*len(df))  \u003cspan class=\\\"hljs-comment\\\"\u003e# drop columns with \u0026gt;30% missing\u003c/span\u003e\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eOr simply use mean/median imputation without worrying about bias.\u003c/p\u003e\\n\u003chr /\u003e\\n\u003ch3 id=\\\"heading-2-mar\\\"\u003e\u003cstrong\u003e2. MAR\u003c/strong\u003e\u003c/h3\u003e\\n\u003cp\u003e➡️ \u003cstrong\u003eBest handled by imputation methods that leverage observed data\u003c/strong\u003e.\u003cbr /\u003eOptions:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eGroup-wise imputation\u003c/strong\u003e:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eWe fill missing values in the income column by gender, using each gender's median income. It then stores the result back in the income column.\u003c/p\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003e  df[\u003cspan class=\\\"hljs-string\\\"\u003e'income'\u003c/span\u003e] = df.groupby(\u003cspan class=\\\"hljs-string\\\"\u003e'gender'\u003c/span\u003e)[\u003cspan class=\\\"hljs-string\\\"\u003e'income'\u003c/span\u003e].transform(\\n      \u003cspan class=\\\"hljs-keyword\\\"\u003elambda\u003c/span\u003e x: x.fillna(x.median()))\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eMultiple Imputation (MICE/Iterative)\u003c/strong\u003e:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eUse the other features to predict missing values, one column at a time, in a round-robin fashion\u003c/p\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003e  \u003cspan class=\\\"hljs-keyword\\\"\u003efrom\u003c/span\u003e sklearn.experimental \u003cspan class=\\\"hljs-keyword\\\"\u003eimport\u003c/span\u003e enable_iterative_imputer\\n  \u003cspan class=\\\"hljs-keyword\\\"\u003efrom\u003c/span\u003e sklearn.impute \u003cspan class=\\\"hljs-keyword\\\"\u003eimport\u003c/span\u003e IterativeImputer\\n  imp = IterativeImputer(random_state=\u003cspan class=\\\"hljs-number\\\"\u003e0\u003c/span\u003e)\\n  df[numeric_cols] = imp.fit_transform(df[numeric_cols])\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eHow IterativeImputer works (high-level)\u003c/strong\u003e\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eIt treats each feature with missing values as the target and uses the other features as predictors.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eIt iteratively:\u003c/p\u003e\\n\u003col\u003e\\n\u003cli\u003e\u003cp\u003eRegresses the missing values of one feature on the others.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eReplaces missing values with the predicted ones.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eMoves to the next feature with missing values and repeats.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ol\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eThis process continues for several iterations until convergence, producing several imputed values that reflect potential relationships among features.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003ch3 id=\\\"heading-step-by-step-intuition\\\"\u003eStep-by-step intuition:\u003c/h3\u003e\\n\u003col\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eStart with missing values\u003c/strong\u003e\u003cbr /\u003e You have a dataset where some entries are \u003ccode\u003eNaN\u003c/code\u003e.\u003c/p\u003e\\n\u003cp\u003e Example:\u003c/p\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003e Age   Salary   Experience\\n \u003cspan class=\\\"hljs-number\\\"\u003e25\u003c/span\u003e    \u003cspan class=\\\"hljs-number\\\"\u003e50\u003c/span\u003ek      \u003cspan class=\\\"hljs-number\\\"\u003e2\u003c/span\u003e\\n NaN   \u003cspan class=\\\"hljs-number\\\"\u003e60\u003c/span\u003ek      \u003cspan class=\\\"hljs-number\\\"\u003e3\u003c/span\u003e\\n \u003cspan class=\\\"hljs-number\\\"\u003e30\u003c/span\u003e    NaN      \u003cspan class=\\\"hljs-number\\\"\u003e4\u003c/span\u003e\\n \u003cspan class=\\\"hljs-number\\\"\u003e28\u003c/span\u003e    \u003cspan class=\\\"hljs-number\\\"\u003e55\u003c/span\u003ek      NaN\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eChoose one column with missing values (say Age).\u003c/strong\u003e\u003cbr /\u003e Treat \u003cem\u003eAge\u003c/em\u003e as the \\\"target variable\\\" and the other columns (Salary, Experience) as predictors.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eTrain a regression model\u003c/strong\u003e\u003cbr /\u003e Use the rows where \u003cem\u003eAge\u003c/em\u003e is known to train a model like:\u003c/p\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003e Age ~ Salary + Experience\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003ePredict missing values for Age\u003c/strong\u003e\u003cbr /\u003e Use the trained model to fill in the \u003ccode\u003eNaN\u003c/code\u003es in Age.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eMove to the next column with missing values (say Salary)\u003c/strong\u003e\u003cbr /\u003e Now treat \u003cem\u003eSalary\u003c/em\u003e as the target and use Age and Experience (with the newly imputed values) as predictors to estimate missing Salary.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eRepeat for all columns with missing values.\u003c/strong\u003e\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eIterate multiple times\u003c/strong\u003e\u003cbr /\u003e Because each column’s imputation depends on the others (which may also be imputed), the algorithm cycles through all columns multiple times. With each iteration, the imputations become more stable and consistent.\u003c/p\u003e\\n\u003cp\u003e That’s why it’s called \u003cstrong\u003eIterative\u003c/strong\u003e Imputer.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ol\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003e            \u003cstrong\u003eWhen to use\u003c/strong\u003e\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eWhen you believe there are inter-feature relationships and you want a more sophisticated imputation than simple statistics (e.g., mean/median).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eFor datasets where the pattern of missingness might depend on other observed features.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003e            \u003cstrong\u003eNotes and best practices\u003c/strong\u003e\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eEnsure numeric_cols is a list of the numeric column names you want to impute.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eYou can customize the underlying estimator (e.g., linear regression, decision trees) by passing estimator= to the IterativeImputer.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eBe mindful of potential data leakage: fit on training data only, and apply the same transformation to validation/test data.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eAdd missing indicator features\u003c/strong\u003e:\u003c/p\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003e  df[\u003cspan class=\\\"hljs-string\\\"\u003e'income_missing'\u003c/span\u003e] = df[\u003cspan class=\\\"hljs-string\\\"\u003e'income'\u003c/span\u003e].isna().astype(int)\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003chr /\u003e\\n\u003ch3 id=\\\"heading-3-mnar\\\"\u003e\u003cstrong\u003e3. MNAR\u003c/strong\u003e\u003c/h3\u003e\\n\u003cp\u003e➡️ \u003cstrong\u003eHardest case — no purely data-driven solution.\u003c/strong\u003e\u003cbr /\u003eTypical strategies:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eDomain-informed imputation\u003c/strong\u003e (use expert rules, external benchmarks).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eSensitivity analysis\u003c/strong\u003e: Try imputations under different assumptions (e.g., assume the missing group is 10% higher/lower).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eHeckman correction / selection models\u003c/strong\u003e (statsmodels has limited support).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003ePattern mixture models / Bayesian methods\u003c/strong\u003e.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003e👉 Often you need to:\u003cbr /\u003eA. Report that data may be MNAR.\u003cbr /\u003eB. Perform robustness checks (see if conclusions hold under different plausible imputations).\u003c/p\u003e\\n\",\"markdown\":\"# Understanding the Terms Using an Analogy- **Shoppers \u0026 Shopping Carts Analogy**\\n\\n## 1\\\\. MCAR – Missing Completely At Random\\n\\n**Story:** You’re analyzing what items shoppers add to their online carts. Suddenly, due to a system glitch, a random subset of cart items (across all shoppers) **fails to get recorded**. The missingness has nothing to do with what the items are or who the shopper is — just bad luck\\n\\n**Key idea:** Missingness is completely random. Dropping the missing data still gives you a fair picture of what people buy.\\n\\n💡 Real-world analogy: A lab machine randomly crashes and corrupts some test results.\\n\\n## 2\\\\. MAR – Missing At Random\\n\\n**Story:** Suppose items added using a **mobile app** are sometimes not saved to the cart due to slower connections. But **within the mobile shoppers**, whether an item is missing doesn’t depend on the type of item itself — only on the platform used.\\n\\n**Key idea:** Missingness depends on an **observed factor** (web vs mobile), but not on the item itself. If you know the shopper’s platform, you can correct for this bias.\\n\\n💡 Real-world analogy: Older survey respondents are less likely to report income → depends on age (observed), not income value itself.\\n\\n## 3\\\\. MNAR – Missing Not At Random\\n\\n**Story:** Some shoppers deliberately **remove expensive luxury items** from their cart before checking out, because they’re embarrassed or may not be able to afford them. The chance of an item being missing directly depends on the item’s **price** itself (higher price → more likely to go missing).\\n\\n**Key idea:** Missingness depends **directly on the unseen value** (price of item missing). Hardest case to handle, because the missingness itself hides something important.\\n\\n💡 Real-world analogy: High-income people avoid disclosing income → missingness depends on the true income itself.\\n\\n## 📝 Recap\\n\\n* **MCAR:** Missing values are caused by pure chance → *A random system glitch deletes some cart items* 🌐 — purely random.\\n    \\n    **MAR:** Missingness depends on other observed variables, not the missing value itself → *Mobile app shoppers lose items due to bad connection* 📱 — depends on platform (observed), not the item itself.\\n    \\n    **MNAR:** Missingness depends directly on the missing value itself → *Shoppers deliberately remove very expensive items* 💎 — depends on the value (price) that’s missing.\\n    \\n\\n👉 Think: **MCAR = random glitch 💨, MAR = depends on something you can observe 👀, MNAR = depends on what’s hidden 🔒.**\\n\\n# How To Check Programmatically\\n\\n## **Testing MCAR (Missing Completely At Random)**\\n\\n### Little’s MCAR test\\n\\n* ONLY FOR NUMERIC COLUMNS ☹\\n    \\n* **Purpose**: Global test for whether data are Missing Completely At Random (MCAR) across the whole dataset, considering multivariate patterns of missingness.\\n    \\n* **Interpretation**:\\n    \\n* * * High p value → cannot reject MCAR (missingness is plausibly random).\\n            \\n            * Low p value → reject MCAR (missingness related to data → MAR or MNAR).\\n                \\n            * **Strength**: Multivariate test, designed for exactly this situation.\\n                \\n        * **Weakness**: Sensitive to sample size, not always available in every package.\\n            \\n\\n```python\\n# 1. LITTLE’S MCAR TEST ---------------------------------------\\n# note: not natively in statsmodels yet (as of v0.14)\\n# package option: `little-mcar-test`\\n# install: pip install little-mcar-test\\n\\nfrom little_mcar_test import mcartest\\n\\n# run on numeric part of your dataset\\nnumeric_df = df.select_dtypes(include=[np.number])\\nstat, dof, p_value = mcartest(numeric_df.to_numpy())\\nprint(f\\\"Little’s MCAR test: χ2 = {stat}, dof = {dof}, p = {p_value}\\\")\\n\\nif p_value \u003e 0.05:\\n    print(\\\"Fail to reject MCAR → data plausibly MCAR\\\")\\nelse:\\n    print(\\\"Reject MCAR → data likely MAR or MNAR\\\")\\n```\\n\\n### Cross-tab with Target + Chi-square\\n\\n* We create a new column first (a “missingness indicator”)\\n    \\n* Cross-tabulate this with the target variable and run a chi-square test of independence.\\n    \\n\\n```python\\nimport pandas as pd\\nfrom scipy.stats import chi2_contingency\\n# heuristic check: does missingness in one col depend on another categorical col?\\n\\ndf['col_missing'] = df['some_feature'].isna().astype(int)\\nctab = pd.crosstab(df['col_missing'], df['target'])\\nchi2, p, dof, exp = chi2_contingency(ctab)\\nprint(\\\"p-value =\\\", p)\\n```\\n\\n* **Interpretation**:\\n    \\n    * If p \u0026lt; 0.05 → missingness of this feature is associated with the target.\\n        \\n        * That suggests missingness carries information → at least MAR, possibly MNAR.\\n            \\n* **Strength**: Very intuitive for predictive modeling (if missingness depends on target → definitely not MCAR).\\n    \\n* **Weakness**:\\n    \\n    * Only checks relation with target, not with all other features.\\n        \\n        * So you might miss situations where missingness is related to a predictor but not the target.\\n            \\n\\n### Compare distributions manually\\n\\n* Examine whether the distribution of observed variables differs between rows with and without missing values (should not).\\n    \\n\\n```python\\ndf.groupby(df['col_with_missing'].isna())['other_col'].mean()\\n```\\n\\n## **Testing for MAR (Missing At Random)**\\n\\n* * Strict tests don’t exist (because MAR involves unobserved missing values).\\n        \\n        * **Practical check**: Create \\\"missingness indicator\\\" (flag if a variable is missing) and test correlation with other observed variables.\\n            \\n        * ```python\\n              df['age_missing'] = df['age'].isna()\\n              \\n              #We want to compare age with gender to assess if missing age \\n              #is related to gender value.\\n              #As we are grouping by gender and then calculating mean of the age_missing column, \\n              #this mean equals the proportion of missing ages in that group.\\n              df.groupby('gender')['age_missing'].mean()\\n            ```\\n            \\n* **What this tells you**\\n    \\n* The result is the missingness rate of the `age` column for each gender:\\n    \\n    * If the value is 0.0 for a gender, no missing ages in that group.\\n        \\n    * If the value is 1.0 for a gender, all ages are missing in that group.\\n        \\n    * Values between 0 and 1 indicate the fraction of records with missing age within that gender group.\\n        \\n\\n### **Diagnosing MNAR (Missing Not At Random)**\\n\\n* True MNAR is **untestable from the data alone** (since it depends on the unobserved value).\\n    \\n* Requires:  \\n    • Domain expertise (e.g., we know high salaries are underreported).  \\n    • **Sensitivity analysis**: Assume plausible MNAR mechanisms and check how conclusions change.  \\n    • Specialized models (selection models, Heckman correction, pattern-mixture models).\\n    \\n\\n# **🛠 Handling Missing Data Mechanisms**\\n\\n### **1\\\\. MCAR**\\n\\n➡️ **Safe to drop rows or columns** (analysis unbiased, only reduced sample size).  \\nOptions:\\n\\n```python\\ndf_clean = df.dropna()                     # drop rows\\ndf_clean = df.dropna(axis=1, thresh=0.7*len(df))  # drop columns with \u003e30% missing\\n```\\n\\nOr simply use mean/median imputation without worrying about bias.\\n\\n---\\n\\n### **2\\\\. MAR**\\n\\n➡️ **Best handled by imputation methods that leverage observed data**.  \\nOptions:\\n\\n* **Group-wise imputation**:\\n    \\n    * We fill missing values in the income column by gender, using each gender's median income. It then stores the result back in the income column.\\n        \\n        ```python\\n        df['income'] = df.groupby('gender')['income'].transform(\\n            lambda x: x.fillna(x.median()))\\n        ```\\n        \\n* **Multiple Imputation (MICE/Iterative)**:\\n    \\n    * Use the other features to predict missing values, one column at a time, in a round-robin fashion\\n        \\n        ```python\\n        from sklearn.experimental import enable_iterative_imputer\\n        from sklearn.impute import IterativeImputer\\n        imp = IterativeImputer(random_state=0)\\n        df[numeric_cols] = imp.fit_transform(df[numeric_cols])\\n        ```\\n        \\n        * **How IterativeImputer works (high-level)**\\n            \\n            * It treats each feature with missing values as the target and uses the other features as predictors.\\n                \\n            * It iteratively:\\n                \\n                1. Regresses the missing values of one feature on the others.\\n                    \\n                2. Replaces missing values with the predicted ones.\\n                    \\n                3. Moves to the next feature with missing values and repeats.\\n                    \\n            * This process continues for several iterations until convergence, producing several imputed values that reflect potential relationships among features.\\n                \\n            * ### Step-by-step intuition:\\n                \\n                1. **Start with missing values**  \\n                    You have a dataset where some entries are `NaN`.\\n                    \\n                    Example:\\n                    \\n                    ```python\\n                    Age   Salary   Experience\\n                    25    50k      2\\n                    NaN   60k      3\\n                    30    NaN      4\\n                    28    55k      NaN\\n                    ```\\n                    \\n                2. **Choose one column with missing values (say Age).**  \\n                    Treat *Age* as the \\\"target variable\\\" and the other columns (Salary, Experience) as predictors.\\n                    \\n                3. **Train a regression model**  \\n                    Use the rows where *Age* is known to train a model like:\\n                    \\n                    ```python\\n                    Age ~ Salary + Experience\\n                    ```\\n                    \\n                4. **Predict missing values for Age**  \\n                    Use the trained model to fill in the `NaN`s in Age.\\n                    \\n                5. **Move to the next column with missing values (say Salary)**  \\n                    Now treat *Salary* as the target and use Age and Experience (with the newly imputed values) as predictors to estimate missing Salary.\\n                    \\n                6. **Repeat for all columns with missing values.**\\n                    \\n                7. **Iterate multiple times**  \\n                    Because each column’s imputation depends on the others (which may also be imputed), the algorithm cycles through all columns multiple times. With each iteration, the imputations become more stable and consistent.\\n                    \\n                    That’s why it’s called **Iterative** Imputer.\\n                    \\n                \\n            \\n            **When to use**\\n            \\n            * When you believe there are inter-feature relationships and you want a more sophisticated imputation than simple statistics (e.g., mean/median).\\n                \\n            * For datasets where the pattern of missingness might depend on other observed features.\\n                \\n            \\n            **Notes and best practices**\\n            \\n            * Ensure numeric\\\\_cols is a list of the numeric column names you want to impute.\\n                \\n            * You can customize the underlying estimator (e.g., linear regression, decision trees) by passing estimator= to the IterativeImputer.\\n                \\n            * Be mindful of potential data leakage: fit on training data only, and apply the same transformation to validation/test data.\\n                \\n* **Add missing indicator features**:\\n    \\n    ```python\\n    df['income_missing'] = df['income'].isna().astype(int)\\n    ```\\n    \\n\\n---\\n\\n### **3\\\\. MNAR**\\n\\n➡️ **Hardest case — no purely data-driven solution.**  \\nTypical strategies:\\n\\n* **Domain-informed imputation** (use expert rules, external benchmarks).\\n    \\n* **Sensitivity analysis**: Try imputations under different assumptions (e.g., assume the missing group is 10% higher/lower).\\n    \\n* **Heckman correction / selection models** (statsmodels has limited support).\\n    \\n* **Pattern mixture models / Bayesian methods**.\\n    \\n\\n👉 Often you need to:  \\nA. Report that data may be MNAR.  \\nB. Perform robustness checks (see if conclusions hold under different plausible imputations).\"},\"views\":14,\"preferences\":{\"pinnedToBlog\":false,\"disableComments\":false,\"stickCoverToBottom\":false,\"isDelisted\":false},\"readTimeInMinutes\":7,\"series\":null,\"tags\":[{\"id\":\"56744722958ef13879b951ac\",\"slug\":\"data-analysis\",\"name\":\"data analysis\"},{\"id\":\"63f336bb322fd23a23d459f7\",\"slug\":\"analogy\",\"name\":\"analogy\"},{\"id\":\"62e40f16430bd1d04a83efb7\",\"slug\":\"missing-data\",\"name\":\"missing data\"}],\"ogMetaData\":{\"image\":null},\"canonicalUrl\":null,\"hasLatexInPost\":false,\"audioUrls\":null,\"isFollowed\":null,\"bookmarked\":false,\"features\":{\"tableOfContents\":{\"isEnabled\":true,\"items\":[{\"__typename\":\"TableOfContentsItem\",\"id\":\"0d54f408-d734-4f19-ae67-76fd78baf26b\",\"level\":1,\"slug\":\"understanding-the-terms-using-an-analogy-shoppers-amp-shopping-carts-analogy\",\"title\":\"Understanding the Terms Using an Analogy- Shoppers \u0026amp; Shopping Carts Analogy\",\"parentId\":null},{\"__typename\":\"TableOfContentsItem\",\"id\":\"84251c14-d5ea-4069-a4c2-7e803e38760e\",\"level\":2,\"slug\":\"1-mcar-missing-completely-at-random\",\"title\":\"1. MCAR – Missing Completely At Random\",\"parentId\":\"0d54f408-d734-4f19-ae67-76fd78baf26b\"},{\"__typename\":\"TableOfContentsItem\",\"id\":\"27d4fa19-f1be-44bd-a38d-47acb0026af2\",\"level\":2,\"slug\":\"2-mar-missing-at-random\",\"title\":\"2. MAR – Missing At Random\",\"parentId\":\"0d54f408-d734-4f19-ae67-76fd78baf26b\"},{\"__typename\":\"TableOfContentsItem\",\"id\":\"924bf17c-929c-4081-ab52-82014f57551f\",\"level\":2,\"slug\":\"3-mnar-missing-not-at-random\",\"title\":\"3. MNAR – Missing Not At Random\",\"parentId\":\"0d54f408-d734-4f19-ae67-76fd78baf26b\"},{\"__typename\":\"TableOfContentsItem\",\"id\":\"cb081174-cd0f-4a49-bbca-9e7ae6a215f1\",\"level\":2,\"slug\":\"recap\",\"title\":\"📝 Recap\",\"parentId\":\"0d54f408-d734-4f19-ae67-76fd78baf26b\"},{\"__typename\":\"TableOfContentsItem\",\"id\":\"d502bfec-4f5a-4afd-ac1a-782a847d858b\",\"level\":1,\"slug\":\"how-to-check-programmatically\",\"title\":\"How To Check Programmatically\",\"parentId\":null},{\"__typename\":\"TableOfContentsItem\",\"id\":\"72c60015-95be-4d18-8d21-363a08fa58ce\",\"level\":2,\"slug\":\"testing-mcar-missing-completely-at-random\",\"title\":\"Testing MCAR (Missing Completely At Random)\",\"parentId\":\"d502bfec-4f5a-4afd-ac1a-782a847d858b\"},{\"__typename\":\"TableOfContentsItem\",\"id\":\"f498281d-b432-4f82-b897-61fe79ece742\",\"level\":3,\"slug\":\"littles-mcar-test\",\"title\":\"Little’s MCAR test\",\"parentId\":\"72c60015-95be-4d18-8d21-363a08fa58ce\"},{\"__typename\":\"TableOfContentsItem\",\"id\":\"c75365a6-f524-4f99-a806-c5bc7ae26187\",\"level\":3,\"slug\":\"cross-tab-with-target-chi-square\",\"title\":\"Cross-tab with Target + Chi-square\",\"parentId\":\"72c60015-95be-4d18-8d21-363a08fa58ce\"},{\"__typename\":\"TableOfContentsItem\",\"id\":\"af722907-885e-4267-a27d-61d6a5fd6330\",\"level\":3,\"slug\":\"compare-distributions-manually\",\"title\":\"Compare distributions manually\",\"parentId\":\"72c60015-95be-4d18-8d21-363a08fa58ce\"},{\"__typename\":\"TableOfContentsItem\",\"id\":\"a257c107-58dc-4add-8786-637ea8cc5224\",\"level\":2,\"slug\":\"testing-for-mar-missing-at-random\",\"title\":\"Testing for MAR (Missing At Random)\",\"parentId\":\"d502bfec-4f5a-4afd-ac1a-782a847d858b\"},{\"__typename\":\"TableOfContentsItem\",\"id\":\"2022af84-7ec0-4ae1-920e-54922b26faf3\",\"level\":3,\"slug\":\"diagnosing-mnar-missing-not-at-random\",\"title\":\"Diagnosing MNAR (Missing Not At Random)\",\"parentId\":\"a257c107-58dc-4add-8786-637ea8cc5224\"},{\"__typename\":\"TableOfContentsItem\",\"id\":\"681eba31-4e59-46ff-bd96-a3857f93a4fe\",\"level\":1,\"slug\":\"handling-missing-data-mechanisms\",\"title\":\"🛠 Handling Missing Data Mechanisms\",\"parentId\":null},{\"__typename\":\"TableOfContentsItem\",\"id\":\"255f15b0-6d3e-4af5-b590-1600cc1e474e\",\"level\":3,\"slug\":\"1-mcar\",\"title\":\"1. MCAR\",\"parentId\":\"681eba31-4e59-46ff-bd96-a3857f93a4fe\"},{\"__typename\":\"TableOfContentsItem\",\"id\":\"5e02dd39-772e-4db6-b37e-41765e33d8d0\",\"level\":3,\"slug\":\"2-mar\",\"title\":\"2. MAR\",\"parentId\":\"681eba31-4e59-46ff-bd96-a3857f93a4fe\"},{\"__typename\":\"TableOfContentsItem\",\"id\":\"55cec363-4166-4e25-b34f-a01021ec1593\",\"level\":3,\"slug\":\"step-by-step-intuition\",\"title\":\"Step-by-step intuition:\",\"parentId\":\"681eba31-4e59-46ff-bd96-a3857f93a4fe\"},{\"__typename\":\"TableOfContentsItem\",\"id\":\"43c07aa0-e737-4b4c-83d3-5b157798fe91\",\"level\":3,\"slug\":\"3-mnar\",\"title\":\"3. MNAR\",\"parentId\":\"681eba31-4e59-46ff-bd96-a3857f93a4fe\"}]},\"badges\":{\"isEnabled\":true,\"items\":[]}},\"isAutoPublishedFromRSS\":false,\"authenticatedUserLikes\":{\"edges\":[]},\"totalUserLikes\":{\"totalDocuments\":0},\"isShadowBanned\":false,\"isAskMeAnything\":false},\"redirectedPost\":null,\"staticPage\":null}","totalUsersWhoLikedArticle":0,"integrations":{"fbPixelID":null,"fathomSiteID":null,"fathomCustomDomainEnabled":null,"fathomCustomDomain":null,"hotjarSiteID":null,"matomoSiteID":null,"matomoURL":null,"gaTrackingID":null,"gTagManagerID":null,"plausibleAnalyticsEnabled":null,"koalaPublicKey":null,"msClarityID":null,"domainURL":"amitguptaforwork.hashnode.dev"},"rootLayout":{"legacyPublicationJSON":"{\"_id\":\"68dd2c04a3a82b4b10e631ed\",\"author\":{\"_id\":\"67fce0562e7f83246dceb67d\",\"name\":\"Amit Gupta\",\"photo\":\"https://cdn.hashnode.com/res/hashnode/image/upload/v1744625750145/964daf99-0a9d-4c05-89d2-b108baacda11.png\",\"username\":\"learnwithamit\"},\"badgePageEnabled\":true,\"description\":\"Just wanted to share my notes as I learn new exciting things\\n\",\"domain\":\"\",\"domainStatus\":{},\"wwwPrefixedDomainStatus\":{},\"customCSSEnabled\":false,\"customCSSPublished\":{\"homeMin\":\"\",\"postMin\":\"\",\"staticMin\":\"\"},\"customRules\":[],\"darkModeEnabled\":false,\"darkModeLogo\":\"\",\"disableFooterBranding\":false,\"isSubscriptionModalDisabled\":false,\"publicMembersCount\":1,\"displayTitle\":\"\",\"favicon\":\"\",\"gaTrackingID\":\"\",\"gTagManagerID\":\"\",\"hasBadges\":false,\"headerColor\":\"\",\"hideMembersPage\":false,\"isTeam\":true,\"layout\":\"grid\",\"membersPageEnabled\":true,\"menu\":[],\"metaHTML\":\"\u003cp\u003eJust wanted to share my notes as I learn new exciting things\u003c/p\u003e\\n\",\"metaHTMLSanitized\":\"Just wanted to share my notes as I learn new exciting things\\n\",\"newsletterEnabled\":true,\"proTeamEnabled\":false,\"newsletterPageEnabled\":false,\"ogImage\":\"\",\"logo\":\"\",\"textSelectionSharerEnabled\":true,\"title\":\"Amit's Learning Notes\",\"urlPattern\":\"simple\",\"username\":\"amitguptaforwork\",\"viewCountVisible\":false,\"readTimeHidden\":false,\"links\":{\"twitter\":\"\",\"instagram\":\"\",\"github\":\"\",\"website\":\"https://amitguptaforwork.github.io/\",\"hashnode\":\"\",\"youtube\":\"\",\"dailydev\":\"\",\"linkedin\":\"https://www.linkedin.com/in/amitguptaforwork/\",\"mastodon\":\"\",\"facebook\":\"\"},\"numPosts\":2,\"sponsorship\":{\"content\":\"\",\"contentMarkdown\":\"\"},\"allowContributorEdits\":true,\"allowCrawlingByGPT\":false}","legacyPostJSON":"{\"_id\":\"68dd3a1cf0b5f3410cada41b\",\"partOfPublication\":true,\"author\":{\"_id\":\"67fce0562e7f83246dceb67d\",\"name\":\"Amit Gupta\",\"photo\":\"https://cdn.hashnode.com/res/hashnode/image/upload/v1744625750145/964daf99-0a9d-4c05-89d2-b108baacda11.png\",\"username\":\"learnwithamit\",\"bio\":\"\",\"socialMedia\":{},\"isDeactivated\":false},\"bookmarkedIn\":[],\"publication\":{\"_id\":\"68dd2c04a3a82b4b10e631ed\",\"author\":{\"_id\":\"67fce0562e7f83246dceb67d\",\"name\":\"Amit Gupta\",\"photo\":\"https://cdn.hashnode.com/res/hashnode/image/upload/v1744625750145/964daf99-0a9d-4c05-89d2-b108baacda11.png\",\"username\":\"learnwithamit\"},\"badgePageEnabled\":true,\"description\":\"Just wanted to share my notes as I learn new exciting things\\n\",\"domain\":\"\",\"domainStatus\":{},\"wwwPrefixedDomainStatus\":{},\"customCSSEnabled\":false,\"customCSSPublished\":{\"homeMin\":\"\",\"postMin\":\"\",\"staticMin\":\"\"},\"customRules\":[],\"darkModeEnabled\":false,\"darkModeLogo\":\"\",\"disableFooterBranding\":false,\"isSubscriptionModalDisabled\":false,\"publicMembersCount\":1,\"displayTitle\":\"\",\"favicon\":\"\",\"gaTrackingID\":\"\",\"gTagManagerID\":\"\",\"hasBadges\":false,\"headerColor\":\"\",\"hideMembersPage\":false,\"isTeam\":true,\"layout\":\"grid\",\"membersPageEnabled\":true,\"menu\":[],\"metaHTML\":\"\u003cp\u003eJust wanted to share my notes as I learn new exciting things\u003c/p\u003e\\n\",\"metaHTMLSanitized\":\"Just wanted to share my notes as I learn new exciting things\\n\",\"newsletterEnabled\":true,\"proTeamEnabled\":false,\"newsletterPageEnabled\":false,\"ogImage\":\"\",\"logo\":\"\",\"textSelectionSharerEnabled\":true,\"title\":\"Amit's Learning Notes\",\"urlPattern\":\"simple\",\"username\":\"amitguptaforwork\",\"viewCountVisible\":false,\"readTimeHidden\":false,\"links\":{\"twitter\":\"\",\"instagram\":\"\",\"github\":\"\",\"website\":\"https://amitguptaforwork.github.io/\",\"hashnode\":\"\",\"youtube\":\"\",\"dailydev\":\"\",\"linkedin\":\"https://www.linkedin.com/in/amitguptaforwork/\",\"mastodon\":\"\",\"facebook\":\"\"},\"numPosts\":2,\"sponsorship\":{\"content\":\"\",\"contentMarkdown\":\"\"},\"allowContributorEdits\":true,\"allowCrawlingByGPT\":false},\"tags\":[{\"_id\":\"56744722958ef13879b951ac\",\"slug\":\"data-analysis\",\"name\":\"data analysis\",\"isActive\":true,\"isApproved\":true},{\"_id\":\"63f336bb322fd23a23d459f7\",\"slug\":\"analogy\",\"name\":\"analogy\",\"isActive\":true,\"isApproved\":true},{\"_id\":\"62e40f16430bd1d04a83efb7\",\"slug\":\"missing-data\",\"name\":\"missing data\",\"isActive\":true,\"isApproved\":true}],\"coAuthors\":[],\"responseCount\":0,\"replyCount\":0,\"contentMarkdown\":\"# Understanding the Terms Using an Analogy- **Shoppers \u0026 Shopping Carts Analogy**\\n\\n## 1\\\\. MCAR – Missing Completely At Random\\n\\n**Story:** You’re analyzing what items shoppers add to their online carts. Suddenly, due to a system glitch, a random subset of cart items (across all shoppers) **fails to get recorded**. The missingness has nothing to do with what the items are or who the shopper is — just bad luck\\n\\n**Key idea:** Missingness is completely random. Dropping the missing data still gives you a fair picture of what people buy.\\n\\n💡 Real-world analogy: A lab machine randomly crashes and corrupts some test results.\\n\\n## 2\\\\. MAR – Missing At Random\\n\\n**Story:** Suppose items added using a **mobile app** are sometimes not saved to the cart due to slower connections. But **within the mobile shoppers**, whether an item is missing doesn’t depend on the type of item itself — only on the platform used.\\n\\n**Key idea:** Missingness depends on an **observed factor** (web vs mobile), but not on the item itself. If you know the shopper’s platform, you can correct for this bias.\\n\\n💡 Real-world analogy: Older survey respondents are less likely to report income → depends on age (observed), not income value itself.\\n\\n## 3\\\\. MNAR – Missing Not At Random\\n\\n**Story:** Some shoppers deliberately **remove expensive luxury items** from their cart before checking out, because they’re embarrassed or may not be able to afford them. The chance of an item being missing directly depends on the item’s **price** itself (higher price → more likely to go missing).\\n\\n**Key idea:** Missingness depends **directly on the unseen value** (price of item missing). Hardest case to handle, because the missingness itself hides something important.\\n\\n💡 Real-world analogy: High-income people avoid disclosing income → missingness depends on the true income itself.\\n\\n## 📝 Recap\\n\\n* **MCAR:** Missing values are caused by pure chance → *A random system glitch deletes some cart items* 🌐 — purely random.\\n    \\n    **MAR:** Missingness depends on other observed variables, not the missing value itself → *Mobile app shoppers lose items due to bad connection* 📱 — depends on platform (observed), not the item itself.\\n    \\n    **MNAR:** Missingness depends directly on the missing value itself → *Shoppers deliberately remove very expensive items* 💎 — depends on the value (price) that’s missing.\\n    \\n\\n👉 Think: **MCAR = random glitch 💨, MAR = depends on something you can observe 👀, MNAR = depends on what’s hidden 🔒.**\\n\\n# How To Check Programmatically\\n\\n## **Testing MCAR (Missing Completely At Random)**\\n\\n### Little’s MCAR test\\n\\n* ONLY FOR NUMERIC COLUMNS ☹\\n    \\n* **Purpose**: Global test for whether data are Missing Completely At Random (MCAR) across the whole dataset, considering multivariate patterns of missingness.\\n    \\n* **Interpretation**:\\n    \\n* * * High p value → cannot reject MCAR (missingness is plausibly random).\\n            \\n            * Low p value → reject MCAR (missingness related to data → MAR or MNAR).\\n                \\n            * **Strength**: Multivariate test, designed for exactly this situation.\\n                \\n        * **Weakness**: Sensitive to sample size, not always available in every package.\\n            \\n\\n```python\\n# 1. LITTLE’S MCAR TEST ---------------------------------------\\n# note: not natively in statsmodels yet (as of v0.14)\\n# package option: `little-mcar-test`\\n# install: pip install little-mcar-test\\n\\nfrom little_mcar_test import mcartest\\n\\n# run on numeric part of your dataset\\nnumeric_df = df.select_dtypes(include=[np.number])\\nstat, dof, p_value = mcartest(numeric_df.to_numpy())\\nprint(f\\\"Little’s MCAR test: χ2 = {stat}, dof = {dof}, p = {p_value}\\\")\\n\\nif p_value \u003e 0.05:\\n    print(\\\"Fail to reject MCAR → data plausibly MCAR\\\")\\nelse:\\n    print(\\\"Reject MCAR → data likely MAR or MNAR\\\")\\n```\\n\\n### Cross-tab with Target + Chi-square\\n\\n* We create a new column first (a “missingness indicator”)\\n    \\n* Cross-tabulate this with the target variable and run a chi-square test of independence.\\n    \\n\\n```python\\nimport pandas as pd\\nfrom scipy.stats import chi2_contingency\\n# heuristic check: does missingness in one col depend on another categorical col?\\n\\ndf['col_missing'] = df['some_feature'].isna().astype(int)\\nctab = pd.crosstab(df['col_missing'], df['target'])\\nchi2, p, dof, exp = chi2_contingency(ctab)\\nprint(\\\"p-value =\\\", p)\\n```\\n\\n* **Interpretation**:\\n    \\n    * If p \u0026lt; 0.05 → missingness of this feature is associated with the target.\\n        \\n        * That suggests missingness carries information → at least MAR, possibly MNAR.\\n            \\n* **Strength**: Very intuitive for predictive modeling (if missingness depends on target → definitely not MCAR).\\n    \\n* **Weakness**:\\n    \\n    * Only checks relation with target, not with all other features.\\n        \\n        * So you might miss situations where missingness is related to a predictor but not the target.\\n            \\n\\n### Compare distributions manually\\n\\n* Examine whether the distribution of observed variables differs between rows with and without missing values (should not).\\n    \\n\\n```python\\ndf.groupby(df['col_with_missing'].isna())['other_col'].mean()\\n```\\n\\n## **Testing for MAR (Missing At Random)**\\n\\n* * Strict tests don’t exist (because MAR involves unobserved missing values).\\n        \\n        * **Practical check**: Create \\\"missingness indicator\\\" (flag if a variable is missing) and test correlation with other observed variables.\\n            \\n        * ```python\\n              df['age_missing'] = df['age'].isna()\\n              \\n              #We want to compare age with gender to assess if missing age \\n              #is related to gender value.\\n              #As we are grouping by gender and then calculating mean of the age_missing column, \\n              #this mean equals the proportion of missing ages in that group.\\n              df.groupby('gender')['age_missing'].mean()\\n            ```\\n            \\n* **What this tells you**\\n    \\n* The result is the missingness rate of the `age` column for each gender:\\n    \\n    * If the value is 0.0 for a gender, no missing ages in that group.\\n        \\n    * If the value is 1.0 for a gender, all ages are missing in that group.\\n        \\n    * Values between 0 and 1 indicate the fraction of records with missing age within that gender group.\\n        \\n\\n### **Diagnosing MNAR (Missing Not At Random)**\\n\\n* True MNAR is **untestable from the data alone** (since it depends on the unobserved value).\\n    \\n* Requires:  \\n    • Domain expertise (e.g., we know high salaries are underreported).  \\n    • **Sensitivity analysis**: Assume plausible MNAR mechanisms and check how conclusions change.  \\n    • Specialized models (selection models, Heckman correction, pattern-mixture models).\\n    \\n\\n# **🛠 Handling Missing Data Mechanisms**\\n\\n### **1\\\\. MCAR**\\n\\n➡️ **Safe to drop rows or columns** (analysis unbiased, only reduced sample size).  \\nOptions:\\n\\n```python\\ndf_clean = df.dropna()                     # drop rows\\ndf_clean = df.dropna(axis=1, thresh=0.7*len(df))  # drop columns with \u003e30% missing\\n```\\n\\nOr simply use mean/median imputation without worrying about bias.\\n\\n---\\n\\n### **2\\\\. MAR**\\n\\n➡️ **Best handled by imputation methods that leverage observed data**.  \\nOptions:\\n\\n* **Group-wise imputation**:\\n    \\n    * We fill missing values in the income column by gender, using each gender's median income. It then stores the result back in the income column.\\n        \\n        ```python\\n        df['income'] = df.groupby('gender')['income'].transform(\\n            lambda x: x.fillna(x.median()))\\n        ```\\n        \\n* **Multiple Imputation (MICE/Iterative)**:\\n    \\n    * Use the other features to predict missing values, one column at a time, in a round-robin fashion\\n        \\n        ```python\\n        from sklearn.experimental import enable_iterative_imputer\\n        from sklearn.impute import IterativeImputer\\n        imp = IterativeImputer(random_state=0)\\n        df[numeric_cols] = imp.fit_transform(df[numeric_cols])\\n        ```\\n        \\n        * **How IterativeImputer works (high-level)**\\n            \\n            * It treats each feature with missing values as the target and uses the other features as predictors.\\n                \\n            * It iteratively:\\n                \\n                1. Regresses the missing values of one feature on the others.\\n                    \\n                2. Replaces missing values with the predicted ones.\\n                    \\n                3. Moves to the next feature with missing values and repeats.\\n                    \\n            * This process continues for several iterations until convergence, producing several imputed values that reflect potential relationships among features.\\n                \\n            * ### Step-by-step intuition:\\n                \\n                1. **Start with missing values**  \\n                    You have a dataset where some entries are `NaN`.\\n                    \\n                    Example:\\n                    \\n                    ```python\\n                    Age   Salary   Experience\\n                    25    50k      2\\n                    NaN   60k      3\\n                    30    NaN      4\\n                    28    55k      NaN\\n                    ```\\n                    \\n                2. **Choose one column with missing values (say Age).**  \\n                    Treat *Age* as the \\\"target variable\\\" and the other columns (Salary, Experience) as predictors.\\n                    \\n                3. **Train a regression model**  \\n                    Use the rows where *Age* is known to train a model like:\\n                    \\n                    ```python\\n                    Age ~ Salary + Experience\\n                    ```\\n                    \\n                4. **Predict missing values for Age**  \\n                    Use the trained model to fill in the `NaN`s in Age.\\n                    \\n                5. **Move to the next column with missing values (say Salary)**  \\n                    Now treat *Salary* as the target and use Age and Experience (with the newly imputed values) as predictors to estimate missing Salary.\\n                    \\n                6. **Repeat for all columns with missing values.**\\n                    \\n                7. **Iterate multiple times**  \\n                    Because each column’s imputation depends on the others (which may also be imputed), the algorithm cycles through all columns multiple times. With each iteration, the imputations become more stable and consistent.\\n                    \\n                    That’s why it’s called **Iterative** Imputer.\\n                    \\n                \\n            \\n            **When to use**\\n            \\n            * When you believe there are inter-feature relationships and you want a more sophisticated imputation than simple statistics (e.g., mean/median).\\n                \\n            * For datasets where the pattern of missingness might depend on other observed features.\\n                \\n            \\n            **Notes and best practices**\\n            \\n            * Ensure numeric\\\\_cols is a list of the numeric column names you want to impute.\\n                \\n            * You can customize the underlying estimator (e.g., linear regression, decision trees) by passing estimator= to the IterativeImputer.\\n                \\n            * Be mindful of potential data leakage: fit on training data only, and apply the same transformation to validation/test data.\\n                \\n* **Add missing indicator features**:\\n    \\n    ```python\\n    df['income_missing'] = df['income'].isna().astype(int)\\n    ```\\n    \\n\\n---\\n\\n### **3\\\\. MNAR**\\n\\n➡️ **Hardest case — no purely data-driven solution.**  \\nTypical strategies:\\n\\n* **Domain-informed imputation** (use expert rules, external benchmarks).\\n    \\n* **Sensitivity analysis**: Try imputations under different assumptions (e.g., assume the missing group is 10% higher/lower).\\n    \\n* **Heckman correction / selection models** (statsmodels has limited support).\\n    \\n* **Pattern mixture models / Bayesian methods**.\\n    \\n\\n👉 Often you need to:  \\nA. Report that data may be MNAR.  \\nB. Perform robustness checks (see if conclusions hold under different plausible imputations).\",\"content\":\"\u003ch1 id=\\\"heading-understanding-the-terms-using-an-analogy-shoppers-amp-shopping-carts-analogy\\\"\u003eUnderstanding the Terms Using an Analogy- \u003cstrong\u003eShoppers \u0026amp; Shopping Carts Analogy\u003c/strong\u003e\u003c/h1\u003e\\n\u003ch2 id=\\\"heading-1-mcar-missing-completely-at-random\\\"\u003e1. MCAR – Missing Completely At Random\u003c/h2\u003e\\n\u003cp\u003e\u003cstrong\u003eStory:\u003c/strong\u003e You’re analyzing what items shoppers add to their online carts. Suddenly, due to a system glitch, a random subset of cart items (across all shoppers) \u003cstrong\u003efails to get recorded\u003c/strong\u003e. The missingness has nothing to do with what the items are or who the shopper is — just bad luck\u003c/p\u003e\\n\u003cp\u003e\u003cstrong\u003eKey idea:\u003c/strong\u003e Missingness is completely random. Dropping the missing data still gives you a fair picture of what people buy.\u003c/p\u003e\\n\u003cp\u003e💡 Real-world analogy: A lab machine randomly crashes and corrupts some test results.\u003c/p\u003e\\n\u003ch2 id=\\\"heading-2-mar-missing-at-random\\\"\u003e2. MAR – Missing At Random\u003c/h2\u003e\\n\u003cp\u003e\u003cstrong\u003eStory:\u003c/strong\u003e Suppose items added using a \u003cstrong\u003emobile app\u003c/strong\u003e are sometimes not saved to the cart due to slower connections. But \u003cstrong\u003ewithin the mobile shoppers\u003c/strong\u003e, whether an item is missing doesn’t depend on the type of item itself — only on the platform used.\u003c/p\u003e\\n\u003cp\u003e\u003cstrong\u003eKey idea:\u003c/strong\u003e Missingness depends on an \u003cstrong\u003eobserved factor\u003c/strong\u003e (web vs mobile), but not on the item itself. If you know the shopper’s platform, you can correct for this bias.\u003c/p\u003e\\n\u003cp\u003e💡 Real-world analogy: Older survey respondents are less likely to report income → depends on age (observed), not income value itself.\u003c/p\u003e\\n\u003ch2 id=\\\"heading-3-mnar-missing-not-at-random\\\"\u003e3. MNAR – Missing Not At Random\u003c/h2\u003e\\n\u003cp\u003e\u003cstrong\u003eStory:\u003c/strong\u003e Some shoppers deliberately \u003cstrong\u003eremove expensive luxury items\u003c/strong\u003e from their cart before checking out, because they’re embarrassed or may not be able to afford them. The chance of an item being missing directly depends on the item’s \u003cstrong\u003eprice\u003c/strong\u003e itself (higher price → more likely to go missing).\u003c/p\u003e\\n\u003cp\u003e\u003cstrong\u003eKey idea:\u003c/strong\u003e Missingness depends \u003cstrong\u003edirectly on the unseen value\u003c/strong\u003e (price of item missing). Hardest case to handle, because the missingness itself hides something important.\u003c/p\u003e\\n\u003cp\u003e💡 Real-world analogy: High-income people avoid disclosing income → missingness depends on the true income itself.\u003c/p\u003e\\n\u003ch2 id=\\\"heading-recap\\\"\u003e📝 Recap\u003c/h2\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eMCAR:\u003c/strong\u003e Missing values are caused by pure chance → \u003cem\u003eA random system glitch deletes some cart items\u003c/em\u003e 🌐 — purely random.\u003c/p\u003e\\n\u003cp\u003e  \u003cstrong\u003eMAR:\u003c/strong\u003e Missingness depends on other observed variables, not the missing value itself → \u003cem\u003eMobile app shoppers lose items due to bad connection\u003c/em\u003e 📱 — depends on platform (observed), not the item itself.\u003c/p\u003e\\n\u003cp\u003e  \u003cstrong\u003eMNAR:\u003c/strong\u003e Missingness depends directly on the missing value itself → \u003cem\u003eShoppers deliberately remove very expensive items\u003c/em\u003e 💎 — depends on the value (price) that’s missing.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003e👉 Think: \u003cstrong\u003eMCAR = random glitch 💨, MAR = depends on something you can observe 👀, MNAR = depends on what’s hidden 🔒.\u003c/strong\u003e\u003c/p\u003e\\n\u003ch1 id=\\\"heading-how-to-check-programmatically\\\"\u003eHow To Check Programmatically\u003c/h1\u003e\\n\u003ch2 id=\\\"heading-testing-mcar-missing-completely-at-random\\\"\u003e\u003cstrong\u003eTesting MCAR (Missing Completely At Random)\u003c/strong\u003e\u003c/h2\u003e\\n\u003ch3 id=\\\"heading-littles-mcar-test\\\"\u003eLittle’s MCAR test\u003c/h3\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eONLY FOR NUMERIC COLUMNS ☹\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003ePurpose\u003c/strong\u003e: Global test for whether data are Missing Completely At Random (MCAR) across the whole dataset, considering multivariate patterns of missingness.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eInterpretation\u003c/strong\u003e:\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cul\u003e\\n\u003cli\u003e\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eHigh p value → cannot reject MCAR (missingness is plausibly random).\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eLow p value → reject MCAR (missingness related to data → MAR or MNAR).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eStrength\u003c/strong\u003e: Multivariate test, designed for exactly this situation.\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cstrong\u003eWeakness\u003c/strong\u003e: Sensitive to sample size, not always available in every package.\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003e\u003cspan class=\\\"hljs-comment\\\"\u003e# 1. LITTLE’S MCAR TEST ---------------------------------------\u003c/span\u003e\\n\u003cspan class=\\\"hljs-comment\\\"\u003e# note: not natively in statsmodels yet (as of v0.14)\u003c/span\u003e\\n\u003cspan class=\\\"hljs-comment\\\"\u003e# package option: `little-mcar-test`\u003c/span\u003e\\n\u003cspan class=\\\"hljs-comment\\\"\u003e# install: pip install little-mcar-test\u003c/span\u003e\\n\\n\u003cspan class=\\\"hljs-keyword\\\"\u003efrom\u003c/span\u003e little_mcar_test \u003cspan class=\\\"hljs-keyword\\\"\u003eimport\u003c/span\u003e mcartest\\n\\n\u003cspan class=\\\"hljs-comment\\\"\u003e# run on numeric part of your dataset\u003c/span\u003e\\nnumeric_df = df.select_dtypes(include=[np.number])\\nstat, dof, p_value = mcartest(numeric_df.to_numpy())\\nprint(\u003cspan class=\\\"hljs-string\\\"\u003ef\\\"Little’s MCAR test: χ2 = \u003cspan class=\\\"hljs-subst\\\"\u003e{stat}\u003c/span\u003e, dof = \u003cspan class=\\\"hljs-subst\\\"\u003e{dof}\u003c/span\u003e, p = \u003cspan class=\\\"hljs-subst\\\"\u003e{p_value}\u003c/span\u003e\\\"\u003c/span\u003e)\\n\\n\u003cspan class=\\\"hljs-keyword\\\"\u003eif\u003c/span\u003e p_value \u0026gt; \u003cspan class=\\\"hljs-number\\\"\u003e0.05\u003c/span\u003e:\\n    print(\u003cspan class=\\\"hljs-string\\\"\u003e\\\"Fail to reject MCAR → data plausibly MCAR\\\"\u003c/span\u003e)\\n\u003cspan class=\\\"hljs-keyword\\\"\u003eelse\u003c/span\u003e:\\n    print(\u003cspan class=\\\"hljs-string\\\"\u003e\\\"Reject MCAR → data likely MAR or MNAR\\\"\u003c/span\u003e)\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003ch3 id=\\\"heading-cross-tab-with-target-chi-square\\\"\u003eCross-tab with Target + Chi-square\u003c/h3\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eWe create a new column first (a “missingness indicator”)\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eCross-tabulate this with the target variable and run a chi-square test of independence.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003e\u003cspan class=\\\"hljs-keyword\\\"\u003eimport\u003c/span\u003e pandas \u003cspan class=\\\"hljs-keyword\\\"\u003eas\u003c/span\u003e pd\\n\u003cspan class=\\\"hljs-keyword\\\"\u003efrom\u003c/span\u003e scipy.stats \u003cspan class=\\\"hljs-keyword\\\"\u003eimport\u003c/span\u003e chi2_contingency\\n\u003cspan class=\\\"hljs-comment\\\"\u003e# heuristic check: does missingness in one col depend on another categorical col?\u003c/span\u003e\\n\\ndf[\u003cspan class=\\\"hljs-string\\\"\u003e'col_missing'\u003c/span\u003e] = df[\u003cspan class=\\\"hljs-string\\\"\u003e'some_feature'\u003c/span\u003e].isna().astype(int)\\nctab = pd.crosstab(df[\u003cspan class=\\\"hljs-string\\\"\u003e'col_missing'\u003c/span\u003e], df[\u003cspan class=\\\"hljs-string\\\"\u003e'target'\u003c/span\u003e])\\nchi2, p, dof, exp = chi2_contingency(ctab)\\nprint(\u003cspan class=\\\"hljs-string\\\"\u003e\\\"p-value =\\\"\u003c/span\u003e, p)\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eInterpretation\u003c/strong\u003e:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eIf p \u0026lt; 0.05 → missingness of this feature is associated with the target.\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003eThat suggests missingness carries information → at least MAR, possibly MNAR.\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eStrength\u003c/strong\u003e: Very intuitive for predictive modeling (if missingness depends on target → definitely not MCAR).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eWeakness\u003c/strong\u003e:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eOnly checks relation with target, not with all other features.\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003eSo you might miss situations where missingness is related to a predictor but not the target.\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003ch3 id=\\\"heading-compare-distributions-manually\\\"\u003eCompare distributions manually\u003c/h3\u003e\\n\u003cul\u003e\\n\u003cli\u003eExamine whether the distribution of observed variables differs between rows with and without missing values (should not).\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003edf.groupby(df[\u003cspan class=\\\"hljs-string\\\"\u003e'col_with_missing'\u003c/span\u003e].isna())[\u003cspan class=\\\"hljs-string\\\"\u003e'other_col'\u003c/span\u003e].mean()\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003ch2 id=\\\"heading-testing-for-mar-missing-at-random\\\"\u003e\u003cstrong\u003eTesting for MAR (Missing At Random)\u003c/strong\u003e\u003c/h2\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eStrict tests don’t exist (because MAR involves unobserved missing values).\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003ePractical check\u003c/strong\u003e: Create \\\"missingness indicator\\\" (flag if a variable is missing) and test correlation with other observed variables.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003e    df[\u003cspan class=\\\"hljs-string\\\"\u003e'age_missing'\u003c/span\u003e] = df[\u003cspan class=\\\"hljs-string\\\"\u003e'age'\u003c/span\u003e].isna()\\n\\n    \u003cspan class=\\\"hljs-comment\\\"\u003e#We want to compare age with gender to assess if missing age \u003c/span\u003e\\n    \u003cspan class=\\\"hljs-comment\\\"\u003e#is related to gender value.\u003c/span\u003e\\n    \u003cspan class=\\\"hljs-comment\\\"\u003e#As we are grouping by gender and then calculating mean of the age_missing column, \u003c/span\u003e\\n    \u003cspan class=\\\"hljs-comment\\\"\u003e#this mean equals the proportion of missing ages in that group.\u003c/span\u003e\\n    df.groupby(\u003cspan class=\\\"hljs-string\\\"\u003e'gender'\u003c/span\u003e)[\u003cspan class=\\\"hljs-string\\\"\u003e'age_missing'\u003c/span\u003e].mean()\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eWhat this tells you\u003c/strong\u003e\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eThe result is the missingness rate of the \u003ccode\u003eage\u003c/code\u003e column for each gender:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eIf the value is 0.0 for a gender, no missing ages in that group.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eIf the value is 1.0 for a gender, all ages are missing in that group.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eValues between 0 and 1 indicate the fraction of records with missing age within that gender group.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003ch3 id=\\\"heading-diagnosing-mnar-missing-not-at-random\\\"\u003e\u003cstrong\u003eDiagnosing MNAR (Missing Not At Random)\u003c/strong\u003e\u003c/h3\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eTrue MNAR is \u003cstrong\u003euntestable from the data alone\u003c/strong\u003e (since it depends on the unobserved value).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eRequires:\u003cbr /\u003e  • Domain expertise (e.g., we know high salaries are underreported).\u003cbr /\u003e  • \u003cstrong\u003eSensitivity analysis\u003c/strong\u003e: Assume plausible MNAR mechanisms and check how conclusions change.\u003cbr /\u003e  • Specialized models (selection models, Heckman correction, pattern-mixture models).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003ch1 id=\\\"heading-handling-missing-data-mechanisms\\\"\u003e\u003cstrong\u003e🛠 Handling Missing Data Mechanisms\u003c/strong\u003e\u003c/h1\u003e\\n\u003ch3 id=\\\"heading-1-mcar\\\"\u003e\u003cstrong\u003e1. MCAR\u003c/strong\u003e\u003c/h3\u003e\\n\u003cp\u003e➡️ \u003cstrong\u003eSafe to drop rows or columns\u003c/strong\u003e (analysis unbiased, only reduced sample size).\u003cbr /\u003eOptions:\u003c/p\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003edf_clean = df.dropna()                     \u003cspan class=\\\"hljs-comment\\\"\u003e# drop rows\u003c/span\u003e\\ndf_clean = df.dropna(axis=\u003cspan class=\\\"hljs-number\\\"\u003e1\u003c/span\u003e, thresh=\u003cspan class=\\\"hljs-number\\\"\u003e0.7\u003c/span\u003e*len(df))  \u003cspan class=\\\"hljs-comment\\\"\u003e# drop columns with \u0026gt;30% missing\u003c/span\u003e\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eOr simply use mean/median imputation without worrying about bias.\u003c/p\u003e\\n\u003chr /\u003e\\n\u003ch3 id=\\\"heading-2-mar\\\"\u003e\u003cstrong\u003e2. MAR\u003c/strong\u003e\u003c/h3\u003e\\n\u003cp\u003e➡️ \u003cstrong\u003eBest handled by imputation methods that leverage observed data\u003c/strong\u003e.\u003cbr /\u003eOptions:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eGroup-wise imputation\u003c/strong\u003e:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eWe fill missing values in the income column by gender, using each gender's median income. It then stores the result back in the income column.\u003c/p\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003e  df[\u003cspan class=\\\"hljs-string\\\"\u003e'income'\u003c/span\u003e] = df.groupby(\u003cspan class=\\\"hljs-string\\\"\u003e'gender'\u003c/span\u003e)[\u003cspan class=\\\"hljs-string\\\"\u003e'income'\u003c/span\u003e].transform(\\n      \u003cspan class=\\\"hljs-keyword\\\"\u003elambda\u003c/span\u003e x: x.fillna(x.median()))\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eMultiple Imputation (MICE/Iterative)\u003c/strong\u003e:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eUse the other features to predict missing values, one column at a time, in a round-robin fashion\u003c/p\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003e  \u003cspan class=\\\"hljs-keyword\\\"\u003efrom\u003c/span\u003e sklearn.experimental \u003cspan class=\\\"hljs-keyword\\\"\u003eimport\u003c/span\u003e enable_iterative_imputer\\n  \u003cspan class=\\\"hljs-keyword\\\"\u003efrom\u003c/span\u003e sklearn.impute \u003cspan class=\\\"hljs-keyword\\\"\u003eimport\u003c/span\u003e IterativeImputer\\n  imp = IterativeImputer(random_state=\u003cspan class=\\\"hljs-number\\\"\u003e0\u003c/span\u003e)\\n  df[numeric_cols] = imp.fit_transform(df[numeric_cols])\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eHow IterativeImputer works (high-level)\u003c/strong\u003e\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eIt treats each feature with missing values as the target and uses the other features as predictors.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eIt iteratively:\u003c/p\u003e\\n\u003col\u003e\\n\u003cli\u003e\u003cp\u003eRegresses the missing values of one feature on the others.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eReplaces missing values with the predicted ones.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eMoves to the next feature with missing values and repeats.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ol\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eThis process continues for several iterations until convergence, producing several imputed values that reflect potential relationships among features.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003ch3 id=\\\"heading-step-by-step-intuition\\\"\u003eStep-by-step intuition:\u003c/h3\u003e\\n\u003col\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eStart with missing values\u003c/strong\u003e\u003cbr /\u003e You have a dataset where some entries are \u003ccode\u003eNaN\u003c/code\u003e.\u003c/p\u003e\\n\u003cp\u003e Example:\u003c/p\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003e Age   Salary   Experience\\n \u003cspan class=\\\"hljs-number\\\"\u003e25\u003c/span\u003e    \u003cspan class=\\\"hljs-number\\\"\u003e50\u003c/span\u003ek      \u003cspan class=\\\"hljs-number\\\"\u003e2\u003c/span\u003e\\n NaN   \u003cspan class=\\\"hljs-number\\\"\u003e60\u003c/span\u003ek      \u003cspan class=\\\"hljs-number\\\"\u003e3\u003c/span\u003e\\n \u003cspan class=\\\"hljs-number\\\"\u003e30\u003c/span\u003e    NaN      \u003cspan class=\\\"hljs-number\\\"\u003e4\u003c/span\u003e\\n \u003cspan class=\\\"hljs-number\\\"\u003e28\u003c/span\u003e    \u003cspan class=\\\"hljs-number\\\"\u003e55\u003c/span\u003ek      NaN\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eChoose one column with missing values (say Age).\u003c/strong\u003e\u003cbr /\u003e Treat \u003cem\u003eAge\u003c/em\u003e as the \\\"target variable\\\" and the other columns (Salary, Experience) as predictors.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eTrain a regression model\u003c/strong\u003e\u003cbr /\u003e Use the rows where \u003cem\u003eAge\u003c/em\u003e is known to train a model like:\u003c/p\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003e Age ~ Salary + Experience\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003ePredict missing values for Age\u003c/strong\u003e\u003cbr /\u003e Use the trained model to fill in the \u003ccode\u003eNaN\u003c/code\u003es in Age.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eMove to the next column with missing values (say Salary)\u003c/strong\u003e\u003cbr /\u003e Now treat \u003cem\u003eSalary\u003c/em\u003e as the target and use Age and Experience (with the newly imputed values) as predictors to estimate missing Salary.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eRepeat for all columns with missing values.\u003c/strong\u003e\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eIterate multiple times\u003c/strong\u003e\u003cbr /\u003e Because each column’s imputation depends on the others (which may also be imputed), the algorithm cycles through all columns multiple times. With each iteration, the imputations become more stable and consistent.\u003c/p\u003e\\n\u003cp\u003e That’s why it’s called \u003cstrong\u003eIterative\u003c/strong\u003e Imputer.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ol\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003e            \u003cstrong\u003eWhen to use\u003c/strong\u003e\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eWhen you believe there are inter-feature relationships and you want a more sophisticated imputation than simple statistics (e.g., mean/median).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eFor datasets where the pattern of missingness might depend on other observed features.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003e            \u003cstrong\u003eNotes and best practices\u003c/strong\u003e\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eEnsure numeric_cols is a list of the numeric column names you want to impute.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eYou can customize the underlying estimator (e.g., linear regression, decision trees) by passing estimator= to the IterativeImputer.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eBe mindful of potential data leakage: fit on training data only, and apply the same transformation to validation/test data.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eAdd missing indicator features\u003c/strong\u003e:\u003c/p\u003e\\n\u003cpre\u003e\u003ccode class=\\\"lang-python\\\"\u003e  df[\u003cspan class=\\\"hljs-string\\\"\u003e'income_missing'\u003c/span\u003e] = df[\u003cspan class=\\\"hljs-string\\\"\u003e'income'\u003c/span\u003e].isna().astype(int)\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003chr /\u003e\\n\u003ch3 id=\\\"heading-3-mnar\\\"\u003e\u003cstrong\u003e3. MNAR\u003c/strong\u003e\u003c/h3\u003e\\n\u003cp\u003e➡️ \u003cstrong\u003eHardest case — no purely data-driven solution.\u003c/strong\u003e\u003cbr /\u003eTypical strategies:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eDomain-informed imputation\u003c/strong\u003e (use expert rules, external benchmarks).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eSensitivity analysis\u003c/strong\u003e: Try imputations under different assumptions (e.g., assume the missing group is 10% higher/lower).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eHeckman correction / selection models\u003c/strong\u003e (statsmodels has limited support).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003ePattern mixture models / Bayesian methods\u003c/strong\u003e.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003e👉 Often you need to:\u003cbr /\u003eA. Report that data may be MNAR.\u003cbr /\u003eB. Perform robustness checks (see if conclusions hold under different plausible imputations).\u003c/p\u003e\\n\",\"cuid\":\"cmg82xvor000002ifbt0ccbob\",\"views\":14,\"title\":\"📘 Mcar, Mar, Mnar —\",\"slug\":\"mcar-mar-mnar\",\"dateAdded\":\"2025-10-01T14:26:36.843Z\",\"dateUpdated\":\"2025-10-01T16:16:07.545Z\",\"type\":\"story\",\"coverImage\":\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759335348948/4ead57b0-0b81-454f-a5d5-938972c37be8.jpeg\",\"isCoverImagePortrait\":false,\"isCoverAttributionHidden\":false,\"brief\":\"Understanding the Terms Using an Analogy- Shoppers \u0026 Shopping Carts Analogy\\n1. MCAR – Missing Completely At Random\\nStory: You’re analyzing what items shoppers add to their online carts. Suddenly, due to a system glitch, a random subset of cart items ...\",\"isFollowing\":false,\"totalReactions\":0,\"totalReactionsByCurrentUser\":0,\"series\":null,\"isPinnedToBlog\":false,\"readTime\":7,\"sB\":false,\"isAMA\":false,\"subtitle\":\"\",\"isPartOfSeries\":false,\"hasTags\":true,\"ogImage\":\"\",\"metaTitle\":\"\",\"metaDescription\":\"\",\"isRepublished\":false,\"autoPublishedFromRSS\":false,\"responses\":[],\"isFeatured\":false,\"hasLatex\":false,\"stickCoverToBottom\":false,\"hideBadges\":false,\"badges\":[],\"isDelisted\":false,\"audioUrls\":{},\"disableComments\":false,\"enableToc\":true,\"toc\":[[{\"id\":\"0d54f408-d734-4f19-ae67-76fd78baf26b\",\"level\":1,\"slug\":\"understanding-the-terms-using-an-analogy-shoppers-amp-shopping-carts-analogy\",\"title\":\"Understanding the Terms Using an Analogy- Shoppers \u0026amp; Shopping Carts Analogy\",\"parentId\":null}],[{\"id\":\"84251c14-d5ea-4069-a4c2-7e803e38760e\",\"level\":2,\"slug\":\"1-mcar-missing-completely-at-random\",\"title\":\"1. MCAR – Missing Completely At Random\",\"parentId\":\"0d54f408-d734-4f19-ae67-76fd78baf26b\"}],[{\"id\":\"27d4fa19-f1be-44bd-a38d-47acb0026af2\",\"level\":2,\"slug\":\"2-mar-missing-at-random\",\"title\":\"2. MAR – Missing At Random\",\"parentId\":\"0d54f408-d734-4f19-ae67-76fd78baf26b\"}],[{\"id\":\"924bf17c-929c-4081-ab52-82014f57551f\",\"level\":2,\"slug\":\"3-mnar-missing-not-at-random\",\"title\":\"3. MNAR – Missing Not At Random\",\"parentId\":\"0d54f408-d734-4f19-ae67-76fd78baf26b\"}],[{\"id\":\"cb081174-cd0f-4a49-bbca-9e7ae6a215f1\",\"level\":2,\"slug\":\"recap\",\"title\":\"📝 Recap\",\"parentId\":\"0d54f408-d734-4f19-ae67-76fd78baf26b\"}],[{\"id\":\"d502bfec-4f5a-4afd-ac1a-782a847d858b\",\"level\":1,\"slug\":\"how-to-check-programmatically\",\"title\":\"How To Check Programmatically\",\"parentId\":null}],[{\"id\":\"72c60015-95be-4d18-8d21-363a08fa58ce\",\"level\":2,\"slug\":\"testing-mcar-missing-completely-at-random\",\"title\":\"Testing MCAR (Missing Completely At Random)\",\"parentId\":\"d502bfec-4f5a-4afd-ac1a-782a847d858b\"}],[{\"id\":\"f498281d-b432-4f82-b897-61fe79ece742\",\"level\":3,\"slug\":\"littles-mcar-test\",\"title\":\"Little’s MCAR test\",\"parentId\":\"72c60015-95be-4d18-8d21-363a08fa58ce\"}],[{\"id\":\"c75365a6-f524-4f99-a806-c5bc7ae26187\",\"level\":3,\"slug\":\"cross-tab-with-target-chi-square\",\"title\":\"Cross-tab with Target + Chi-square\",\"parentId\":\"72c60015-95be-4d18-8d21-363a08fa58ce\"}],[{\"id\":\"af722907-885e-4267-a27d-61d6a5fd6330\",\"level\":3,\"slug\":\"compare-distributions-manually\",\"title\":\"Compare distributions manually\",\"parentId\":\"72c60015-95be-4d18-8d21-363a08fa58ce\"}],[{\"id\":\"a257c107-58dc-4add-8786-637ea8cc5224\",\"level\":2,\"slug\":\"testing-for-mar-missing-at-random\",\"title\":\"Testing for MAR (Missing At Random)\",\"parentId\":\"d502bfec-4f5a-4afd-ac1a-782a847d858b\"}],[{\"id\":\"2022af84-7ec0-4ae1-920e-54922b26faf3\",\"level\":3,\"slug\":\"diagnosing-mnar-missing-not-at-random\",\"title\":\"Diagnosing MNAR (Missing Not At Random)\",\"parentId\":\"a257c107-58dc-4add-8786-637ea8cc5224\"}],[{\"id\":\"681eba31-4e59-46ff-bd96-a3857f93a4fe\",\"level\":1,\"slug\":\"handling-missing-data-mechanisms\",\"title\":\"🛠 Handling Missing Data Mechanisms\",\"parentId\":null}],[{\"id\":\"255f15b0-6d3e-4af5-b590-1600cc1e474e\",\"level\":3,\"slug\":\"1-mcar\",\"title\":\"1. MCAR\",\"parentId\":\"681eba31-4e59-46ff-bd96-a3857f93a4fe\"}],[{\"id\":\"5e02dd39-772e-4db6-b37e-41765e33d8d0\",\"level\":3,\"slug\":\"2-mar\",\"title\":\"2. MAR\",\"parentId\":\"681eba31-4e59-46ff-bd96-a3857f93a4fe\"}],[{\"id\":\"55cec363-4166-4e25-b34f-a01021ec1593\",\"level\":3,\"slug\":\"step-by-step-intuition\",\"title\":\"Step-by-step intuition:\",\"parentId\":\"681eba31-4e59-46ff-bd96-a3857f93a4fe\"}],[{\"id\":\"43c07aa0-e737-4b4c-83d3-5b157798fe91\",\"level\":3,\"slug\":\"3-mnar\",\"title\":\"3. MNAR\",\"parentId\":\"681eba31-4e59-46ff-bd96-a3857f93a4fe\"}]],\"noIndex\":false}","legacySeriesJSON":null,"headProps":{"title":"📘 Mcar, Mar, Mnar —","description":"Understanding the Terms Using an Analogy- Shoppers \u0026 Shopping Carts Analogy\n1. MCAR – Missing Completely At Random\nStory: You’re analyzing what items shoppers add to their online carts. Suddenly, due to a system glitch, a random subset of cart items ...","author":{"name":"Amit Gupta","username":"learnwithamit"},"links":[{"rel":"canonical","href":"https://amitguptaforwork.hashnode.dev/mcar-mar-mnar"}],"pageType":"article","bannerType":"large","ogSiteName":"Amit's Learning Notes","url":"https://amitguptaforwork.hashnode.dev/mcar-mar-mnar","ogImage":"https://hashnode.com/utility/r?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1759335348948%2F4ead57b0-0b81-454f-a5d5-938972c37be8.jpeg%3Fw%3D1200%26h%3D630%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp%26fm%3Dpng","twitterImage":"https://hashnode.com/utility/r?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1759335348948%2F4ead57b0-0b81-454f-a5d5-938972c37be8.jpeg%3Fw%3D1200%26h%3D630%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp%26fm%3Dpng","twitterHandle":"","monetization":null,"style":{},"customHeadItems":{"customFavicon":null,"customTheme":null,"customMeta":null},"hljs":true},"isDarkTheme":false,"headerColor":null,"isBadge":null,"isRecommendations":null,"isHome":null,"currentMenuId":null,"hnmcMode":false,"postCUID":"cmg82xvor000002ifbt0ccbob","seoSchema":{"@context":"https://schema.org","@type":"NewsArticle","url":"https://amitguptaforwork.hashnode.dev/mcar-mar-mnar","mainEntityOfPage":"https://amitguptaforwork.hashnode.dev/mcar-mar-mnar","headline":"📘 Mcar, Mar, Mnar —","description":"Understanding the Terms Using an Analogy- Shoppers \u0026amp; Shopping Carts Analogy\n1. MCAR – Missing Completely At Random\nStory: You’re analyzing what items shoppers add to their online carts. Suddenly, due to a system glitch, a random subset of cart items ...","datePublished":"2025-10-01T14:26:36.843Z","dateModified":"2025-10-01T16:16:07.545Z","isAccessibleForFree":true,"author":{"@type":"Person","name":"Amit Gupta","url":"https://hashnode.com/@learnwithamit"},"publisher":{"@type":"Organization","name":"Amit's Learning Notes","url":"https://amitguptaforwork.hashnode.dev","logo":"https://hashnode.com/utility/r?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1559814205701%2Fek9fO-yT0.jpeg%3Fw%3D800%26bm%3Dnormal%26balph%3D100%26txt64%3DQW1pdCdzIExlYXJuaW5nIE5vdGVz%26txtsize%3D42%26txtfit%3Dmax%26txtalign%3Dmiddle%2Ccenter%26txtfont%3DHelvetica%20Neue%2CBold%26txtclr%3D000000%26blend%3Dffffff"},"image":{"@type":"ImageObject","url":"https://cdn.hashnode.com/res/hashnode/image/upload/v1759335348948/4ead57b0-0b81-454f-a5d5-938972c37be8.jpeg"}},"publication":{"__typename":"Publication","id":"68dd2c04a3a82b4b10e631ed","url":"https://amitguptaforwork.hashnode.dev","canonicalURL":"https://amitguptaforwork.hashnode.dev","urlPattern":"SIMPLE","title":"Amit's Learning Notes","displayTitle":null,"hasBadges":false,"descriptionSEO":"Just wanted to share my notes as I learn new exciting things\n","publicMembers":{"totalDocuments":1},"about":{"html":"\u003cp\u003eJust wanted to share my notes as I learn new exciting things\u003c/p\u003e\n","text":"Just wanted to share my notes as I learn new exciting things\n"},"features":{"proTeam":{"isEnabled":false},"newsletter":{"isEnabled":true},"viewCount":{"isEnabled":false},"readTime":{"isEnabled":true},"textSelectionSharer":{"isEnabled":true},"customCSS":{"isEnabled":false,"published":null,"draft":null},"gptBotCrawling":{"__typename":"GPTBotCrawlingFeature","isEnabled":false}},"metaTags":null,"ogMetaData":{"image":null},"author":{"__typename":"User","id":"67fce0562e7f83246dceb67d","name":"Amit Gupta","username":"learnwithamit","profilePicture":"https://cdn.hashnode.com/res/hashnode/image/upload/v1744625750145/964daf99-0a9d-4c05-89d2-b108baacda11.png"},"preferences":{"__typename":"Preferences","logo":null,"darkMode":{"__typename":"DarkModePreferences","logo":null,"enabled":false},"navbarItems":[],"enabledPages":{"__typename":"PagesPreferences","badges":false,"newsletter":true,"members":true},"layout":"grid","disableFooterBranding":false,"isSubscriptionModalDisabled":false},"favicon":null,"headerColor":null,"integrations":{"fbPixelID":null,"fathomSiteID":null,"fathomCustomDomainEnabled":null,"fathomCustomDomain":null,"hotjarSiteID":null,"matomoSiteID":null,"matomoURL":null,"gaTrackingID":null,"gTagManagerID":null,"plausibleAnalyticsEnabled":null,"koalaPublicKey":null,"msClarityID":null},"imprintV2":null,"postsCount":{"totalDocuments":2},"isTeam":true,"links":{"twitter":null,"instagram":null,"github":null,"website":"https://amitguptaforwork.github.io/","hashnode":null,"youtube":null,"dailydev":null,"linkedin":"https://www.linkedin.com/in/amitguptaforwork/","mastodon":null,"facebook":null,"bluesky":null},"domainInfo":{"__typename":"DomainInfo","hashnodeSubdomain":"amitguptaforwork","domain":null,"wwwPrefixedDomain":null},"redirectionRules":[],"totalRecommendedPublications":0,"sponsorship":{"content":null,"stripe":null},"allowContributorEdits":true,"rssImport":null,"post":{"id":"68dd3a1cf0b5f3410cada41b","cuid":"cmg82xvor000002ifbt0ccbob","title":"📘 Mcar, Mar, Mnar —","subtitle":null,"slug":"mcar-mar-mnar","brief":"Understanding the Terms Using an Analogy- Shoppers \u0026 Shopping Carts Analogy\n1. MCAR – Missing Completely At Random\nStory: You’re analyzing what items shoppers add to their online carts. Suddenly, due to a system glitch, a random subset of cart items ...","featured":false,"publishedAt":"2025-10-01T14:26:36.843Z","updatedAt":"2025-10-01T16:16:07.545Z","author":{"__typename":"User","id":"67fce0562e7f83246dceb67d","name":"Amit Gupta","username":"learnwithamit","deactivated":false,"profilePicture":"https://cdn.hashnode.com/res/hashnode/image/upload/v1744625750145/964daf99-0a9d-4c05-89d2-b108baacda11.png","bio":{"html":""},"socialMediaLinks":{"website":null,"github":null,"twitter":null,"facebook":null,"stackoverflow":null,"linkedin":null}},"coAuthors":[],"seo":{"title":null,"description":null,"shouldNotIndex":false},"coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/upload/v1759335348948/4ead57b0-0b81-454f-a5d5-938972c37be8.jpeg","isPortrait":false,"attribution":null,"isAttributionHidden":false,"photographer":null},"responseCount":0,"reactionCount":0,"replyCount":0,"content":{"html":"\u003ch1 id=\"heading-understanding-the-terms-using-an-analogy-shoppers-amp-shopping-carts-analogy\"\u003eUnderstanding the Terms Using an Analogy- \u003cstrong\u003eShoppers \u0026amp; Shopping Carts Analogy\u003c/strong\u003e\u003c/h1\u003e\n\u003ch2 id=\"heading-1-mcar-missing-completely-at-random\"\u003e1. MCAR – Missing Completely At Random\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eStory:\u003c/strong\u003e You’re analyzing what items shoppers add to their online carts. Suddenly, due to a system glitch, a random subset of cart items (across all shoppers) \u003cstrong\u003efails to get recorded\u003c/strong\u003e. The missingness has nothing to do with what the items are or who the shopper is — just bad luck\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eKey idea:\u003c/strong\u003e Missingness is completely random. Dropping the missing data still gives you a fair picture of what people buy.\u003c/p\u003e\n\u003cp\u003e💡 Real-world analogy: A lab machine randomly crashes and corrupts some test results.\u003c/p\u003e\n\u003ch2 id=\"heading-2-mar-missing-at-random\"\u003e2. MAR – Missing At Random\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eStory:\u003c/strong\u003e Suppose items added using a \u003cstrong\u003emobile app\u003c/strong\u003e are sometimes not saved to the cart due to slower connections. But \u003cstrong\u003ewithin the mobile shoppers\u003c/strong\u003e, whether an item is missing doesn’t depend on the type of item itself — only on the platform used.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eKey idea:\u003c/strong\u003e Missingness depends on an \u003cstrong\u003eobserved factor\u003c/strong\u003e (web vs mobile), but not on the item itself. If you know the shopper’s platform, you can correct for this bias.\u003c/p\u003e\n\u003cp\u003e💡 Real-world analogy: Older survey respondents are less likely to report income → depends on age (observed), not income value itself.\u003c/p\u003e\n\u003ch2 id=\"heading-3-mnar-missing-not-at-random\"\u003e3. MNAR – Missing Not At Random\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eStory:\u003c/strong\u003e Some shoppers deliberately \u003cstrong\u003eremove expensive luxury items\u003c/strong\u003e from their cart before checking out, because they’re embarrassed or may not be able to afford them. The chance of an item being missing directly depends on the item’s \u003cstrong\u003eprice\u003c/strong\u003e itself (higher price → more likely to go missing).\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eKey idea:\u003c/strong\u003e Missingness depends \u003cstrong\u003edirectly on the unseen value\u003c/strong\u003e (price of item missing). Hardest case to handle, because the missingness itself hides something important.\u003c/p\u003e\n\u003cp\u003e💡 Real-world analogy: High-income people avoid disclosing income → missingness depends on the true income itself.\u003c/p\u003e\n\u003ch2 id=\"heading-recap\"\u003e📝 Recap\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eMCAR:\u003c/strong\u003e Missing values are caused by pure chance → \u003cem\u003eA random system glitch deletes some cart items\u003c/em\u003e 🌐 — purely random.\u003c/p\u003e\n\u003cp\u003e  \u003cstrong\u003eMAR:\u003c/strong\u003e Missingness depends on other observed variables, not the missing value itself → \u003cem\u003eMobile app shoppers lose items due to bad connection\u003c/em\u003e 📱 — depends on platform (observed), not the item itself.\u003c/p\u003e\n\u003cp\u003e  \u003cstrong\u003eMNAR:\u003c/strong\u003e Missingness depends directly on the missing value itself → \u003cem\u003eShoppers deliberately remove very expensive items\u003c/em\u003e 💎 — depends on the value (price) that’s missing.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e👉 Think: \u003cstrong\u003eMCAR = random glitch 💨, MAR = depends on something you can observe 👀, MNAR = depends on what’s hidden 🔒.\u003c/strong\u003e\u003c/p\u003e\n\u003ch1 id=\"heading-how-to-check-programmatically\"\u003eHow To Check Programmatically\u003c/h1\u003e\n\u003ch2 id=\"heading-testing-mcar-missing-completely-at-random\"\u003e\u003cstrong\u003eTesting MCAR (Missing Completely At Random)\u003c/strong\u003e\u003c/h2\u003e\n\u003ch3 id=\"heading-littles-mcar-test\"\u003eLittle’s MCAR test\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eONLY FOR NUMERIC COLUMNS ☹\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003ePurpose\u003c/strong\u003e: Global test for whether data are Missing Completely At Random (MCAR) across the whole dataset, considering multivariate patterns of missingness.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eInterpretation\u003c/strong\u003e:\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cul\u003e\n\u003cli\u003e\u003cul\u003e\n\u003cli\u003e\u003cp\u003eHigh p value → cannot reject MCAR (missingness is plausibly random).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eLow p value → reject MCAR (missingness related to data → MAR or MNAR).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eStrength\u003c/strong\u003e: Multivariate test, designed for exactly this situation.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eWeakness\u003c/strong\u003e: Sensitive to sample size, not always available in every package.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e\u003cspan class=\"hljs-comment\"\u003e# 1. LITTLE’S MCAR TEST ---------------------------------------\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# note: not natively in statsmodels yet (as of v0.14)\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# package option: `little-mcar-test`\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# install: pip install little-mcar-test\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e little_mcar_test \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e mcartest\n\n\u003cspan class=\"hljs-comment\"\u003e# run on numeric part of your dataset\u003c/span\u003e\nnumeric_df = df.select_dtypes(include=[np.number])\nstat, dof, p_value = mcartest(numeric_df.to_numpy())\nprint(\u003cspan class=\"hljs-string\"\u003ef\"Little’s MCAR test: χ2 = \u003cspan class=\"hljs-subst\"\u003e{stat}\u003c/span\u003e, dof = \u003cspan class=\"hljs-subst\"\u003e{dof}\u003c/span\u003e, p = \u003cspan class=\"hljs-subst\"\u003e{p_value}\u003c/span\u003e\"\u003c/span\u003e)\n\n\u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e p_value \u0026gt; \u003cspan class=\"hljs-number\"\u003e0.05\u003c/span\u003e:\n    print(\u003cspan class=\"hljs-string\"\u003e\"Fail to reject MCAR → data plausibly MCAR\"\u003c/span\u003e)\n\u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e:\n    print(\u003cspan class=\"hljs-string\"\u003e\"Reject MCAR → data likely MAR or MNAR\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"heading-cross-tab-with-target-chi-square\"\u003eCross-tab with Target + Chi-square\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eWe create a new column first (a “missingness indicator”)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eCross-tabulate this with the target variable and run a chi-square test of independence.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pandas \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e pd\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e scipy.stats \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e chi2_contingency\n\u003cspan class=\"hljs-comment\"\u003e# heuristic check: does missingness in one col depend on another categorical col?\u003c/span\u003e\n\ndf[\u003cspan class=\"hljs-string\"\u003e'col_missing'\u003c/span\u003e] = df[\u003cspan class=\"hljs-string\"\u003e'some_feature'\u003c/span\u003e].isna().astype(int)\nctab = pd.crosstab(df[\u003cspan class=\"hljs-string\"\u003e'col_missing'\u003c/span\u003e], df[\u003cspan class=\"hljs-string\"\u003e'target'\u003c/span\u003e])\nchi2, p, dof, exp = chi2_contingency(ctab)\nprint(\u003cspan class=\"hljs-string\"\u003e\"p-value =\"\u003c/span\u003e, p)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eInterpretation\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eIf p \u0026lt; 0.05 → missingness of this feature is associated with the target.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThat suggests missingness carries information → at least MAR, possibly MNAR.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eStrength\u003c/strong\u003e: Very intuitive for predictive modeling (if missingness depends on target → definitely not MCAR).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eWeakness\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eOnly checks relation with target, not with all other features.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSo you might miss situations where missingness is related to a predictor but not the target.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"heading-compare-distributions-manually\"\u003eCompare distributions manually\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eExamine whether the distribution of observed variables differs between rows with and without missing values (should not).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003edf.groupby(df[\u003cspan class=\"hljs-string\"\u003e'col_with_missing'\u003c/span\u003e].isna())[\u003cspan class=\"hljs-string\"\u003e'other_col'\u003c/span\u003e].mean()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"heading-testing-for-mar-missing-at-random\"\u003e\u003cstrong\u003eTesting for MAR (Missing At Random)\u003c/strong\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cul\u003e\n\u003cli\u003e\u003cp\u003eStrict tests don’t exist (because MAR involves unobserved missing values).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003ePractical check\u003c/strong\u003e: Create \"missingness indicator\" (flag if a variable is missing) and test correlation with other observed variables.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e    df[\u003cspan class=\"hljs-string\"\u003e'age_missing'\u003c/span\u003e] = df[\u003cspan class=\"hljs-string\"\u003e'age'\u003c/span\u003e].isna()\n\n    \u003cspan class=\"hljs-comment\"\u003e#We want to compare age with gender to assess if missing age \u003c/span\u003e\n    \u003cspan class=\"hljs-comment\"\u003e#is related to gender value.\u003c/span\u003e\n    \u003cspan class=\"hljs-comment\"\u003e#As we are grouping by gender and then calculating mean of the age_missing column, \u003c/span\u003e\n    \u003cspan class=\"hljs-comment\"\u003e#this mean equals the proportion of missing ages in that group.\u003c/span\u003e\n    df.groupby(\u003cspan class=\"hljs-string\"\u003e'gender'\u003c/span\u003e)[\u003cspan class=\"hljs-string\"\u003e'age_missing'\u003c/span\u003e].mean()\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eWhat this tells you\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eThe result is the missingness rate of the \u003ccode\u003eage\u003c/code\u003e column for each gender:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eIf the value is 0.0 for a gender, no missing ages in that group.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eIf the value is 1.0 for a gender, all ages are missing in that group.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eValues between 0 and 1 indicate the fraction of records with missing age within that gender group.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"heading-diagnosing-mnar-missing-not-at-random\"\u003e\u003cstrong\u003eDiagnosing MNAR (Missing Not At Random)\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eTrue MNAR is \u003cstrong\u003euntestable from the data alone\u003c/strong\u003e (since it depends on the unobserved value).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eRequires:\u003cbr /\u003e  • Domain expertise (e.g., we know high salaries are underreported).\u003cbr /\u003e  • \u003cstrong\u003eSensitivity analysis\u003c/strong\u003e: Assume plausible MNAR mechanisms and check how conclusions change.\u003cbr /\u003e  • Specialized models (selection models, Heckman correction, pattern-mixture models).\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"heading-handling-missing-data-mechanisms\"\u003e\u003cstrong\u003e🛠 Handling Missing Data Mechanisms\u003c/strong\u003e\u003c/h1\u003e\n\u003ch3 id=\"heading-1-mcar\"\u003e\u003cstrong\u003e1. MCAR\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e➡️ \u003cstrong\u003eSafe to drop rows or columns\u003c/strong\u003e (analysis unbiased, only reduced sample size).\u003cbr /\u003eOptions:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003edf_clean = df.dropna()                     \u003cspan class=\"hljs-comment\"\u003e# drop rows\u003c/span\u003e\ndf_clean = df.dropna(axis=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, thresh=\u003cspan class=\"hljs-number\"\u003e0.7\u003c/span\u003e*len(df))  \u003cspan class=\"hljs-comment\"\u003e# drop columns with \u0026gt;30% missing\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOr simply use mean/median imputation without worrying about bias.\u003c/p\u003e\n\u003chr /\u003e\n\u003ch3 id=\"heading-2-mar\"\u003e\u003cstrong\u003e2. MAR\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e➡️ \u003cstrong\u003eBest handled by imputation methods that leverage observed data\u003c/strong\u003e.\u003cbr /\u003eOptions:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eGroup-wise imputation\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eWe fill missing values in the income column by gender, using each gender's median income. It then stores the result back in the income column.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e  df[\u003cspan class=\"hljs-string\"\u003e'income'\u003c/span\u003e] = df.groupby(\u003cspan class=\"hljs-string\"\u003e'gender'\u003c/span\u003e)[\u003cspan class=\"hljs-string\"\u003e'income'\u003c/span\u003e].transform(\n      \u003cspan class=\"hljs-keyword\"\u003elambda\u003c/span\u003e x: x.fillna(x.median()))\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eMultiple Imputation (MICE/Iterative)\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eUse the other features to predict missing values, one column at a time, in a round-robin fashion\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e  \u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.experimental \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e enable_iterative_imputer\n  \u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.impute \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e IterativeImputer\n  imp = IterativeImputer(random_state=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\n  df[numeric_cols] = imp.fit_transform(df[numeric_cols])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eHow IterativeImputer works (high-level)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eIt treats each feature with missing values as the target and uses the other features as predictors.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eIt iteratively:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cp\u003eRegresses the missing values of one feature on the others.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eReplaces missing values with the predicted ones.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eMoves to the next feature with missing values and repeats.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eThis process continues for several iterations until convergence, producing several imputed values that reflect potential relationships among features.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ch3 id=\"heading-step-by-step-intuition\"\u003eStep-by-step intuition:\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eStart with missing values\u003c/strong\u003e\u003cbr /\u003e You have a dataset where some entries are \u003ccode\u003eNaN\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e Example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e Age   Salary   Experience\n \u003cspan class=\"hljs-number\"\u003e25\u003c/span\u003e    \u003cspan class=\"hljs-number\"\u003e50\u003c/span\u003ek      \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e\n NaN   \u003cspan class=\"hljs-number\"\u003e60\u003c/span\u003ek      \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e\n \u003cspan class=\"hljs-number\"\u003e30\u003c/span\u003e    NaN      \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e\n \u003cspan class=\"hljs-number\"\u003e28\u003c/span\u003e    \u003cspan class=\"hljs-number\"\u003e55\u003c/span\u003ek      NaN\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eChoose one column with missing values (say Age).\u003c/strong\u003e\u003cbr /\u003e Treat \u003cem\u003eAge\u003c/em\u003e as the \"target variable\" and the other columns (Salary, Experience) as predictors.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eTrain a regression model\u003c/strong\u003e\u003cbr /\u003e Use the rows where \u003cem\u003eAge\u003c/em\u003e is known to train a model like:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e Age ~ Salary + Experience\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003ePredict missing values for Age\u003c/strong\u003e\u003cbr /\u003e Use the trained model to fill in the \u003ccode\u003eNaN\u003c/code\u003es in Age.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eMove to the next column with missing values (say Salary)\u003c/strong\u003e\u003cbr /\u003e Now treat \u003cem\u003eSalary\u003c/em\u003e as the target and use Age and Experience (with the newly imputed values) as predictors to estimate missing Salary.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eRepeat for all columns with missing values.\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eIterate multiple times\u003c/strong\u003e\u003cbr /\u003e Because each column’s imputation depends on the others (which may also be imputed), the algorithm cycles through all columns multiple times. With each iteration, the imputations become more stable and consistent.\u003c/p\u003e\n\u003cp\u003e That’s why it’s called \u003cstrong\u003eIterative\u003c/strong\u003e Imputer.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e            \u003cstrong\u003eWhen to use\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eWhen you believe there are inter-feature relationships and you want a more sophisticated imputation than simple statistics (e.g., mean/median).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eFor datasets where the pattern of missingness might depend on other observed features.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e            \u003cstrong\u003eNotes and best practices\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eEnsure numeric_cols is a list of the numeric column names you want to impute.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eYou can customize the underlying estimator (e.g., linear regression, decision trees) by passing estimator= to the IterativeImputer.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eBe mindful of potential data leakage: fit on training data only, and apply the same transformation to validation/test data.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eAdd missing indicator features\u003c/strong\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e  df[\u003cspan class=\"hljs-string\"\u003e'income_missing'\u003c/span\u003e] = df[\u003cspan class=\"hljs-string\"\u003e'income'\u003c/span\u003e].isna().astype(int)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr /\u003e\n\u003ch3 id=\"heading-3-mnar\"\u003e\u003cstrong\u003e3. MNAR\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e➡️ \u003cstrong\u003eHardest case — no purely data-driven solution.\u003c/strong\u003e\u003cbr /\u003eTypical strategies:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eDomain-informed imputation\u003c/strong\u003e (use expert rules, external benchmarks).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eSensitivity analysis\u003c/strong\u003e: Try imputations under different assumptions (e.g., assume the missing group is 10% higher/lower).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eHeckman correction / selection models\u003c/strong\u003e (statsmodels has limited support).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003ePattern mixture models / Bayesian methods\u003c/strong\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e👉 Often you need to:\u003cbr /\u003eA. Report that data may be MNAR.\u003cbr /\u003eB. Perform robustness checks (see if conclusions hold under different plausible imputations).\u003c/p\u003e\n","markdown":"# Understanding the Terms Using an Analogy- **Shoppers \u0026 Shopping Carts Analogy**\n\n## 1\\. MCAR – Missing Completely At Random\n\n**Story:** You’re analyzing what items shoppers add to their online carts. Suddenly, due to a system glitch, a random subset of cart items (across all shoppers) **fails to get recorded**. The missingness has nothing to do with what the items are or who the shopper is — just bad luck\n\n**Key idea:** Missingness is completely random. Dropping the missing data still gives you a fair picture of what people buy.\n\n💡 Real-world analogy: A lab machine randomly crashes and corrupts some test results.\n\n## 2\\. MAR – Missing At Random\n\n**Story:** Suppose items added using a **mobile app** are sometimes not saved to the cart due to slower connections. But **within the mobile shoppers**, whether an item is missing doesn’t depend on the type of item itself — only on the platform used.\n\n**Key idea:** Missingness depends on an **observed factor** (web vs mobile), but not on the item itself. If you know the shopper’s platform, you can correct for this bias.\n\n💡 Real-world analogy: Older survey respondents are less likely to report income → depends on age (observed), not income value itself.\n\n## 3\\. MNAR – Missing Not At Random\n\n**Story:** Some shoppers deliberately **remove expensive luxury items** from their cart before checking out, because they’re embarrassed or may not be able to afford them. The chance of an item being missing directly depends on the item’s **price** itself (higher price → more likely to go missing).\n\n**Key idea:** Missingness depends **directly on the unseen value** (price of item missing). Hardest case to handle, because the missingness itself hides something important.\n\n💡 Real-world analogy: High-income people avoid disclosing income → missingness depends on the true income itself.\n\n## 📝 Recap\n\n* **MCAR:** Missing values are caused by pure chance → *A random system glitch deletes some cart items* 🌐 — purely random.\n    \n    **MAR:** Missingness depends on other observed variables, not the missing value itself → *Mobile app shoppers lose items due to bad connection* 📱 — depends on platform (observed), not the item itself.\n    \n    **MNAR:** Missingness depends directly on the missing value itself → *Shoppers deliberately remove very expensive items* 💎 — depends on the value (price) that’s missing.\n    \n\n👉 Think: **MCAR = random glitch 💨, MAR = depends on something you can observe 👀, MNAR = depends on what’s hidden 🔒.**\n\n# How To Check Programmatically\n\n## **Testing MCAR (Missing Completely At Random)**\n\n### Little’s MCAR test\n\n* ONLY FOR NUMERIC COLUMNS ☹\n    \n* **Purpose**: Global test for whether data are Missing Completely At Random (MCAR) across the whole dataset, considering multivariate patterns of missingness.\n    \n* **Interpretation**:\n    \n* * * High p value → cannot reject MCAR (missingness is plausibly random).\n            \n            * Low p value → reject MCAR (missingness related to data → MAR or MNAR).\n                \n            * **Strength**: Multivariate test, designed for exactly this situation.\n                \n        * **Weakness**: Sensitive to sample size, not always available in every package.\n            \n\n```python\n# 1. LITTLE’S MCAR TEST ---------------------------------------\n# note: not natively in statsmodels yet (as of v0.14)\n# package option: `little-mcar-test`\n# install: pip install little-mcar-test\n\nfrom little_mcar_test import mcartest\n\n# run on numeric part of your dataset\nnumeric_df = df.select_dtypes(include=[np.number])\nstat, dof, p_value = mcartest(numeric_df.to_numpy())\nprint(f\"Little’s MCAR test: χ2 = {stat}, dof = {dof}, p = {p_value}\")\n\nif p_value \u003e 0.05:\n    print(\"Fail to reject MCAR → data plausibly MCAR\")\nelse:\n    print(\"Reject MCAR → data likely MAR or MNAR\")\n```\n\n### Cross-tab with Target + Chi-square\n\n* We create a new column first (a “missingness indicator”)\n    \n* Cross-tabulate this with the target variable and run a chi-square test of independence.\n    \n\n```python\nimport pandas as pd\nfrom scipy.stats import chi2_contingency\n# heuristic check: does missingness in one col depend on another categorical col?\n\ndf['col_missing'] = df['some_feature'].isna().astype(int)\nctab = pd.crosstab(df['col_missing'], df['target'])\nchi2, p, dof, exp = chi2_contingency(ctab)\nprint(\"p-value =\", p)\n```\n\n* **Interpretation**:\n    \n    * If p \u0026lt; 0.05 → missingness of this feature is associated with the target.\n        \n        * That suggests missingness carries information → at least MAR, possibly MNAR.\n            \n* **Strength**: Very intuitive for predictive modeling (if missingness depends on target → definitely not MCAR).\n    \n* **Weakness**:\n    \n    * Only checks relation with target, not with all other features.\n        \n        * So you might miss situations where missingness is related to a predictor but not the target.\n            \n\n### Compare distributions manually\n\n* Examine whether the distribution of observed variables differs between rows with and without missing values (should not).\n    \n\n```python\ndf.groupby(df['col_with_missing'].isna())['other_col'].mean()\n```\n\n## **Testing for MAR (Missing At Random)**\n\n* * Strict tests don’t exist (because MAR involves unobserved missing values).\n        \n        * **Practical check**: Create \"missingness indicator\" (flag if a variable is missing) and test correlation with other observed variables.\n            \n        * ```python\n              df['age_missing'] = df['age'].isna()\n              \n              #We want to compare age with gender to assess if missing age \n              #is related to gender value.\n              #As we are grouping by gender and then calculating mean of the age_missing column, \n              #this mean equals the proportion of missing ages in that group.\n              df.groupby('gender')['age_missing'].mean()\n            ```\n            \n* **What this tells you**\n    \n* The result is the missingness rate of the `age` column for each gender:\n    \n    * If the value is 0.0 for a gender, no missing ages in that group.\n        \n    * If the value is 1.0 for a gender, all ages are missing in that group.\n        \n    * Values between 0 and 1 indicate the fraction of records with missing age within that gender group.\n        \n\n### **Diagnosing MNAR (Missing Not At Random)**\n\n* True MNAR is **untestable from the data alone** (since it depends on the unobserved value).\n    \n* Requires:  \n    • Domain expertise (e.g., we know high salaries are underreported).  \n    • **Sensitivity analysis**: Assume plausible MNAR mechanisms and check how conclusions change.  \n    • Specialized models (selection models, Heckman correction, pattern-mixture models).\n    \n\n# **🛠 Handling Missing Data Mechanisms**\n\n### **1\\. MCAR**\n\n➡️ **Safe to drop rows or columns** (analysis unbiased, only reduced sample size).  \nOptions:\n\n```python\ndf_clean = df.dropna()                     # drop rows\ndf_clean = df.dropna(axis=1, thresh=0.7*len(df))  # drop columns with \u003e30% missing\n```\n\nOr simply use mean/median imputation without worrying about bias.\n\n---\n\n### **2\\. MAR**\n\n➡️ **Best handled by imputation methods that leverage observed data**.  \nOptions:\n\n* **Group-wise imputation**:\n    \n    * We fill missing values in the income column by gender, using each gender's median income. It then stores the result back in the income column.\n        \n        ```python\n        df['income'] = df.groupby('gender')['income'].transform(\n            lambda x: x.fillna(x.median()))\n        ```\n        \n* **Multiple Imputation (MICE/Iterative)**:\n    \n    * Use the other features to predict missing values, one column at a time, in a round-robin fashion\n        \n        ```python\n        from sklearn.experimental import enable_iterative_imputer\n        from sklearn.impute import IterativeImputer\n        imp = IterativeImputer(random_state=0)\n        df[numeric_cols] = imp.fit_transform(df[numeric_cols])\n        ```\n        \n        * **How IterativeImputer works (high-level)**\n            \n            * It treats each feature with missing values as the target and uses the other features as predictors.\n                \n            * It iteratively:\n                \n                1. Regresses the missing values of one feature on the others.\n                    \n                2. Replaces missing values with the predicted ones.\n                    \n                3. Moves to the next feature with missing values and repeats.\n                    \n            * This process continues for several iterations until convergence, producing several imputed values that reflect potential relationships among features.\n                \n            * ### Step-by-step intuition:\n                \n                1. **Start with missing values**  \n                    You have a dataset where some entries are `NaN`.\n                    \n                    Example:\n                    \n                    ```python\n                    Age   Salary   Experience\n                    25    50k      2\n                    NaN   60k      3\n                    30    NaN      4\n                    28    55k      NaN\n                    ```\n                    \n                2. **Choose one column with missing values (say Age).**  \n                    Treat *Age* as the \"target variable\" and the other columns (Salary, Experience) as predictors.\n                    \n                3. **Train a regression model**  \n                    Use the rows where *Age* is known to train a model like:\n                    \n                    ```python\n                    Age ~ Salary + Experience\n                    ```\n                    \n                4. **Predict missing values for Age**  \n                    Use the trained model to fill in the `NaN`s in Age.\n                    \n                5. **Move to the next column with missing values (say Salary)**  \n                    Now treat *Salary* as the target and use Age and Experience (with the newly imputed values) as predictors to estimate missing Salary.\n                    \n                6. **Repeat for all columns with missing values.**\n                    \n                7. **Iterate multiple times**  \n                    Because each column’s imputation depends on the others (which may also be imputed), the algorithm cycles through all columns multiple times. With each iteration, the imputations become more stable and consistent.\n                    \n                    That’s why it’s called **Iterative** Imputer.\n                    \n                \n            \n            **When to use**\n            \n            * When you believe there are inter-feature relationships and you want a more sophisticated imputation than simple statistics (e.g., mean/median).\n                \n            * For datasets where the pattern of missingness might depend on other observed features.\n                \n            \n            **Notes and best practices**\n            \n            * Ensure numeric\\_cols is a list of the numeric column names you want to impute.\n                \n            * You can customize the underlying estimator (e.g., linear regression, decision trees) by passing estimator= to the IterativeImputer.\n                \n            * Be mindful of potential data leakage: fit on training data only, and apply the same transformation to validation/test data.\n                \n* **Add missing indicator features**:\n    \n    ```python\n    df['income_missing'] = df['income'].isna().astype(int)\n    ```\n    \n\n---\n\n### **3\\. MNAR**\n\n➡️ **Hardest case — no purely data-driven solution.**  \nTypical strategies:\n\n* **Domain-informed imputation** (use expert rules, external benchmarks).\n    \n* **Sensitivity analysis**: Try imputations under different assumptions (e.g., assume the missing group is 10% higher/lower).\n    \n* **Heckman correction / selection models** (statsmodels has limited support).\n    \n* **Pattern mixture models / Bayesian methods**.\n    \n\n👉 Often you need to:  \nA. Report that data may be MNAR.  \nB. Perform robustness checks (see if conclusions hold under different plausible imputations)."},"views":14,"preferences":{"pinnedToBlog":false,"disableComments":false,"stickCoverToBottom":false,"isDelisted":false},"readTimeInMinutes":7,"series":null,"tags":[{"id":"56744722958ef13879b951ac","slug":"data-analysis","name":"data analysis"},{"id":"63f336bb322fd23a23d459f7","slug":"analogy","name":"analogy"},{"id":"62e40f16430bd1d04a83efb7","slug":"missing-data","name":"missing data"}],"ogMetaData":{"image":null},"canonicalUrl":null,"hasLatexInPost":false,"audioUrls":null,"isFollowed":null,"bookmarked":false,"features":{"tableOfContents":{"isEnabled":true,"items":[{"__typename":"TableOfContentsItem","id":"0d54f408-d734-4f19-ae67-76fd78baf26b","level":1,"slug":"understanding-the-terms-using-an-analogy-shoppers-amp-shopping-carts-analogy","title":"Understanding the Terms Using an Analogy- Shoppers \u0026amp; Shopping Carts Analogy","parentId":null},{"__typename":"TableOfContentsItem","id":"84251c14-d5ea-4069-a4c2-7e803e38760e","level":2,"slug":"1-mcar-missing-completely-at-random","title":"1. MCAR – Missing Completely At Random","parentId":"0d54f408-d734-4f19-ae67-76fd78baf26b"},{"__typename":"TableOfContentsItem","id":"27d4fa19-f1be-44bd-a38d-47acb0026af2","level":2,"slug":"2-mar-missing-at-random","title":"2. MAR – Missing At Random","parentId":"0d54f408-d734-4f19-ae67-76fd78baf26b"},{"__typename":"TableOfContentsItem","id":"924bf17c-929c-4081-ab52-82014f57551f","level":2,"slug":"3-mnar-missing-not-at-random","title":"3. MNAR – Missing Not At Random","parentId":"0d54f408-d734-4f19-ae67-76fd78baf26b"},{"__typename":"TableOfContentsItem","id":"cb081174-cd0f-4a49-bbca-9e7ae6a215f1","level":2,"slug":"recap","title":"📝 Recap","parentId":"0d54f408-d734-4f19-ae67-76fd78baf26b"},{"__typename":"TableOfContentsItem","id":"d502bfec-4f5a-4afd-ac1a-782a847d858b","level":1,"slug":"how-to-check-programmatically","title":"How To Check Programmatically","parentId":null},{"__typename":"TableOfContentsItem","id":"72c60015-95be-4d18-8d21-363a08fa58ce","level":2,"slug":"testing-mcar-missing-completely-at-random","title":"Testing MCAR (Missing Completely At Random)","parentId":"d502bfec-4f5a-4afd-ac1a-782a847d858b"},{"__typename":"TableOfContentsItem","id":"f498281d-b432-4f82-b897-61fe79ece742","level":3,"slug":"littles-mcar-test","title":"Little’s MCAR test","parentId":"72c60015-95be-4d18-8d21-363a08fa58ce"},{"__typename":"TableOfContentsItem","id":"c75365a6-f524-4f99-a806-c5bc7ae26187","level":3,"slug":"cross-tab-with-target-chi-square","title":"Cross-tab with Target + Chi-square","parentId":"72c60015-95be-4d18-8d21-363a08fa58ce"},{"__typename":"TableOfContentsItem","id":"af722907-885e-4267-a27d-61d6a5fd6330","level":3,"slug":"compare-distributions-manually","title":"Compare distributions manually","parentId":"72c60015-95be-4d18-8d21-363a08fa58ce"},{"__typename":"TableOfContentsItem","id":"a257c107-58dc-4add-8786-637ea8cc5224","level":2,"slug":"testing-for-mar-missing-at-random","title":"Testing for MAR (Missing At Random)","parentId":"d502bfec-4f5a-4afd-ac1a-782a847d858b"},{"__typename":"TableOfContentsItem","id":"2022af84-7ec0-4ae1-920e-54922b26faf3","level":3,"slug":"diagnosing-mnar-missing-not-at-random","title":"Diagnosing MNAR (Missing Not At Random)","parentId":"a257c107-58dc-4add-8786-637ea8cc5224"},{"__typename":"TableOfContentsItem","id":"681eba31-4e59-46ff-bd96-a3857f93a4fe","level":1,"slug":"handling-missing-data-mechanisms","title":"🛠 Handling Missing Data Mechanisms","parentId":null},{"__typename":"TableOfContentsItem","id":"255f15b0-6d3e-4af5-b590-1600cc1e474e","level":3,"slug":"1-mcar","title":"1. MCAR","parentId":"681eba31-4e59-46ff-bd96-a3857f93a4fe"},{"__typename":"TableOfContentsItem","id":"5e02dd39-772e-4db6-b37e-41765e33d8d0","level":3,"slug":"2-mar","title":"2. MAR","parentId":"681eba31-4e59-46ff-bd96-a3857f93a4fe"},{"__typename":"TableOfContentsItem","id":"55cec363-4166-4e25-b34f-a01021ec1593","level":3,"slug":"step-by-step-intuition","title":"Step-by-step intuition:","parentId":"681eba31-4e59-46ff-bd96-a3857f93a4fe"},{"__typename":"TableOfContentsItem","id":"43c07aa0-e737-4b4c-83d3-5b157798fe91","level":3,"slug":"3-mnar","title":"3. MNAR","parentId":"681eba31-4e59-46ff-bd96-a3857f93a4fe"}]},"badges":{"isEnabled":true,"items":[]}},"isAutoPublishedFromRSS":false,"authenticatedUserLikes":{"edges":[]},"totalUserLikes":{"totalDocuments":0},"isShadowBanned":false,"isAskMeAnything":false},"redirectedPost":null,"staticPage":null},"series":null}},"__N_SSP":true},"page":"/[...slug]","query":{"slug":["mcar-mar-mnar"]},"buildId":"MQT9dcvepPIR9449I1Jve","isFallback":false,"dynamicIds":[87179],"gssp":true,"scriptLoader":[]}</script><div id="hn-modal"></div><div id="hn-toast"></div><script>(function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML="window.__CF$cv$params={r:'98837e0e6f6fe1c0',t:'MTc1OTQwMDY5MA=='};var a=document.createElement('script');a.src='cdn-cgi/challenge-platform/h/g/scripts/jsd/4687995f25e1/maind41d.js';document.getElementsByTagName('head')[0].appendChild(a);";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();</script></body>
<!-- Mirrored from amitguptaforwork.hashnode.dev/mcar-mar-mnar by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 02 Oct 2025 10:25:05 GMT -->
</html>