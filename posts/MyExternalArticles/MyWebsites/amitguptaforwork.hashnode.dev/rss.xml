<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:hashnode="https://hashnode.com/rss"><channel><title><![CDATA[Amit's Learning Notes]]></title><description><![CDATA[Just wanted to share my notes as I learn new exciting things
]]></description><link>https://amitguptaforwork.hashnode.dev</link><generator>RSS for Node</generator><lastBuildDate>Thu, 02 Oct 2025 10:18:52 GMT</lastBuildDate><atom:link href="https://amitguptaforwork.hashnode.dev/rss.xml" rel="self" type="application/rss+xml"/><language><![CDATA[en]]></language><ttl>60</ttl><atom:link rel="next" href="https://amitguptaforwork.hashnode.dev/rss.xml?page=2"/><atom:link rel="previous" href="https://amitguptaforwork.hashnode.dev/rss.xml"/><item><title><![CDATA[Understanding Precision and Recall using Policeman analogy]]></title><link>https://amitguptaforwork.hashnode.dev/understanding-precision-and-recall-using-policeman-analogy</link><guid isPermaLink="true">https://amitguptaforwork.hashnode.dev/understanding-precision-and-recall-using-policeman-analogy</guid><category><![CDATA[Data Science]]></category><category><![CDATA[data analysis]]></category><category><![CDATA[metrics]]></category><category><![CDATA[classification problems]]></category><category><![CDATA[analogy]]></category><dc:creator><![CDATA[Amit Gupta]]></dc:creator><pubDate>Wed, 01 Oct 2025 16:37:44 GMT</pubDate><content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src=&quot;https://cdn.hashnode.com/res/hashnode/image/upload/v1759335604205/452ea1ac-7734-4bbe-bc2f-14ec29888ad9.png&quot; alt class=&quot;image--center mx-auto&quot; /&gt;&lt;/p&gt;]]&gt;</content:encoded><hashnode:content>&lt;![CDATA[&lt;p&gt;&lt;img src=&quot;https://cdn.hashnode.com/res/hashnode/image/upload/v1759335604205/452ea1ac-7734-4bbe-bc2f-14ec29888ad9.png&quot; alt class=&quot;image--center mx-auto&quot; /&gt;&lt;/p&gt;]]&gt;</hashnode:content><hashnode:coverImage>https://cdn.hashnode.com/res/hashnode/image/upload/v1759336518981/5f593c85-46f0-45e7-bd5b-6c7016ce8c18.jpeg</hashnode:coverImage></item><item><title><![CDATA[üìò Mcar, Mar, Mnar ‚Äî]]></title><description><![CDATA[Understanding the Terms Using an Analogy- Shoppers & Shopping Carts Analogy
1. MCAR ‚Äì Missing Completely At Random
Story: You‚Äôre analyzing what items shoppers add to their online carts. Suddenly, due to a system glitch, a random subset of cart items ...]]></description><link>https://amitguptaforwork.hashnode.dev/mcar-mar-mnar</link><guid isPermaLink="true">https://amitguptaforwork.hashnode.dev/mcar-mar-mnar</guid><category><![CDATA[data analysis]]></category><category><![CDATA[analogy]]></category><category><![CDATA[missing data]]></category><dc:creator><![CDATA[Amit Gupta]]></dc:creator><pubDate>Wed, 01 Oct 2025 14:26:36 GMT</pubDate><content:encoded>&lt;![CDATA[&lt;h1 id=&quot;heading-understanding-the-terms-using-an-analogy-shoppers-amp-shopping-carts-analogy&quot;&gt;Understanding the Terms Using an Analogy- &lt;strong&gt;Shoppers &amp;amp; Shopping Carts Analogy&lt;/strong&gt;&lt;/h1&gt;&lt;h2 id=&quot;heading-1-mcar-missing-completely-at-random&quot;&gt;1. MCAR  Missing Completely At Random&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Story:&lt;/strong&gt; Youre analyzing what items shoppers add to their online carts. Suddenly, due to a system glitch, a random subset of cart items (across all shoppers) &lt;strong&gt;fails to get recorded&lt;/strong&gt;. The missingness has nothing to do with what the items are or who the shopper is  just bad luck&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Key idea:&lt;/strong&gt; Missingness is completely random. Dropping the missing data still gives you a fair picture of what people buy.&lt;/p&gt;&lt;p&gt;üí° Real-world analogy: A lab machine randomly crashes and corrupts some test results.&lt;/p&gt;&lt;h2 id=&quot;heading-2-mar-missing-at-random&quot;&gt;2. MAR  Missing At Random&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Story:&lt;/strong&gt; Suppose items added using a &lt;strong&gt;mobile app&lt;/strong&gt; are sometimes not saved to the cart due to slower connections. But &lt;strong&gt;within the mobile shoppers&lt;/strong&gt;, whether an item is missing doesnt depend on the type of item itself  only on the platform used.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Key idea:&lt;/strong&gt; Missingness depends on an &lt;strong&gt;observed factor&lt;/strong&gt; (web vs mobile), but not on the item itself. If you know the shoppers platform, you can correct for this bias.&lt;/p&gt;&lt;p&gt;üí° Real-world analogy: Older survey respondents are less likely to report income  depends on age (observed), not income value itself.&lt;/p&gt;&lt;h2 id=&quot;heading-3-mnar-missing-not-at-random&quot;&gt;3. MNAR  Missing Not At Random&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Story:&lt;/strong&gt; Some shoppers deliberately &lt;strong&gt;remove expensive luxury items&lt;/strong&gt; from their cart before checking out, because theyre embarrassed or may not be able to afford them. The chance of an item being missing directly depends on the items &lt;strong&gt;price&lt;/strong&gt; itself (higher price  more likely to go missing).&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Key idea:&lt;/strong&gt; Missingness depends &lt;strong&gt;directly on the unseen value&lt;/strong&gt; (price of item missing). Hardest case to handle, because the missingness itself hides something important.&lt;/p&gt;&lt;p&gt;üí° Real-world analogy: High-income people avoid disclosing income  missingness depends on the true income itself.&lt;/p&gt;&lt;h2 id=&quot;heading-recap&quot;&gt;üìù Recap&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;MCAR:&lt;/strong&gt; Missing values are caused by pure chance  &lt;em&gt;A random system glitch deletes some cart items&lt;/em&gt; üåê  purely random.&lt;/p&gt;&lt;p&gt;  &lt;strong&gt;MAR:&lt;/strong&gt; Missingness depends on other observed variables, not the missing value itself  &lt;em&gt;Mobile app shoppers lose items due to bad connection&lt;/em&gt; üì±  depends on platform (observed), not the item itself.&lt;/p&gt;&lt;p&gt;  &lt;strong&gt;MNAR:&lt;/strong&gt; Missingness depends directly on the missing value itself  &lt;em&gt;Shoppers deliberately remove very expensive items&lt;/em&gt; üíé  depends on the value (price) thats missing.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;üëâ Think: &lt;strong&gt;MCAR = random glitch üí®, MAR = depends on something you can observe üëÄ, MNAR = depends on whats hidden üîí.&lt;/strong&gt;&lt;/p&gt;&lt;h1 id=&quot;heading-how-to-check-programmatically&quot;&gt;How To Check Programmatically&lt;/h1&gt;&lt;h2 id=&quot;heading-testing-mcar-missing-completely-at-random&quot;&gt;&lt;strong&gt;Testing MCAR (Missing Completely At Random)&lt;/strong&gt;&lt;/h2&gt;&lt;h3 id=&quot;heading-littles-mcar-test&quot;&gt;Littles MCAR test&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;ONLY FOR NUMERIC COLUMNS &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Global test for whether data are Missing Completely At Random (MCAR) across the whole dataset, considering multivariate patterns of missingness.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Interpretation&lt;/strong&gt;:&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;ul&gt;&lt;li&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;High p value  cannot reject MCAR (missingness is plausibly random).&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Low p value  reject MCAR (missingness related to data  MAR or MNAR).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Strength&lt;/strong&gt;: Multivariate test, designed for exactly this situation.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Weakness&lt;/strong&gt;: Sensitive to sample size, not always available in every package.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code class=&quot;lang-python&quot;&gt;&lt;span class=&quot;hljs-comment&quot;&gt;# 1. LITTLES MCAR TEST ---------------------------------------&lt;/span&gt;&lt;span class=&quot;hljs-comment&quot;&gt;# note: not natively in statsmodels yet (as of v0.14)&lt;/span&gt;&lt;span class=&quot;hljs-comment&quot;&gt;# package option: `little-mcar-test`&lt;/span&gt;&lt;span class=&quot;hljs-comment&quot;&gt;# install: pip install little-mcar-test&lt;/span&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;from&lt;/span&gt; little_mcar_test &lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; mcartest&lt;span class=&quot;hljs-comment&quot;&gt;# run on numeric part of your dataset&lt;/span&gt;numeric_df = df.select_dtypes(include=[np.number])stat, dof, p_value = mcartest(numeric_df.to_numpy())print(&lt;span class=&quot;hljs-string&quot;&gt;f&quot;Littles MCAR test: 2 = &lt;span class=&quot;hljs-subst&quot;&gt;{stat}&lt;/span&gt;, dof = &lt;span class=&quot;hljs-subst&quot;&gt;{dof}&lt;/span&gt;, p = &lt;span class=&quot;hljs-subst&quot;&gt;{p_value}&lt;/span&gt;&quot;&lt;/span&gt;)&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; p_value &amp;gt; &lt;span class=&quot;hljs-number&quot;&gt;0.05&lt;/span&gt;:    print(&lt;span class=&quot;hljs-string&quot;&gt;&quot;Fail to reject MCAR  data plausibly MCAR&quot;&lt;/span&gt;)&lt;span class=&quot;hljs-keyword&quot;&gt;else&lt;/span&gt;:    print(&lt;span class=&quot;hljs-string&quot;&gt;&quot;Reject MCAR  data likely MAR or MNAR&quot;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;heading-cross-tab-with-target-chi-square&quot;&gt;Cross-tab with Target + Chi-square&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;We create a new column first (a missingness indicator)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cross-tabulate this with the target variable and run a chi-square test of independence.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code class=&quot;lang-python&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;hljs-keyword&quot;&gt;as&lt;/span&gt; pd&lt;span class=&quot;hljs-keyword&quot;&gt;from&lt;/span&gt; scipy.stats &lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; chi2_contingency&lt;span class=&quot;hljs-comment&quot;&gt;# heuristic check: does missingness in one col depend on another categorical col?&lt;/span&gt;df[&lt;span class=&quot;hljs-string&quot;&gt;&apos;col_missing&apos;&lt;/span&gt;] = df[&lt;span class=&quot;hljs-string&quot;&gt;&apos;some_feature&apos;&lt;/span&gt;].isna().astype(int)ctab = pd.crosstab(df[&lt;span class=&quot;hljs-string&quot;&gt;&apos;col_missing&apos;&lt;/span&gt;], df[&lt;span class=&quot;hljs-string&quot;&gt;&apos;target&apos;&lt;/span&gt;])chi2, p, dof, exp = chi2_contingency(ctab)print(&lt;span class=&quot;hljs-string&quot;&gt;&quot;p-value =&quot;&lt;/span&gt;, p)&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Interpretation&lt;/strong&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;If p &amp;lt; 0.05  missingness of this feature is associated with the target.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;That suggests missingness carries information  at least MAR, possibly MNAR.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Strength&lt;/strong&gt;: Very intuitive for predictive modeling (if missingness depends on target  definitely not MCAR).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Weakness&lt;/strong&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Only checks relation with target, not with all other features.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;So you might miss situations where missingness is related to a predictor but not the target.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;heading-compare-distributions-manually&quot;&gt;Compare distributions manually&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Examine whether the distribution of observed variables differs between rows with and without missing values (should not).&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code class=&quot;lang-python&quot;&gt;df.groupby(df[&lt;span class=&quot;hljs-string&quot;&gt;&apos;col_with_missing&apos;&lt;/span&gt;].isna())[&lt;span class=&quot;hljs-string&quot;&gt;&apos;other_col&apos;&lt;/span&gt;].mean()&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;heading-testing-for-mar-missing-at-random&quot;&gt;&lt;strong&gt;Testing for MAR (Missing At Random)&lt;/strong&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Strict tests dont exist (because MAR involves unobserved missing values).&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Practical check&lt;/strong&gt;: Create &quot;missingness indicator&quot; (flag if a variable is missing) and test correlation with other observed variables.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;pre&gt;&lt;code class=&quot;lang-python&quot;&gt;    df[&lt;span class=&quot;hljs-string&quot;&gt;&apos;age_missing&apos;&lt;/span&gt;] = df[&lt;span class=&quot;hljs-string&quot;&gt;&apos;age&apos;&lt;/span&gt;].isna()    &lt;span class=&quot;hljs-comment&quot;&gt;#We want to compare age with gender to assess if missing age &lt;/span&gt;    &lt;span class=&quot;hljs-comment&quot;&gt;#is related to gender value.&lt;/span&gt;    &lt;span class=&quot;hljs-comment&quot;&gt;#As we are grouping by gender and then calculating mean of the age_missing column, &lt;/span&gt;    &lt;span class=&quot;hljs-comment&quot;&gt;#this mean equals the proportion of missing ages in that group.&lt;/span&gt;    df.groupby(&lt;span class=&quot;hljs-string&quot;&gt;&apos;gender&apos;&lt;/span&gt;)[&lt;span class=&quot;hljs-string&quot;&gt;&apos;age_missing&apos;&lt;/span&gt;].mean()&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;What this tells you&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The result is the missingness rate of the &lt;code&gt;age&lt;/code&gt; column for each gender:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;If the value is 0.0 for a gender, no missing ages in that group.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;If the value is 1.0 for a gender, all ages are missing in that group.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Values between 0 and 1 indicate the fraction of records with missing age within that gender group.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;heading-diagnosing-mnar-missing-not-at-random&quot;&gt;&lt;strong&gt;Diagnosing MNAR (Missing Not At Random)&lt;/strong&gt;&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;True MNAR is &lt;strong&gt;untestable from the data alone&lt;/strong&gt; (since it depends on the unobserved value).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Requires:&lt;br /&gt;   Domain expertise (e.g., we know high salaries are underreported).&lt;br /&gt;   &lt;strong&gt;Sensitivity analysis&lt;/strong&gt;: Assume plausible MNAR mechanisms and check how conclusions change.&lt;br /&gt;   Specialized models (selection models, Heckman correction, pattern-mixture models).&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h1 id=&quot;heading-handling-missing-data-mechanisms&quot;&gt;&lt;strong&gt;üõ† Handling Missing Data Mechanisms&lt;/strong&gt;&lt;/h1&gt;&lt;h3 id=&quot;heading-1-mcar&quot;&gt;&lt;strong&gt;1. MCAR&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt; &lt;strong&gt;Safe to drop rows or columns&lt;/strong&gt; (analysis unbiased, only reduced sample size).&lt;br /&gt;Options:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;lang-python&quot;&gt;df_clean = df.dropna()                     &lt;span class=&quot;hljs-comment&quot;&gt;# drop rows&lt;/span&gt;df_clean = df.dropna(axis=&lt;span class=&quot;hljs-number&quot;&gt;1&lt;/span&gt;, thresh=&lt;span class=&quot;hljs-number&quot;&gt;0.7&lt;/span&gt;*len(df))  &lt;span class=&quot;hljs-comment&quot;&gt;# drop columns with &amp;gt;30% missing&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Or simply use mean/median imputation without worrying about bias.&lt;/p&gt;&lt;hr /&gt;&lt;h3 id=&quot;heading-2-mar&quot;&gt;&lt;strong&gt;2. MAR&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt; &lt;strong&gt;Best handled by imputation methods that leverage observed data&lt;/strong&gt;.&lt;br /&gt;Options:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Group-wise imputation&lt;/strong&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;We fill missing values in the income column by gender, using each gender&apos;s median income. It then stores the result back in the income column.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;lang-python&quot;&gt;  df[&lt;span class=&quot;hljs-string&quot;&gt;&apos;income&apos;&lt;/span&gt;] = df.groupby(&lt;span class=&quot;hljs-string&quot;&gt;&apos;gender&apos;&lt;/span&gt;)[&lt;span class=&quot;hljs-string&quot;&gt;&apos;income&apos;&lt;/span&gt;].transform(      &lt;span class=&quot;hljs-keyword&quot;&gt;lambda&lt;/span&gt; x: x.fillna(x.median()))&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Multiple Imputation (MICE/Iterative)&lt;/strong&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Use the other features to predict missing values, one column at a time, in a round-robin fashion&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;lang-python&quot;&gt;  &lt;span class=&quot;hljs-keyword&quot;&gt;from&lt;/span&gt; sklearn.experimental &lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; enable_iterative_imputer  &lt;span class=&quot;hljs-keyword&quot;&gt;from&lt;/span&gt; sklearn.impute &lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; IterativeImputer  imp = IterativeImputer(random_state=&lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;)  df[numeric_cols] = imp.fit_transform(df[numeric_cols])&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;How IterativeImputer works (high-level)&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;It treats each feature with missing values as the target and uses the other features as predictors.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;It iteratively:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Regresses the missing values of one feature on the others.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Replaces missing values with the predicted ones.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Moves to the next feature with missing values and repeats.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;This process continues for several iterations until convergence, producing several imputed values that reflect potential relationships among features.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;h3 id=&quot;heading-step-by-step-intuition&quot;&gt;Step-by-step intuition:&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Start with missing values&lt;/strong&gt;&lt;br /&gt; You have a dataset where some entries are &lt;code&gt;NaN&lt;/code&gt;.&lt;/p&gt;&lt;p&gt; Example:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;lang-python&quot;&gt; Age   Salary   Experience &lt;span class=&quot;hljs-number&quot;&gt;25&lt;/span&gt;    &lt;span class=&quot;hljs-number&quot;&gt;50&lt;/span&gt;k      &lt;span class=&quot;hljs-number&quot;&gt;2&lt;/span&gt; NaN   &lt;span class=&quot;hljs-number&quot;&gt;60&lt;/span&gt;k      &lt;span class=&quot;hljs-number&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;30&lt;/span&gt;    NaN      &lt;span class=&quot;hljs-number&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;28&lt;/span&gt;    &lt;span class=&quot;hljs-number&quot;&gt;55&lt;/span&gt;k      NaN&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Choose one column with missing values (say Age).&lt;/strong&gt;&lt;br /&gt; Treat &lt;em&gt;Age&lt;/em&gt; as the &quot;target variable&quot; and the other columns (Salary, Experience) as predictors.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Train a regression model&lt;/strong&gt;&lt;br /&gt; Use the rows where &lt;em&gt;Age&lt;/em&gt; is known to train a model like:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;lang-python&quot;&gt; Age ~ Salary + Experience&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Predict missing values for Age&lt;/strong&gt;&lt;br /&gt; Use the trained model to fill in the &lt;code&gt;NaN&lt;/code&gt;s in Age.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Move to the next column with missing values (say Salary)&lt;/strong&gt;&lt;br /&gt; Now treat &lt;em&gt;Salary&lt;/em&gt; as the target and use Age and Experience (with the newly imputed values) as predictors to estimate missing Salary.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Repeat for all columns with missing values.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Iterate multiple times&lt;/strong&gt;&lt;br /&gt; Because each columns imputation depends on the others (which may also be imputed), the algorithm cycles through all columns multiple times. With each iteration, the imputations become more stable and consistent.&lt;/p&gt;&lt;p&gt; Thats why its called &lt;strong&gt;Iterative&lt;/strong&gt; Imputer.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;            &lt;strong&gt;When to use&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;When you believe there are inter-feature relationships and you want a more sophisticated imputation than simple statistics (e.g., mean/median).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;For datasets where the pattern of missingness might depend on other observed features.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;            &lt;strong&gt;Notes and best practices&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Ensure numeric_cols is a list of the numeric column names you want to impute.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;You can customize the underlying estimator (e.g., linear regression, decision trees) by passing estimator= to the IterativeImputer.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Be mindful of potential data leakage: fit on training data only, and apply the same transformation to validation/test data.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Add missing indicator features&lt;/strong&gt;:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;lang-python&quot;&gt;  df[&lt;span class=&quot;hljs-string&quot;&gt;&apos;income_missing&apos;&lt;/span&gt;] = df[&lt;span class=&quot;hljs-string&quot;&gt;&apos;income&apos;&lt;/span&gt;].isna().astype(int)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&lt;/ul&gt;&lt;hr /&gt;&lt;h3 id=&quot;heading-3-mnar&quot;&gt;&lt;strong&gt;3. MNAR&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt; &lt;strong&gt;Hardest case  no purely data-driven solution.&lt;/strong&gt;&lt;br /&gt;Typical strategies:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Domain-informed imputation&lt;/strong&gt; (use expert rules, external benchmarks).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Sensitivity analysis&lt;/strong&gt;: Try imputations under different assumptions (e.g., assume the missing group is 10% higher/lower).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Heckman correction / selection models&lt;/strong&gt; (statsmodels has limited support).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Pattern mixture models / Bayesian methods&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;üëâ Often you need to:&lt;br /&gt;A. Report that data may be MNAR.&lt;br /&gt;B. Perform robustness checks (see if conclusions hold under different plausible imputations).&lt;/p&gt;]]&gt;</content:encoded><hashnode:content>&lt;![CDATA[&lt;h1 id=&quot;heading-understanding-the-terms-using-an-analogy-shoppers-amp-shopping-carts-analogy&quot;&gt;Understanding the Terms Using an Analogy- &lt;strong&gt;Shoppers &amp;amp; Shopping Carts Analogy&lt;/strong&gt;&lt;/h1&gt;&lt;h2 id=&quot;heading-1-mcar-missing-completely-at-random&quot;&gt;1. MCAR  Missing Completely At Random&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Story:&lt;/strong&gt; Youre analyzing what items shoppers add to their online carts. Suddenly, due to a system glitch, a random subset of cart items (across all shoppers) &lt;strong&gt;fails to get recorded&lt;/strong&gt;. The missingness has nothing to do with what the items are or who the shopper is  just bad luck&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Key idea:&lt;/strong&gt; Missingness is completely random. Dropping the missing data still gives you a fair picture of what people buy.&lt;/p&gt;&lt;p&gt;üí° Real-world analogy: A lab machine randomly crashes and corrupts some test results.&lt;/p&gt;&lt;h2 id=&quot;heading-2-mar-missing-at-random&quot;&gt;2. MAR  Missing At Random&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Story:&lt;/strong&gt; Suppose items added using a &lt;strong&gt;mobile app&lt;/strong&gt; are sometimes not saved to the cart due to slower connections. But &lt;strong&gt;within the mobile shoppers&lt;/strong&gt;, whether an item is missing doesnt depend on the type of item itself  only on the platform used.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Key idea:&lt;/strong&gt; Missingness depends on an &lt;strong&gt;observed factor&lt;/strong&gt; (web vs mobile), but not on the item itself. If you know the shoppers platform, you can correct for this bias.&lt;/p&gt;&lt;p&gt;üí° Real-world analogy: Older survey respondents are less likely to report income  depends on age (observed), not income value itself.&lt;/p&gt;&lt;h2 id=&quot;heading-3-mnar-missing-not-at-random&quot;&gt;3. MNAR  Missing Not At Random&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Story:&lt;/strong&gt; Some shoppers deliberately &lt;strong&gt;remove expensive luxury items&lt;/strong&gt; from their cart before checking out, because theyre embarrassed or may not be able to afford them. The chance of an item being missing directly depends on the items &lt;strong&gt;price&lt;/strong&gt; itself (higher price  more likely to go missing).&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Key idea:&lt;/strong&gt; Missingness depends &lt;strong&gt;directly on the unseen value&lt;/strong&gt; (price of item missing). Hardest case to handle, because the missingness itself hides something important.&lt;/p&gt;&lt;p&gt;üí° Real-world analogy: High-income people avoid disclosing income  missingness depends on the true income itself.&lt;/p&gt;&lt;h2 id=&quot;heading-recap&quot;&gt;üìù Recap&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;MCAR:&lt;/strong&gt; Missing values are caused by pure chance  &lt;em&gt;A random system glitch deletes some cart items&lt;/em&gt; üåê  purely random.&lt;/p&gt;&lt;p&gt;  &lt;strong&gt;MAR:&lt;/strong&gt; Missingness depends on other observed variables, not the missing value itself  &lt;em&gt;Mobile app shoppers lose items due to bad connection&lt;/em&gt; üì±  depends on platform (observed), not the item itself.&lt;/p&gt;&lt;p&gt;  &lt;strong&gt;MNAR:&lt;/strong&gt; Missingness depends directly on the missing value itself  &lt;em&gt;Shoppers deliberately remove very expensive items&lt;/em&gt; üíé  depends on the value (price) thats missing.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;üëâ Think: &lt;strong&gt;MCAR = random glitch üí®, MAR = depends on something you can observe üëÄ, MNAR = depends on whats hidden üîí.&lt;/strong&gt;&lt;/p&gt;&lt;h1 id=&quot;heading-how-to-check-programmatically&quot;&gt;How To Check Programmatically&lt;/h1&gt;&lt;h2 id=&quot;heading-testing-mcar-missing-completely-at-random&quot;&gt;&lt;strong&gt;Testing MCAR (Missing Completely At Random)&lt;/strong&gt;&lt;/h2&gt;&lt;h3 id=&quot;heading-littles-mcar-test&quot;&gt;Littles MCAR test&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;ONLY FOR NUMERIC COLUMNS &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Global test for whether data are Missing Completely At Random (MCAR) across the whole dataset, considering multivariate patterns of missingness.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Interpretation&lt;/strong&gt;:&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;ul&gt;&lt;li&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;High p value  cannot reject MCAR (missingness is plausibly random).&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Low p value  reject MCAR (missingness related to data  MAR or MNAR).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Strength&lt;/strong&gt;: Multivariate test, designed for exactly this situation.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Weakness&lt;/strong&gt;: Sensitive to sample size, not always available in every package.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code class=&quot;lang-python&quot;&gt;&lt;span class=&quot;hljs-comment&quot;&gt;# 1. LITTLES MCAR TEST ---------------------------------------&lt;/span&gt;&lt;span class=&quot;hljs-comment&quot;&gt;# note: not natively in statsmodels yet (as of v0.14)&lt;/span&gt;&lt;span class=&quot;hljs-comment&quot;&gt;# package option: `little-mcar-test`&lt;/span&gt;&lt;span class=&quot;hljs-comment&quot;&gt;# install: pip install little-mcar-test&lt;/span&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;from&lt;/span&gt; little_mcar_test &lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; mcartest&lt;span class=&quot;hljs-comment&quot;&gt;# run on numeric part of your dataset&lt;/span&gt;numeric_df = df.select_dtypes(include=[np.number])stat, dof, p_value = mcartest(numeric_df.to_numpy())print(&lt;span class=&quot;hljs-string&quot;&gt;f&quot;Littles MCAR test: 2 = &lt;span class=&quot;hljs-subst&quot;&gt;{stat}&lt;/span&gt;, dof = &lt;span class=&quot;hljs-subst&quot;&gt;{dof}&lt;/span&gt;, p = &lt;span class=&quot;hljs-subst&quot;&gt;{p_value}&lt;/span&gt;&quot;&lt;/span&gt;)&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; p_value &amp;gt; &lt;span class=&quot;hljs-number&quot;&gt;0.05&lt;/span&gt;:    print(&lt;span class=&quot;hljs-string&quot;&gt;&quot;Fail to reject MCAR  data plausibly MCAR&quot;&lt;/span&gt;)&lt;span class=&quot;hljs-keyword&quot;&gt;else&lt;/span&gt;:    print(&lt;span class=&quot;hljs-string&quot;&gt;&quot;Reject MCAR  data likely MAR or MNAR&quot;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;heading-cross-tab-with-target-chi-square&quot;&gt;Cross-tab with Target + Chi-square&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;We create a new column first (a missingness indicator)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cross-tabulate this with the target variable and run a chi-square test of independence.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code class=&quot;lang-python&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;hljs-keyword&quot;&gt;as&lt;/span&gt; pd&lt;span class=&quot;hljs-keyword&quot;&gt;from&lt;/span&gt; scipy.stats &lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; chi2_contingency&lt;span class=&quot;hljs-comment&quot;&gt;# heuristic check: does missingness in one col depend on another categorical col?&lt;/span&gt;df[&lt;span class=&quot;hljs-string&quot;&gt;&apos;col_missing&apos;&lt;/span&gt;] = df[&lt;span class=&quot;hljs-string&quot;&gt;&apos;some_feature&apos;&lt;/span&gt;].isna().astype(int)ctab = pd.crosstab(df[&lt;span class=&quot;hljs-string&quot;&gt;&apos;col_missing&apos;&lt;/span&gt;], df[&lt;span class=&quot;hljs-string&quot;&gt;&apos;target&apos;&lt;/span&gt;])chi2, p, dof, exp = chi2_contingency(ctab)print(&lt;span class=&quot;hljs-string&quot;&gt;&quot;p-value =&quot;&lt;/span&gt;, p)&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Interpretation&lt;/strong&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;If p &amp;lt; 0.05  missingness of this feature is associated with the target.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;That suggests missingness carries information  at least MAR, possibly MNAR.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Strength&lt;/strong&gt;: Very intuitive for predictive modeling (if missingness depends on target  definitely not MCAR).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Weakness&lt;/strong&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Only checks relation with target, not with all other features.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;So you might miss situations where missingness is related to a predictor but not the target.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;heading-compare-distributions-manually&quot;&gt;Compare distributions manually&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Examine whether the distribution of observed variables differs between rows with and without missing values (should not).&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code class=&quot;lang-python&quot;&gt;df.groupby(df[&lt;span class=&quot;hljs-string&quot;&gt;&apos;col_with_missing&apos;&lt;/span&gt;].isna())[&lt;span class=&quot;hljs-string&quot;&gt;&apos;other_col&apos;&lt;/span&gt;].mean()&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;heading-testing-for-mar-missing-at-random&quot;&gt;&lt;strong&gt;Testing for MAR (Missing At Random)&lt;/strong&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Strict tests dont exist (because MAR involves unobserved missing values).&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Practical check&lt;/strong&gt;: Create &quot;missingness indicator&quot; (flag if a variable is missing) and test correlation with other observed variables.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;pre&gt;&lt;code class=&quot;lang-python&quot;&gt;    df[&lt;span class=&quot;hljs-string&quot;&gt;&apos;age_missing&apos;&lt;/span&gt;] = df[&lt;span class=&quot;hljs-string&quot;&gt;&apos;age&apos;&lt;/span&gt;].isna()    &lt;span class=&quot;hljs-comment&quot;&gt;#We want to compare age with gender to assess if missing age &lt;/span&gt;    &lt;span class=&quot;hljs-comment&quot;&gt;#is related to gender value.&lt;/span&gt;    &lt;span class=&quot;hljs-comment&quot;&gt;#As we are grouping by gender and then calculating mean of the age_missing column, &lt;/span&gt;    &lt;span class=&quot;hljs-comment&quot;&gt;#this mean equals the proportion of missing ages in that group.&lt;/span&gt;    df.groupby(&lt;span class=&quot;hljs-string&quot;&gt;&apos;gender&apos;&lt;/span&gt;)[&lt;span class=&quot;hljs-string&quot;&gt;&apos;age_missing&apos;&lt;/span&gt;].mean()&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;What this tells you&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The result is the missingness rate of the &lt;code&gt;age&lt;/code&gt; column for each gender:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;If the value is 0.0 for a gender, no missing ages in that group.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;If the value is 1.0 for a gender, all ages are missing in that group.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Values between 0 and 1 indicate the fraction of records with missing age within that gender group.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;heading-diagnosing-mnar-missing-not-at-random&quot;&gt;&lt;strong&gt;Diagnosing MNAR (Missing Not At Random)&lt;/strong&gt;&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;True MNAR is &lt;strong&gt;untestable from the data alone&lt;/strong&gt; (since it depends on the unobserved value).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Requires:&lt;br /&gt;   Domain expertise (e.g., we know high salaries are underreported).&lt;br /&gt;   &lt;strong&gt;Sensitivity analysis&lt;/strong&gt;: Assume plausible MNAR mechanisms and check how conclusions change.&lt;br /&gt;   Specialized models (selection models, Heckman correction, pattern-mixture models).&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h1 id=&quot;heading-handling-missing-data-mechanisms&quot;&gt;&lt;strong&gt;üõ† Handling Missing Data Mechanisms&lt;/strong&gt;&lt;/h1&gt;&lt;h3 id=&quot;heading-1-mcar&quot;&gt;&lt;strong&gt;1. MCAR&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt; &lt;strong&gt;Safe to drop rows or columns&lt;/strong&gt; (analysis unbiased, only reduced sample size).&lt;br /&gt;Options:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;lang-python&quot;&gt;df_clean = df.dropna()                     &lt;span class=&quot;hljs-comment&quot;&gt;# drop rows&lt;/span&gt;df_clean = df.dropna(axis=&lt;span class=&quot;hljs-number&quot;&gt;1&lt;/span&gt;, thresh=&lt;span class=&quot;hljs-number&quot;&gt;0.7&lt;/span&gt;*len(df))  &lt;span class=&quot;hljs-comment&quot;&gt;# drop columns with &amp;gt;30% missing&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Or simply use mean/median imputation without worrying about bias.&lt;/p&gt;&lt;hr /&gt;&lt;h3 id=&quot;heading-2-mar&quot;&gt;&lt;strong&gt;2. MAR&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt; &lt;strong&gt;Best handled by imputation methods that leverage observed data&lt;/strong&gt;.&lt;br /&gt;Options:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Group-wise imputation&lt;/strong&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;We fill missing values in the income column by gender, using each gender&apos;s median income. It then stores the result back in the income column.&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;lang-python&quot;&gt;  df[&lt;span class=&quot;hljs-string&quot;&gt;&apos;income&apos;&lt;/span&gt;] = df.groupby(&lt;span class=&quot;hljs-string&quot;&gt;&apos;gender&apos;&lt;/span&gt;)[&lt;span class=&quot;hljs-string&quot;&gt;&apos;income&apos;&lt;/span&gt;].transform(      &lt;span class=&quot;hljs-keyword&quot;&gt;lambda&lt;/span&gt; x: x.fillna(x.median()))&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Multiple Imputation (MICE/Iterative)&lt;/strong&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Use the other features to predict missing values, one column at a time, in a round-robin fashion&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;lang-python&quot;&gt;  &lt;span class=&quot;hljs-keyword&quot;&gt;from&lt;/span&gt; sklearn.experimental &lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; enable_iterative_imputer  &lt;span class=&quot;hljs-keyword&quot;&gt;from&lt;/span&gt; sklearn.impute &lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; IterativeImputer  imp = IterativeImputer(random_state=&lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;)  df[numeric_cols] = imp.fit_transform(df[numeric_cols])&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;How IterativeImputer works (high-level)&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;It treats each feature with missing values as the target and uses the other features as predictors.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;It iteratively:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Regresses the missing values of one feature on the others.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Replaces missing values with the predicted ones.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Moves to the next feature with missing values and repeats.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;This process continues for several iterations until convergence, producing several imputed values that reflect potential relationships among features.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;h3 id=&quot;heading-step-by-step-intuition&quot;&gt;Step-by-step intuition:&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Start with missing values&lt;/strong&gt;&lt;br /&gt; You have a dataset where some entries are &lt;code&gt;NaN&lt;/code&gt;.&lt;/p&gt;&lt;p&gt; Example:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;lang-python&quot;&gt; Age   Salary   Experience &lt;span class=&quot;hljs-number&quot;&gt;25&lt;/span&gt;    &lt;span class=&quot;hljs-number&quot;&gt;50&lt;/span&gt;k      &lt;span class=&quot;hljs-number&quot;&gt;2&lt;/span&gt; NaN   &lt;span class=&quot;hljs-number&quot;&gt;60&lt;/span&gt;k      &lt;span class=&quot;hljs-number&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;30&lt;/span&gt;    NaN      &lt;span class=&quot;hljs-number&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;28&lt;/span&gt;    &lt;span class=&quot;hljs-number&quot;&gt;55&lt;/span&gt;k      NaN&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Choose one column with missing values (say Age).&lt;/strong&gt;&lt;br /&gt; Treat &lt;em&gt;Age&lt;/em&gt; as the &quot;target variable&quot; and the other columns (Salary, Experience) as predictors.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Train a regression model&lt;/strong&gt;&lt;br /&gt; Use the rows where &lt;em&gt;Age&lt;/em&gt; is known to train a model like:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;lang-python&quot;&gt; Age ~ Salary + Experience&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Predict missing values for Age&lt;/strong&gt;&lt;br /&gt; Use the trained model to fill in the &lt;code&gt;NaN&lt;/code&gt;s in Age.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Move to the next column with missing values (say Salary)&lt;/strong&gt;&lt;br /&gt; Now treat &lt;em&gt;Salary&lt;/em&gt; as the target and use Age and Experience (with the newly imputed values) as predictors to estimate missing Salary.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Repeat for all columns with missing values.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Iterate multiple times&lt;/strong&gt;&lt;br /&gt; Because each columns imputation depends on the others (which may also be imputed), the algorithm cycles through all columns multiple times. With each iteration, the imputations become more stable and consistent.&lt;/p&gt;&lt;p&gt; Thats why its called &lt;strong&gt;Iterative&lt;/strong&gt; Imputer.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;            &lt;strong&gt;When to use&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;When you believe there are inter-feature relationships and you want a more sophisticated imputation than simple statistics (e.g., mean/median).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;For datasets where the pattern of missingness might depend on other observed features.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;            &lt;strong&gt;Notes and best practices&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Ensure numeric_cols is a list of the numeric column names you want to impute.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;You can customize the underlying estimator (e.g., linear regression, decision trees) by passing estimator= to the IterativeImputer.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Be mindful of potential data leakage: fit on training data only, and apply the same transformation to validation/test data.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Add missing indicator features&lt;/strong&gt;:&lt;/p&gt;&lt;pre&gt;&lt;code class=&quot;lang-python&quot;&gt;  df[&lt;span class=&quot;hljs-string&quot;&gt;&apos;income_missing&apos;&lt;/span&gt;] = df[&lt;span class=&quot;hljs-string&quot;&gt;&apos;income&apos;&lt;/span&gt;].isna().astype(int)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&lt;/ul&gt;&lt;hr /&gt;&lt;h3 id=&quot;heading-3-mnar&quot;&gt;&lt;strong&gt;3. MNAR&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt; &lt;strong&gt;Hardest case  no purely data-driven solution.&lt;/strong&gt;&lt;br /&gt;Typical strategies:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Domain-informed imputation&lt;/strong&gt; (use expert rules, external benchmarks).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Sensitivity analysis&lt;/strong&gt;: Try imputations under different assumptions (e.g., assume the missing group is 10% higher/lower).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Heckman correction / selection models&lt;/strong&gt; (statsmodels has limited support).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Pattern mixture models / Bayesian methods&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;üëâ Often you need to:&lt;br /&gt;A. Report that data may be MNAR.&lt;br /&gt;B. Perform robustness checks (see if conclusions hold under different plausible imputations).&lt;/p&gt;]]&gt;</hashnode:content><hashnode:coverImage>https://cdn.hashnode.com/res/hashnode/image/upload/v1759335348948/4ead57b0-0b81-454f-a5d5-938972c37be8.jpeg</hashnode:coverImage></item></channel></rss>